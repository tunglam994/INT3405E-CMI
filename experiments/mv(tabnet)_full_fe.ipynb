{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e3bd907",
   "metadata": {
    "papermill": {
     "duration": 0.013204,
     "end_time": "2024-12-22T10:27:40.967356",
     "exception": false,
     "start_time": "2024-12-22T10:27:40.954152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook demonstrates a complete machine learning pipeline, including:\n",
    "- Preprocessing CSV and time-series data.\n",
    "- Using Optuna to tune hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "- Training multiple models with optimized parameters.\n",
    "- Combining predictions using an ensemble method.\n",
    "- Generating a Kaggle submission.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e46447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T13:21:14.156098Z",
     "iopub.status.busy": "2024-12-17T13:21:14.155355Z",
     "iopub.status.idle": "2024-12-17T13:21:14.164244Z",
     "shell.execute_reply": "2024-12-17T13:21:14.162688Z",
     "shell.execute_reply.started": "2024-12-17T13:21:14.156064Z"
    },
    "papermill": {
     "duration": 0.01187,
     "end_time": "2024-12-22T10:27:40.990968",
     "exception": false,
     "start_time": "2024-12-22T10:27:40.979098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Table of Contents**\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries and Utilities](#libraries-and-utilities)\n",
    "3. [Preprocessing](#preprocessing)\n",
    "    - [Preprocessing CSV Data](#preprocessing-csv-data)\n",
    "    - [Preprocessing Time-Series Data](#preprocessing-time-series-data)\n",
    "    - [Merging Preprocessed Data](#merging-preprocessed-data)\n",
    "4. [Hyperparameter Tuning with Optuna](#hyperparameter-tuning)\n",
    "    - [LightGBM](#lightgbm)\n",
    "    - [XGBoost](#xgboost)\n",
    "    - [CatBoost](#catboost)\n",
    "5. [Model Training](#model-training)\n",
    "6. [Ensemble Predictions](#ensemble-predictions)\n",
    "7. [Submission](#submission)\n",
    "8. [Main Pipeline](#main-pipeline)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c2bad",
   "metadata": {
    "papermill": {
     "duration": 0.01123,
     "end_time": "2024-12-22T10:27:41.013634",
     "exception": false,
     "start_time": "2024-12-22T10:27:41.002404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"libraries-and-utilities\"></a>\n",
    "# **Libraries and Utilities**\n",
    "Import the required libraries, including Optuna for hyperparameter optimization.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72b97c",
   "metadata": {
    "papermill": {
     "duration": 0.012242,
     "end_time": "2024-12-22T10:27:41.037158",
     "exception": false,
     "start_time": "2024-12-22T10:27:41.024916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Install packages and import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7371a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:41.061501Z",
     "iopub.status.busy": "2024-12-22T10:27:41.061132Z",
     "iopub.status.idle": "2024-12-22T10:28:21.878543Z",
     "shell.execute_reply": "2024-12-22T10:28:21.877549Z"
    },
    "papermill": {
     "duration": 40.832352,
     "end_time": "2024-12-22T10:28:21.880907",
     "exception": false,
     "start_time": "2024-12-22T10:27:41.048555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2f1451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:28:21.906070Z",
     "iopub.status.busy": "2024-12-22T10:28:21.905718Z",
     "iopub.status.idle": "2024-12-22T10:30:35.528685Z",
     "shell.execute_reply": "2024-12-22T10:30:35.527668Z"
    },
    "papermill": {
     "duration": 133.638569,
     "end_time": "2024-12-22T10:30:35.531335",
     "exception": false,
     "start_time": "2024-12-22T10:28:21.892766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.20.3\r\n",
      "Uninstalling protobuf-3.20.3:\r\n",
      "  Successfully uninstalled protobuf-3.20.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: protobuf\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-3.19.6\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/tabtransformertf-0.0.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.21.6)\r\n",
      "Requirement already satisfied: pandas>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.3.5)\r\n",
      "Requirement already satisfied: tensorflow>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (4.64.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->tabtransformertf==0.0.8) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->tabtransformertf==0.0.8) (2022.7.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (1.2.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.19.6)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (59.8.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.3.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (15.0.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.4.0)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (23.1.21)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.16.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.4.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (23.0)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.2)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.6.3)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.51.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.29.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.8.0)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.14.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.38.4)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.35.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.28.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.2.3)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.9)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.11.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.2.2)\r\n",
      "Installing collected packages: tabtransformertf\r\n",
      "Successfully installed tabtransformertf-0.0.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.19.0) (23.0)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.19.0) (2.13.3)\r\n",
      "tensorflow-addons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf -y\n",
    "!pip install /kaggle/input/ft-transformer/protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install /kaggle/input/ft-transformer/tabtransformertf-0.0.8-py3-none-any.whl\n",
    "!pip install /kaggle/input/ft-transformer/tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0968ea29",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:35.558990Z",
     "iopub.status.busy": "2024-12-22T10:30:35.558597Z",
     "iopub.status.idle": "2024-12-22T10:30:43.291947Z",
     "shell.execute_reply": "2024-12-22T10:30:43.291053Z"
    },
    "papermill": {
     "duration": 7.750237,
     "end_time": "2024-12-22T10:30:43.294496",
     "exception": false,
     "start_time": "2024-12-22T10:30:35.544259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b05e8a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:43.322283Z",
     "iopub.status.busy": "2024-12-22T10:30:43.321320Z",
     "iopub.status.idle": "2024-12-22T10:30:47.423933Z",
     "shell.execute_reply": "2024-12-22T10:30:47.423140Z"
    },
    "papermill": {
     "duration": 4.118662,
     "end_time": "2024-12-22T10:30:47.426218",
     "exception": false,
     "start_time": "2024-12-22T10:30:43.307556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # Linear algebra\n",
    "import pandas as pd  # Data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611394b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.454066Z",
     "iopub.status.busy": "2024-12-22T10:30:47.453353Z",
     "iopub.status.idle": "2024-12-22T10:30:47.460823Z",
     "shell.execute_reply": "2024-12-22T10:30:47.460004Z"
    },
    "papermill": {
     "duration": 0.023002,
     "end_time": "2024-12-22T10:30:47.462584",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.439582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55c6fc",
   "metadata": {
    "papermill": {
     "duration": 0.01237,
     "end_time": "2024-12-22T10:30:47.487452",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.475082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f91044a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.514303Z",
     "iopub.status.busy": "2024-12-22T10:30:47.513611Z",
     "iopub.status.idle": "2024-12-22T10:30:47.519231Z",
     "shell.execute_reply": "2024-12-22T10:30:47.518389Z"
    },
    "papermill": {
     "duration": 0.021184,
     "end_time": "2024-12-22T10:30:47.521123",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.499939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d681c5c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.547672Z",
     "iopub.status.busy": "2024-12-22T10:30:47.547136Z",
     "iopub.status.idle": "2024-12-22T10:30:47.553148Z",
     "shell.execute_reply": "2024-12-22T10:30:47.552377Z"
    },
    "papermill": {
     "duration": 0.021266,
     "end_time": "2024-12-22T10:30:47.554978",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.533712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_kaggle_working_directory(working_dir):\n",
    "    \"\"\"\n",
    "    Cleans up all files and folders in the specified Kaggle working directory.\n",
    "\n",
    "    Args:\n",
    "        working_dir (str): Path to the Kaggle working directory.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for item in os.listdir(working_dir):\n",
    "        item_path = os.path.join(working_dir, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)  # Remove file or symbolic link\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  # Remove directory\n",
    "            print(f\"Deleted: {item_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bd954",
   "metadata": {
    "papermill": {
     "duration": 0.012598,
     "end_time": "2024-12-22T10:30:47.579996",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.567398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "# **Preprocessing**\n",
    "This section contains the preprocessing functions for CSV and time-series data, as well as merging them.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8412a",
   "metadata": {
    "papermill": {
     "duration": 0.012325,
     "end_time": "2024-12-22T10:30:47.604945",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.592620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Imputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a675a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.631914Z",
     "iopub.status.busy": "2024-12-22T10:30:47.631158Z",
     "iopub.status.idle": "2024-12-22T10:30:47.647871Z",
     "shell.execute_reply": "2024-12-22T10:30:47.646896Z"
    },
    "papermill": {
     "duration": 0.032336,
     "end_time": "2024-12-22T10:30:47.649823",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.617487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute(df, method=\"knn\", n_neighbors=5):\n",
    "    # impute categorical columns\n",
    "    if method == \"cat\":\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "        return df\n",
    "\n",
    "    # get numeric columns instead of 'sii'\n",
    "    numeric_cols = df.select_dtypes(include=['number', 'float64', 'int64']).columns\n",
    "    numeric_cols = list(numeric_cols)\n",
    "    if \"sii\" in numeric_cols:\n",
    "        numeric_cols.remove(\"sii\")\n",
    "    numeric_cols = pd.Index(numeric_cols)\n",
    "    \n",
    "    if method == \"knn\":\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        # impute_data = imputer.fit_transform(df[numeric_cols])\n",
    "    if method == \"mean\":\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "    if method == \"median\":\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "    \n",
    "    imputed_data = imputer.fit_transform(df[numeric_cols])\n",
    "    df_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "    for col in df.columns:\n",
    "        if col not in numeric_cols:\n",
    "            df_imputed[col] = df[col]          \n",
    "    df = df_imputed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc7939",
   "metadata": {
    "papermill": {
     "duration": 0.012883,
     "end_time": "2024-12-22T10:30:47.676842",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.663959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess CSV data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059d2bb",
   "metadata": {
    "papermill": {
     "duration": 0.012462,
     "end_time": "2024-12-22T10:30:47.701919",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.689457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Feature engineering for CSV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966f6d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.729317Z",
     "iopub.status.busy": "2024-12-22T10:30:47.728613Z",
     "iopub.status.idle": "2024-12-22T10:30:47.740466Z",
     "shell.execute_reply": "2024-12-22T10:30:47.739583Z"
    },
    "papermill": {
     "duration": 0.02769,
     "end_time": "2024-12-22T10:30:47.742352",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.714662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "\n",
    "    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n",
    "    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n",
    "    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n",
    "    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n",
    "    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n",
    "    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n",
    "    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n",
    "    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n",
    "    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n",
    "    df['FFMI_Age'] = df['BIA-BIA_FFMI'] * df['Basic_Demos-Age']\n",
    "    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n",
    "    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n",
    "    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b23bff",
   "metadata": {
    "papermill": {
     "duration": 0.012194,
     "end_time": "2024-12-22T10:30:47.767565",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.755371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After we analyzed the dataset, we observed that there are lot of unreasonable values in the dataset. Therefore, we decided to prune some anomaly samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5087d62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.794631Z",
     "iopub.status.busy": "2024-12-22T10:30:47.793858Z",
     "iopub.status.idle": "2024-12-22T10:30:47.802668Z",
     "shell.execute_reply": "2024-12-22T10:30:47.801798Z"
    },
    "papermill": {
     "duration": 0.024454,
     "end_time": "2024-12-22T10:30:47.804538",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.780084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    input_length = len(df)\n",
    "    df = df.drop(df[df['Physical-BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Systolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] > 160].index)\n",
    "\n",
    "    children = df[df['Basic_Demos-Age'] <= 12]\n",
    "    df = df.drop(children[children['FGC-FGC_CU'] > 80].index)\n",
    "    df = df.drop(children[children['FGC-FGC_GSND'] > 80].index)\n",
    "\n",
    "    df = df.drop(df[df['BIA-BIA_BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMC'] > 1000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMR'] > 40000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_DEE'] > 60000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ECW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_FFM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ICW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LDM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LST'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_SMM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_TBW'] > 2000].index)\n",
    "    output_length = len(df)\n",
    "    print (input_length, output_length)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638b352",
   "metadata": {
    "papermill": {
     "duration": 0.012189,
     "end_time": "2024-12-22T10:30:47.829115",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.816926",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Preprocess Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "783c5036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.856494Z",
     "iopub.status.busy": "2024-12-22T10:30:47.855499Z",
     "iopub.status.idle": "2024-12-22T10:30:47.863596Z",
     "shell.execute_reply": "2024-12-22T10:30:47.862629Z"
    },
    "papermill": {
     "duration": 0.024025,
     "end_time": "2024-12-22T10:30:47.865476",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.841451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_csv_data(train_path, test_path, sample_path):\n",
    "    \"\"\"\n",
    "    Preprocess CSV data with proper handling of missing columns.\n",
    "    Args:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        test_path (str): Path to the test CSV file.\n",
    "        sample_path (str): Path to the sample submission CSV file\n",
    "    Returns:\n",
    "        tuple: Preprocessed training DataFrame, test DataFrame, and sample submission.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    sample = pd.read_csv(sample_path)\n",
    "\n",
    "    # # remove outlier from train\n",
    "    # train = remove_outliers(train)\n",
    "    # print(\"Train shape after removing outlier: \", train.shape)\n",
    "\n",
    "    # feature engineering for both train and test\n",
    "    train = csv_feature_engineering(train)\n",
    "    test = csv_feature_engineering(test)\n",
    "    print(\"Train shape after feature engineering: \", train.shape)\n",
    "    print(\"Test shape after feature engineering: \", test.shape)\n",
    "\n",
    "    # Only use columns in both train and test\n",
    "    # Ensure that the columns in `train` and `test` match\n",
    "    # Remove some columns\n",
    "    common_columns = test.columns.to_list() \n",
    "    remove_columns = ['BIA-BIA_LDM', 'Physical-Waist_Circumference', 'FGC-FGC_SRL', 'FGC-FGC_GSND', 'BIA-BIA_ECW', 'Physical-BMI', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_FFM', 'BIA-BIA_TBW', 'BIA-BIA_BMR', 'BIA-BIA_ICW', 'BIA-BIA_DEE']\n",
    "    common_columns = [col for col in common_columns if col not in remove_columns]\n",
    "    \n",
    "    train = train[[\"sii\"] + common_columns]\n",
    "    test = test[common_columns]\n",
    "    print(\"Train shape after removing columns: \", train.shape)\n",
    "    print(\"Test shape after removing columns: \", test.shape)\n",
    "\n",
    "    return train, test, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4fefb",
   "metadata": {
    "papermill": {
     "duration": 0.012337,
     "end_time": "2024-12-22T10:30:47.890591",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.878254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dae3a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.917609Z",
     "iopub.status.busy": "2024-12-22T10:30:47.917288Z",
     "iopub.status.idle": "2024-12-22T10:30:47.940438Z",
     "shell.execute_reply": "2024-12-22T10:30:47.939466Z"
    },
    "papermill": {
     "duration": 0.039288,
     "end_time": "2024-12-22T10:30:47.942373",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.903085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering_ts(worn_data):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the time-series data for a single participant.\n",
    "    Returns a DataFrame with all derived features for this participant.\n",
    "    \"\"\"\n",
    "    mvpa_threshold = 0.1\n",
    "    vig_threshold = 0.5\n",
    "    window_size = 12\n",
    "\n",
    "    # Filter worn data\n",
    "    worn_data = worn_data[worn_data['non-wear_flag'] == 0].copy()\n",
    "\n",
    "    # Time of day conversions\n",
    "    worn_data['time_of_day_hours'] = worn_data['time_of_day'] / 1e9 / 3600\n",
    "    worn_data['day_time'] = worn_data['relative_date_PCIAT'] + (worn_data['time_of_day_hours'] / 24)\n",
    "    worn_data['day_period'] = np.where(\n",
    "        (worn_data['time_of_day_hours'] >= 8) & (worn_data['time_of_day_hours'] < 21),\n",
    "        'day', 'night'\n",
    "    )\n",
    "\n",
    "    # Time differences\n",
    "    worn_data['time_diff'] = (worn_data['day_time'].diff() * 86400).round(0)\n",
    "    worn_data['measurement_after_gap'] = worn_data['time_diff'] > 5\n",
    "\n",
    "    # Classify activity levels\n",
    "    worn_data['activity_type'] = pd.cut(\n",
    "        worn_data['enmo'],\n",
    "        bins=[-np.inf, mvpa_threshold, vig_threshold, np.inf],\n",
    "        labels=['low', 'moderate', 'vigorous']\n",
    "    )\n",
    "\n",
    "    # Aggregate activity periods\n",
    "    activity_group = (\n",
    "        (worn_data['activity_type'] != worn_data['activity_type'].shift()) |\n",
    "        (worn_data['measurement_after_gap'])\n",
    "    ).cumsum()\n",
    "\n",
    "    activity_periods = worn_data.groupby(activity_group).agg(\n",
    "        min=('day_time', 'min'),\n",
    "        max=('day_time', 'max'),\n",
    "        activity_type=('activity_type', 'first')\n",
    "    )\n",
    "    activity_periods['duration_sec'] = (activity_periods['max'] - activity_periods['min']) * 86400 + 5\n",
    "    activity_periods = activity_periods[activity_periods['duration_sec'] >= 60]\n",
    "\n",
    "    # Add day and transition number\n",
    "    activity_periods['day'] = activity_periods['min'].astype(int)\n",
    "    activity_periods['transition_num'] = (\n",
    "        activity_periods.groupby('day')['activity_type']\n",
    "        .apply(lambda x: (x != x.shift()).cumsum())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Calculate activity-level summary statistics\n",
    "    activity_summary = {}\n",
    "    for act_type in ['low', 'moderate']:\n",
    "        activity_data = activity_periods[activity_periods['activity_type'] == act_type]\n",
    "        if activity_data.empty:\n",
    "            for stat in ['median', 'max', 'std']:\n",
    "                activity_summary[f'{act_type}_duration_{stat}'] = 0\n",
    "                activity_summary[f'{act_type}_count_periods_{stat}'] = 0\n",
    "            continue\n",
    "        stats = activity_data.groupby('day').agg(\n",
    "            total_duration=('duration_sec', 'sum'),\n",
    "            count_periods=('duration_sec', 'size')\n",
    "        ).agg(['median', 'max', 'std'])\n",
    "\n",
    "        for stat in ['median', 'max', 'std']:\n",
    "            activity_summary[f'{act_type}_duration_{stat}'] = stats.loc[stat, 'total_duration']\n",
    "            activity_summary[f'{act_type}_count_periods_{stat}'] = stats.loc[stat, 'count_periods']\n",
    "\n",
    "    # Add daily transition statistics\n",
    "    daily_transitions = activity_periods.groupby('day')['transition_num'].max()\n",
    "    trans_stats = daily_transitions.agg(['median', 'max', 'std'])\n",
    "    for stat in ['median', 'max', 'std']:\n",
    "        activity_summary[f'transitions_{stat}'] = trans_stats[stat]\n",
    "\n",
    "    # Hourly activity features\n",
    "    hourly_activity = worn_data.groupby(\n",
    "        [worn_data['relative_date_PCIAT'].astype(int),\n",
    "         worn_data['time_of_day_hours'].astype(int),\n",
    "         worn_data['day_period']]\n",
    "    )['enmo'].agg(['mean', 'max'])\n",
    "\n",
    "    features = hourly_activity['mean'].groupby(\n",
    "        ['relative_date_PCIAT', 'day_period']\n",
    "    ).agg(\n",
    "        std_across_hours='std',\n",
    "        peak_hour=lambda x: x.idxmax()[1] if not x.empty else 0,\n",
    "        entropy=lambda x: -(x / x.sum() * np.log(x / x.sum() + 1e-9)).sum() if not x.empty else 0\n",
    "    )\n",
    "\n",
    "    # Day and night features\n",
    "    if 'day' in features.index.get_level_values('day_period'):\n",
    "        day_features = features.xs('day', level='day_period').agg(['median', 'max', 'std']).stack().to_frame().T\n",
    "        day_features.columns = [f'{stat}_{feature}_day' for stat, feature in day_features.columns]\n",
    "    else:\n",
    "        day_features = pd.DataFrame(columns=[f'{stat}_{feature}_day' for stat in ['median', 'max', 'std'] for feature in ['std_across_hours', 'peak_hour', 'entropy']])\n",
    "\n",
    "    if 'night' in features.index.get_level_values('day_period'):\n",
    "        night_features = features.xs('night', level='day_period').agg(['median', 'max', 'std']).stack().to_frame().T\n",
    "        night_features.columns = [f'{stat}_{feature}_night' for stat, feature in night_features.columns]\n",
    "    else:\n",
    "        night_features = pd.DataFrame(columns=[f'{stat}_{feature}_night' for stat in ['median', 'max', 'std'] for feature in ['std_across_hours', 'peak_hour', 'entropy']])\n",
    "\n",
    "    def merge_mvpa_groups(df, allowed_gap=60, merge_gap=60):\n",
    "        last_mvpa_time = df['day_time'].where(df['is_mvpa']).ffill().shift()\n",
    "        mvpa_time_diff = ((df['day_time'] - last_mvpa_time) * 86400).round(0)\n",
    "        mvpa_group = (\n",
    "            (df['is_mvpa'] != df['is_mvpa'].shift()) |\n",
    "            (df['time_diff'] >= allowed_gap)\n",
    "        ).cumsum()\n",
    "        is_mvpa_start = (\n",
    "            (mvpa_group != mvpa_group.shift()) &\n",
    "            df['is_mvpa']\n",
    "        )\n",
    "        group_increment = is_mvpa_start & (\n",
    "            (mvpa_time_diff >= merge_gap) | last_mvpa_time.isnull()\n",
    "        )\n",
    "        merged_group = group_increment.cumsum()\n",
    "        merged_group.loc[~df['is_mvpa']] = np.nan\n",
    "        return merged_group\n",
    "    \n",
    "    # Calculate daily MVPA statistics\n",
    "    worn_data['is_mvpa'] = worn_data['enmo'] > mvpa_threshold\n",
    "    worn_data['mvpa_merged_group'] = merge_mvpa_groups(worn_data)\n",
    "\n",
    "    mvpa_periods = worn_data[worn_data['is_mvpa']].groupby('mvpa_merged_group')['day_time'].agg(['min', 'max'])\n",
    "    mvpa_periods['duration_sec'] = (mvpa_periods['max'] - mvpa_periods['min']) * 86400\n",
    "    mvpa_periods = mvpa_periods[mvpa_periods['duration_sec'] >= 60]\n",
    "\n",
    "    mvpa_periods['day'] = mvpa_periods['min'].astype(int)\n",
    "    daily_stats = mvpa_periods.groupby('day').agg(\n",
    "        total_duration=('duration_sec', 'sum'),\n",
    "        count_periods=('duration_sec', 'size')\n",
    "    )\n",
    "\n",
    "    # Extract daily stats features\n",
    "    daily_stats_features = daily_stats.agg(\n",
    "        ['median', 'max', 'std']\n",
    "    ).unstack().to_frame().T\n",
    "\n",
    "    daily_stats_features.columns = [\n",
    "        f'{stat}_{feature}' for feature, stat in daily_stats_features.columns\n",
    "    ]\n",
    "\n",
    "    # Combine all features\n",
    "    combined_features = pd.concat(\n",
    "        [pd.DataFrame([activity_summary]), day_features, night_features, daily_stats_features],\n",
    "        axis=1\n",
    "    )\n",
    "    # combined_features.fillna(0, inplace=True)  # Ensure no missing values\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "083dcfe5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:47.969341Z",
     "iopub.status.busy": "2024-12-22T10:30:47.968686Z",
     "iopub.status.idle": "2024-12-22T10:30:47.976923Z",
     "shell.execute_reply": "2024-12-22T10:30:47.976013Z"
    },
    "papermill": {
     "duration": 0.023702,
     "end_time": "2024-12-22T10:30:47.978909",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.955207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file_and_engineer_features(filename, dirname, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single participant's Parquet file, apply feature engineering, and save features to disk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Parquet file\n",
    "        file_path = os.path.join(dirname, filename, 'part-0.parquet')\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Add participant ID\n",
    "        participant_id = filename.split('=')[1]\n",
    "        df['id'] = participant_id\n",
    "\n",
    "        # Apply feature engineering\n",
    "        features = feature_engineering_ts(df)\n",
    "\n",
    "        # Add participant ID to features\n",
    "        features['id'] = participant_id\n",
    "\n",
    "        # Save the engineered features to disk\n",
    "        output_file = os.path.join(output_dir, f\"{participant_id}_features.parquet\")\n",
    "        features.to_parquet(output_file, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "\n",
    "def load_time_series(dirname, output_dir) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all Parquet files, save engineered features to disk, and return the combined features.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process participants one by one\n",
    "    for filename in tqdm(os.listdir(dirname), desc=\"Processing participants\"):\n",
    "        process_file_and_engineer_features(filename, dirname, output_dir)\n",
    "\n",
    "    # Combine all saved features into a single DataFrame\n",
    "    feature_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\"_features.parquet\")]\n",
    "    combined_features = pd.concat([pd.read_parquet(f) for f in tqdm(feature_files, desc=\"Combining features\")], ignore_index=True)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890b5de",
   "metadata": {
    "papermill": {
     "duration": 0.012529,
     "end_time": "2024-12-22T10:30:48.003997",
     "exception": false,
     "start_time": "2024-12-22T10:30:47.991468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d550ee1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.031499Z",
     "iopub.status.busy": "2024-12-22T10:30:48.030780Z",
     "iopub.status.idle": "2024-12-22T10:30:48.049617Z",
     "shell.execute_reply": "2024-12-22T10:30:48.048728Z"
    },
    "papermill": {
     "duration": 0.034369,
     "end_time": "2024-12-22T10:30:48.051511",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.017142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sparse Autoencoder\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, sparsity_weight=1e-5):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()  # Outputs in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_data(data, scaler_type=\"MinMaxScaler\"):\n",
    "    \"\"\"\n",
    "    Prepares data for model training, ensuring only numeric columns are scaled.\n",
    "    Returns PyTorch tensor.\n",
    "    \"\"\"\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    if scaler_type == \"RobustScaler\":\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numeric columns\n",
    "    data_scaled = scaler.fit_transform(data[numeric_cols])\n",
    "    return torch.tensor(data_scaled, dtype=torch.float32), scaler\n",
    "\n",
    "\n",
    "# PCA function\n",
    "def apply_pca(data, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    return data_pca, pca\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "# Training function\n",
    "def perform_sparse_autoencoder(data, epochs=100, batch_size=32, learning_rate=0.001, patience=10, scaler_type=\"MinMaxScaler\", use_pca=False, sparsity_weight=1e-5):\n",
    "    # Preprocess data\n",
    "    if use_pca:\n",
    "        data, pca = apply_pca(data)\n",
    "\n",
    "    data_tensor, scaler = prepare_data(data, scaler_type=scaler_type)\n",
    "    train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Ensure data is PyTorch tensors\n",
    "    assert isinstance(train_data, torch.Tensor), \"train_data must be a PyTorch tensor\"\n",
    "    assert isinstance(val_data, torch.Tensor), \"val_data must be a PyTorch tensor\"\n",
    "\n",
    "    # DataLoader setup\n",
    "    train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = SparseAutoencoder(input_dim=data_tensor.shape[1], sparsity_weight=sparsity_weight)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, outputs = model(batch)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Sparsity penalty\n",
    "            l1_penalty = torch.mean(torch.abs(encoded))\n",
    "            loss += sparsity_weight * l1_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[0].to(device)\n",
    "                _, outputs = model(batch)\n",
    "                loss = criterion(outputs, batch)\n",
    "                val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        stopper(val_loss)\n",
    "        if stopper.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Return encoded features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = model(data_tensor.to(device))\n",
    "    encoded_features = encoded_features.cpu().numpy()\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=[f\"feature_{i}\" for i in range(encoded_features.shape[1])])\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f03f72ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.078112Z",
     "iopub.status.busy": "2024-12-22T10:30:48.077784Z",
     "iopub.status.idle": "2024-12-22T10:30:48.090245Z",
     "shell.execute_reply": "2024-12-22T10:30:48.089368Z"
    },
    "papermill": {
     "duration": 0.027962,
     "end_time": "2024-12-22T10:30:48.092173",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.064211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    # Keep only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    # Scale the numeric data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_numeric)\n",
    "    \n",
    "    # Convert to a PyTorch tensor\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    # Define the autoencoder model\n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    # Set up the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    # Extract encoded data\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    # Return the encoded data as a DataFrame\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0f6e1",
   "metadata": {
    "papermill": {
     "duration": 0.012806,
     "end_time": "2024-12-22T10:30:48.118438",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.105632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Preprocess function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55155629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.146014Z",
     "iopub.status.busy": "2024-12-22T10:30:48.145297Z",
     "iopub.status.idle": "2024-12-22T10:30:48.152155Z",
     "shell.execute_reply": "2024-12-22T10:30:48.151266Z"
    },
    "papermill": {
     "duration": 0.022452,
     "end_time": "2024-12-22T10:30:48.154042",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.131590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_time_series_data(train_ts_path, test_ts_path, use_autoencoder=False, use_imputer=False, impute_method=\"mean\", outdir='/kaggle/working/intermediate_results'):\n",
    "    \"\"\"\n",
    "    Preprocess time-series data, including feature engineering and optional autoencoder-based encoding.\n",
    "    Args:\n",
    "        train_ts_path (str): Path to training time-series data.\n",
    "        test_ts_path (str): Path to test time-series data.\n",
    "        use_autoencoder (bool): Whether to encode features with a sparse autoencoder.\n",
    "        use_imputer (bool): Whether to impute missing values.\n",
    "        impute_method (str): Imputation method (\"mean\", \"knn\", etc.).\n",
    "    Returns:\n",
    "        tuple: Preprocessed training and test time-series DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_ts = load_time_series(train_ts_path, '/kaggle/working/final')\n",
    "    test_ts = load_time_series(test_ts_path, '/kaggle/working/final_test')\n",
    "\n",
    "    # # Apply feature engineering\n",
    "    # train_ts = feature_engineering_ts(train_ts)\n",
    "    # test_ts = feature_engineering_ts(test_ts)\n",
    "\n",
    "    # Impute missing values\n",
    "    if use_imputer:\n",
    "        train_ts = impute(train_ts, method=impute_method)\n",
    "        test_ts = impute(test_ts, method=impute_method)\n",
    "\n",
    "    # Encode features with a sparse autoencoder\n",
    "    if use_autoencoder:\n",
    "        train_ts_encoded = perform_autoencoder(\n",
    "            train_ts, encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        test_ts_encoded = perform_autoencoder(\n",
    "            test_ts, \n",
    "            encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        train_ts_encoded['id'] = train_ts[\"id\"]\n",
    "        test_ts_encoded['id'] = test_ts[\"id\"]\n",
    "\n",
    "        return train_ts_encoded, test_ts_encoded\n",
    "\n",
    "    return train_ts, test_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa914493",
   "metadata": {
    "papermill": {
     "duration": 0.012571,
     "end_time": "2024-12-22T10:30:48.179270",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.166699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Merge CSV and time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f51dc4ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.206496Z",
     "iopub.status.busy": "2024-12-22T10:30:48.205854Z",
     "iopub.status.idle": "2024-12-22T10:30:48.215406Z",
     "shell.execute_reply": "2024-12-22T10:30:48.214421Z"
    },
    "papermill": {
     "duration": 0.024986,
     "end_time": "2024-12-22T10:30:48.217243",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.192257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=False, use_numeric_imputation=False, numeric_impute_method=\"knn\"):\n",
    "    \"\"\"\n",
    "    Merge CSV and time-series data into a unified dataset and fill missing values using KNNImputer.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data from CSV.\n",
    "        test (pd.DataFrame): Test data from CSV.\n",
    "        train_ts (pd.DataFrame): Aggregated training time-series data.\n",
    "        test_ts (pd.DataFrame): Aggregated test time-series data.\n",
    "        use_time_series (bool): Whether to include time-series data in the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Merged training and test DataFrames.\n",
    "    \"\"\"\n",
    "    featuresCols = train.columns.to_list()\n",
    "    if use_time_series:\n",
    "        # Merge time-series data with train and test data on 'id'\n",
    "        train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "        test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "    # Drop 'id' column after merging\n",
    "    train = train.drop('id', axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    if use_time_series:\n",
    "        # Feature selection\n",
    "        time_series_cols = train_ts.columns.tolist()\n",
    "        time_series_cols.remove(\"id\")\n",
    "\n",
    "    if np.any(np.isinf(train)):\n",
    "        train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if np.any(np.isinf(test)):\n",
    "        test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if use_numeric_imputation:\n",
    "        train = impute(train, method=numeric_impute_method)\n",
    "        test = impute(test, method=numeric_impute_method)\n",
    "\n",
    "    if use_time_series:\n",
    "        featuresCols += time_series_cols\n",
    "\n",
    "    # Dynamically filter features based on available columns\n",
    "    featuresCols = [col for col in featuresCols if col in train.columns]\n",
    "    print(\"Final features included in train:\", featuresCols)\n",
    "\n",
    "    # Filter features and drop rows with missing target 'sii'\n",
    "    train = train[featuresCols]\n",
    "    train = train.dropna(subset=['sii'])\n",
    "\n",
    "    featuresCols.remove('sii')\n",
    "    test = test[featuresCols]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988a44f4",
   "metadata": {
    "papermill": {
     "duration": 0.012344,
     "end_time": "2024-12-22T10:30:48.242080",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.229736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"hyperparameter-tuning\"></a>\n",
    "# **Hyperparameter Tuning with Optuna**\n",
    "This section includes functions to optimize hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3b568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:20:30.493712Z",
     "iopub.status.busy": "2024-12-17T14:20:30.492829Z",
     "iopub.status.idle": "2024-12-17T14:20:30.519198Z",
     "shell.execute_reply": "2024-12-17T14:20:30.517978Z",
     "shell.execute_reply.started": "2024-12-17T14:20:30.493676Z"
    },
    "papermill": {
     "duration": 0.012331,
     "end_time": "2024-12-22T10:30:48.266907",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.254576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"lightgbm\"></a>\n",
    "### **LightGBM Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e6c5a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.293956Z",
     "iopub.status.busy": "2024-12-22T10:30:48.293602Z",
     "iopub.status.idle": "2024-12-22T10:30:48.302426Z",
     "shell.execute_reply": "2024-12-22T10:30:48.301527Z"
    },
    "papermill": {
     "duration": 0.024769,
     "end_time": "2024-12-22T10:30:48.304345",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.279576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_lightgbm(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n",
    "            \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "            \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "            \"random_state\": 42,\n",
    "            \"n_estimators\": 200\n",
    "        }\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\"\n",
    "            # early_stopping_rounds=50\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for LightGBM:\", study.best_value)\n",
    "    print(\"Best Params for LightGBM:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456b2f9",
   "metadata": {
    "papermill": {
     "duration": 0.015378,
     "end_time": "2024-12-22T10:30:48.332622",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.317244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "### **XGBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1326b25f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.360096Z",
     "iopub.status.busy": "2024-12-22T10:30:48.359743Z",
     "iopub.status.idle": "2024-12-22T10:30:48.368821Z",
     "shell.execute_reply": "2024-12-22T10:30:48.367953Z"
    },
    "papermill": {
     "duration": 0.025375,
     "end_time": "2024-12-22T10:30:48.370684",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.345309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_xgboost(train, target, n_trials=50):\n",
    "    import optuna\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    def is_gpu_available():\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    use_gpu = is_gpu_available()\n",
    "    tree_method = \"gpu_hist\" if use_gpu else \"hist\"\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-5, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-5, 10.0),\n",
    "            \"tree_method\": tree_method,\n",
    "        }\n",
    "\n",
    "        # Preprocess the training data to remove non-numeric columns\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\",\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "\n",
    "        # Predict and evaluate\n",
    "        preds = model.predict(X_valid)\n",
    "        rounded_preds = np.round(preds).astype(int)  # Ensure preds are rounded here\n",
    "        score = quadratic_weighted_kappa(y_valid, rounded_preds)\n",
    "        return score\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for XGBoost:\", study.best_value)\n",
    "    print(\"Best Params for XGBoost:\", study.best_params)\n",
    "\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5d20b",
   "metadata": {
    "papermill": {
     "duration": 0.012183,
     "end_time": "2024-12-22T10:30:48.395482",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.383299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"catboost\"></a>\n",
    "### **CatBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "facfe24a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.421892Z",
     "iopub.status.busy": "2024-12-22T10:30:48.421569Z",
     "iopub.status.idle": "2024-12-22T10:30:48.429379Z",
     "shell.execute_reply": "2024-12-22T10:30:48.428502Z"
    },
    "papermill": {
     "duration": 0.023298,
     "end_time": "2024-12-22T10:30:48.431230",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.407932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_catboost(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n",
    "            \"iterations\": 200,\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for CatBoost:\", study.best_value)\n",
    "    print(\"Best Params for CatBoost:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfafdb3",
   "metadata": {
    "papermill": {
     "duration": 0.012295,
     "end_time": "2024-12-22T10:30:48.456200",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.443905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Prepare Model Optimized Hyperparameter**\n",
    "Create a dictionary of models' best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc59b8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.482794Z",
     "iopub.status.busy": "2024-12-22T10:30:48.482426Z",
     "iopub.status.idle": "2024-12-22T10:30:48.488178Z",
     "shell.execute_reply": "2024-12-22T10:30:48.487347Z"
    },
    "papermill": {
     "duration": 0.021694,
     "end_time": "2024-12-22T10:30:48.490365",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.468671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def models_best_params(use_lightgbm=False, use_xgboost=False, use_catboost= False):\n",
    "    best_params_dict = {}\n",
    "    if use_lightgbm:\n",
    "        best_params_lgbm = optimize_lightgbm(train, target, n_trials=50)\n",
    "        best_params_dict['LightGBM'] = best_params_lgbm\n",
    "    if use_xgboost:\n",
    "        best_params_xgb = optimize_xgboost(train, target, n_trials=50)\n",
    "        best_params_dict['XGBoost'] = best_params_xgb\n",
    "    if use_catboost:\n",
    "        best_params_catboost = optimize_catboost(train, target, n_trials=50)\n",
    "        best_params_dict['CatBoost'] = best_params_catboost\n",
    "    return best_params_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251157a",
   "metadata": {
    "papermill": {
     "duration": 0.012435,
     "end_time": "2024-12-22T10:30:48.515591",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.503156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **FT-Transformer wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bdfe604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.542885Z",
     "iopub.status.busy": "2024-12-22T10:30:48.542570Z",
     "iopub.status.idle": "2024-12-22T10:30:48.561104Z",
     "shell.execute_reply": "2024-12-22T10:30:48.560177Z"
    },
    "papermill": {
     "duration": 0.034561,
     "end_time": "2024-12-22T10:30:48.562901",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.528340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "class FTTransformerWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, \n",
    "                 numerical_features=None,\n",
    "                 categorical_features=None,\n",
    "                 embedding_dim=64,\n",
    "                 depth=4,\n",
    "                 heads=8,\n",
    "                 attn_dropout=0.3,\n",
    "                 ff_dropout=0.3,\n",
    "                 out_dim=1,\n",
    "                 out_activation='relu',\n",
    "                 lr=0.001,\n",
    "                 weight_decay=0.0001,\n",
    "                 loss=None,\n",
    "                 metrics=None,\n",
    "                 epochs=1000,\n",
    "                 patience=10,\n",
    "                 batch_size=1024,\n",
    "                 shuffle=True,\n",
    "                 seed=42):  # Add seed parameter\n",
    "        # Model hyperparameters\n",
    "        self.numerical_features = numerical_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ff_dropout = ff_dropout\n",
    "        self.out_dim = out_dim\n",
    "        self.out_activation = out_activation\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss = loss or tf.keras.losses.MeanSquaredError()\n",
    "        self.metrics = metrics or [tf.keras.metrics.RootMeanSquaredError()]\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed  # Save seed\n",
    "        \n",
    "        # Internal objects\n",
    "        self.imputer = SimpleImputer(strategy='median')  # Handle missing values\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        self._set_random_seed(self.seed)\n",
    "    \n",
    "    def _set_random_seed(self, seed):\n",
    "        \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Create the encoder\n",
    "        encoder = FTTransformerEncoder(\n",
    "            numerical_features=self.numerical_features,\n",
    "            categorical_features=self.categorical_features,\n",
    "            numerical_data=None,  # Placeholder, as data is processed separately\n",
    "            categorical_data=None,  # Placeholder\n",
    "            y=None,\n",
    "            numerical_embedding_type='linear',\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            depth=self.depth,\n",
    "            heads=self.heads,\n",
    "            attn_dropout=self.attn_dropout,\n",
    "            ff_dropout=self.ff_dropout,\n",
    "            explainable=True,\n",
    "        )\n",
    "        \n",
    "        # Create the complete model\n",
    "        model = FTTransformer(\n",
    "            encoder=encoder,\n",
    "            out_dim=self.out_dim,\n",
    "            out_activation=self.out_activation,\n",
    "        )\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = tfa.optimizers.AdamW(\n",
    "            learning_rate=self.lr, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=self.loss,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._set_random_seed(self.seed)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        \n",
    "        \n",
    "        # Split data into train/validation sets with fixed seed\n",
    "        train_data, val_data, train_target, val_target = train_test_split(\n",
    "            X_imputed, y, test_size=0.2, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        train_data = X_imputed\n",
    "        train_target = y\n",
    "\n",
    "        train_data_df = pd.DataFrame(train_data, columns = self.numerical_features)\n",
    "        train_target_df = pd.DataFrame(train_target, columns=['sii'])\n",
    "        \n",
    "        train_data = pd.concat([train_data_df, train_target_df], axis=1)\n",
    "\n",
    "        val_data_df = pd.DataFrame(val_data, columns = self.numerical_features)\n",
    "        val_target_df = pd.DataFrame(val_target, columns=['sii'])\n",
    "        \n",
    "        val_data = pd.concat([val_data_df, val_target_df], axis=1)\n",
    "           \n",
    "        # Use df_to_dataset to create TensorFlow datasets\n",
    "        train_dataset = df_to_dataset(train_data, 'sii', shuffle=self.shuffle, batch_size=self.batch_size)\n",
    "        val_dataset = df_to_dataset(val_data, 'sii', shuffle=False, batch_size=self.batch_size)\n",
    "        \n",
    "        # Build and train the model\n",
    "        self.model = self._build_model()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", mode=\"min\", patience=self.patience\n",
    "        )\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            train_dataset,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self._set_random_seed(self.seed)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "\n",
    "        X_df = pd.DataFrame(X_imputed, columns = self.numerical_features)\n",
    "        \n",
    "        # Convert X to dataset using df_to_dataset\n",
    "        predict_dataset = df_to_dataset(X_df, shuffle=False, batch_size=self.batch_size)\n",
    "        \n",
    "        # Predict using the trained model\n",
    "        return self.model.predict(predict_dataset)['output'].flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2fd60",
   "metadata": {
    "papermill": {
     "duration": 0.017587,
     "end_time": "2024-12-22T10:30:48.593166",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.575579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **TabNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ea5ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.620959Z",
     "iopub.status.busy": "2024-12-22T10:30:48.620341Z",
     "iopub.status.idle": "2024-12-22T10:30:48.646714Z",
     "shell.execute_reply": "2024-12-22T10:30:48.645960Z"
    },
    "papermill": {
     "duration": 0.042392,
     "end_time": "2024-12-22T10:30:48.648668",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.606276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New: TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67504569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:22:26.997103Z",
     "iopub.status.busy": "2024-12-17T14:22:26.996719Z",
     "iopub.status.idle": "2024-12-17T14:22:27.003164Z",
     "shell.execute_reply": "2024-12-17T14:22:27.001924Z",
     "shell.execute_reply.started": "2024-12-17T14:22:26.997069Z"
    },
    "papermill": {
     "duration": 0.012413,
     "end_time": "2024-12-22T10:30:48.674092",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.661679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"model-training\"></a>\n",
    "# **Model Training**\n",
    "Train multiple models with the optimized hyperparameters.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56ebd8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.701426Z",
     "iopub.status.busy": "2024-12-22T10:30:48.701085Z",
     "iopub.status.idle": "2024-12-22T10:30:48.716298Z",
     "shell.execute_reply": "2024-12-22T10:30:48.715040Z"
    },
    "papermill": {
     "duration": 0.032571,
     "end_time": "2024-12-22T10:30:48.719189",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.686618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method=None, weights=None, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train models using cross-validation, optionally using Voting or Stacking.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data.\n",
    "        test (pd.DataFrame): Test data.\n",
    "        target (pd.Series): Target variable.\n",
    "        best_params_dict (dict): Optimized hyperparameters for each model.\n",
    "        ensemble_method (str): 'voting', 'stacking', or None for individual models.\n",
    "        n_splits (int): Number of CV splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: Aggregated predictions for each model or ensemble.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    # target_binned = np.digitize(target, bins=np.linspace(target.min(), target.max(), 5))\n",
    "    trained_models = {}\n",
    "\n",
    "    models = []\n",
    "    for model_name, params in best_params_dict.items():\n",
    "        if model_name == \"LightGBM\":\n",
    "            model = LGBMRegressor(**params, verbose=-1)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**params)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model = CatBoostRegressor(**params)\n",
    "        elif model_name == \"FTTransformer\":\n",
    "            model = FTTransformerWrapper(**params)\n",
    "        elif model_name == \"TabNet\":\n",
    "            model = TabNetWrapper(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        models.append((model_name, model))\n",
    "\n",
    "    if ensemble_method == 'voting':\n",
    "        if weights:\n",
    "            ensemble_model = VotingRegressor(estimators=models, weights=weights)\n",
    "        # Create a Voting Regressor\n",
    "        else:\n",
    "            ensemble_model = VotingRegressor(estimators=models)\n",
    "    elif ensemble_method == 'stacking':\n",
    "        # Create a Stacking Regressor\n",
    "        ensemble_model = StackingRegressor(estimators=models, final_estimator=LinearRegression())\n",
    "\n",
    "    for model_name, model in ([('Ensemble', ensemble_model)] if ensemble_method else models):\n",
    "        print(f\"Training model with CV: {model_name}\")\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        # fold_predictions = np.zeros(test.shape[0])\n",
    "\n",
    "        train_S = []\n",
    "        test_S = []\n",
    "        \n",
    "        oof_non_rounded = np.zeros(len(target), dtype=float) \n",
    "        oof_rounded = np.zeros(len(target), dtype=int) \n",
    "        test_preds = np.zeros((len(test), n_splits))\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(train, target)):\n",
    "            print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "            X_train, X_valid = train.iloc[train_idx], train.iloc[test_idx]\n",
    "            y_train, y_valid = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_valid)\n",
    "\n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "    \n",
    "            train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "            val_kappa = quadratic_weighted_kappa(y_valid, y_val_pred_rounded)\n",
    "    \n",
    "            train_S.append(train_kappa)\n",
    "            test_S.append(val_kappa)\n",
    "            \n",
    "            test_preds[:, fold] = model.predict(test)\n",
    "            \n",
    "            print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "            # clear_output(wait=True)\n",
    "\n",
    "        print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "        print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(target, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "\n",
    "        print(\"KappaOPtimizer.x =\",  KappaOPtimizer.x)\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(target, oof_tuned)\n",
    "    \n",
    "        print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n",
    "    \n",
    "        tpm = test_preds.mean(axis=1)\n",
    "        tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "        predictions[model_name] = tpTuned\n",
    "        trained_models[model_name] = model\n",
    "\n",
    "    return predictions, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e54634",
   "metadata": {
    "papermill": {
     "duration": 0.017376,
     "end_time": "2024-12-22T10:30:48.754732",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.737356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"submission\"></a>\n",
    "# **Submission**\n",
    "Generate and save the final submission file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d243be9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.793162Z",
     "iopub.status.busy": "2024-12-22T10:30:48.792407Z",
     "iopub.status.idle": "2024-12-22T10:30:48.799333Z",
     "shell.execute_reply": "2024-12-22T10:30:48.798250Z"
    },
    "papermill": {
     "duration": 0.027074,
     "end_time": "2024-12-22T10:30:48.801463",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.774389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_submission_file(predictions, sample):\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"sii\": np.round(predictions).astype(int)\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved!\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0338b14",
   "metadata": {
    "papermill": {
     "duration": 0.012961,
     "end_time": "2024-12-22T10:30:48.828274",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.815313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"main-pipeline\"></a>\n",
    "# **Main Pipeline**\n",
    "Run the entire pipeline.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f1ad5",
   "metadata": {
    "papermill": {
     "duration": 0.012709,
     "end_time": "2024-12-22T10:30:48.853468",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.840759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b52b345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:30:48.880860Z",
     "iopub.status.busy": "2024-12-22T10:30:48.880104Z",
     "iopub.status.idle": "2024-12-22T10:48:30.542190Z",
     "shell.execute_reply": "2024-12-22T10:48:30.540876Z"
    },
    "papermill": {
     "duration": 1061.678282,
     "end_time": "2024-12-22T10:48:30.544379",
     "exception": false,
     "start_time": "2024-12-22T10:30:48.866097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after feature engineering:  (3960, 99)\n",
      "Test shape after feature engineering:  (20, 77)\n",
      "Train shape after removing columns:  (3960, 65)\n",
      "Test shape after removing columns:  (20, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 100%|| 996/996 [17:16<00:00,  1.04s/it]\n",
      "Combining features: 100%|| 996/996 [00:04<00:00, 216.41it/s]\n",
      "Processing participants: 100%|| 2/2 [00:00<00:00,  2.76it/s]\n",
      "Combining features: 100%|| 2/2 [00:00<00:00, 199.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.2930]\n",
      "Epoch [20/100], Loss: 0.2688]\n",
      "Epoch [30/100], Loss: 0.2607]\n",
      "Epoch [40/100], Loss: 0.2589]\n",
      "Epoch [50/100], Loss: 0.2568]\n",
      "Epoch [60/100], Loss: 0.2552]\n",
      "Epoch [70/100], Loss: 0.2538]\n",
      "Epoch [80/100], Loss: 0.2535]\n",
      "Epoch [90/100], Loss: 0.2520]\n",
      "Epoch [100/100], Loss: 0.2533]\n",
      "Epoch [10/100], Loss: 1.1297]\n",
      "Epoch [20/100], Loss: 0.8606]\n",
      "Epoch [30/100], Loss: 0.5252]\n",
      "Epoch [40/100], Loss: 0.4616]\n",
      "Epoch [50/100], Loss: 0.4615]\n",
      "Epoch [60/100], Loss: 0.4615]\n",
      "Epoch [70/100], Loss: 0.4615]\n",
      "Epoch [80/100], Loss: 0.4615]\n",
      "Epoch [90/100], Loss: 0.4615]\n",
      "Epoch [100/100], Loss: 0.4615]\n",
      "Final features included in train: ['sii', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-Height', 'Physical-Weight', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age', 'BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight', 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight', 'Sex_BMI', 'Sex_HeartRate', 'Age_WaistCirc', 'BMI_FitnessMaxStage', 'Weight_GripStrengthDominant', 'Weight_GripStrengthNonDominant', 'HeartRate_FitnessTime', 'Age_PushUp', 'FFMI_Age', 'InternetUse_SleepDisturbance', 'CGAS_BMI', 'CGAS_FitnessMaxStage', 'Enc_1', 'Enc_2', 'Enc_3', 'Enc_4', 'Enc_5', 'Enc_6', 'Enc_7', 'Enc_8', 'Enc_9', 'Enc_10', 'Enc_11', 'Enc_12', 'Enc_13', 'Enc_14', 'Enc_15', 'Enc_16', 'Enc_17', 'Enc_18', 'Enc_19', 'Enc_20', 'Enc_21', 'Enc_22', 'Enc_23', 'Enc_24', 'Enc_25', 'Enc_26', 'Enc_27', 'Enc_28', 'Enc_29', 'Enc_30', 'Enc_31', 'Enc_32', 'Enc_33', 'Enc_34', 'Enc_35', 'Enc_36', 'Enc_37', 'Enc_38', 'Enc_39', 'Enc_40', 'Enc_41', 'Enc_42', 'Enc_43', 'Enc_44', 'Enc_45', 'Enc_46', 'Enc_47', 'Enc_48', 'Enc_49', 'Enc_50', 'Enc_51', 'Enc_52', 'Enc_53', 'Enc_54', 'Enc_55', 'Enc_56', 'Enc_57', 'Enc_58', 'Enc_59', 'Enc_60']\n",
      "Deleted: /kaggle/working/__notebook__.ipynb\n",
      "Deleted: /kaggle/working/final\n",
      "Deleted: /kaggle/working/final_test\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "n_folds = 5\n",
    "\n",
    "# Set up logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Limit verbosity\n",
    "\n",
    "# Paths\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "sample_path = '/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv'\n",
    "time_series_train = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n",
    "time_series_test = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n",
    "\n",
    "# Toggle for using time-series data\n",
    "use_time_series = True  # Set to False to skip time-series data\n",
    "\n",
    "# Preprocess data\n",
    "train, test, sample = preprocess_csv_data(train_path, test_path, sample_path)\n",
    "\n",
    "if use_time_series:\n",
    "    train_ts, test_ts = preprocess_time_series_data(time_series_train, time_series_test, use_autoencoder=True, use_imputer=True, impute_method=\"mean\")\n",
    "else:\n",
    "    train_ts, test_ts = None, None\n",
    "train, test = merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=use_time_series, use_numeric_imputation=True, numeric_impute_method=\"knn\")\n",
    "\n",
    "train = train.dropna(subset=['sii'])\n",
    "\n",
    "# Target and features\n",
    "target = train[\"sii\"]\n",
    "train = train.drop(columns=[\"sii\"])  # Drop `sii` from features\n",
    "\n",
    "\n",
    "# Ensure `sii` is not in test data\n",
    "if \"sii\" in test.columns:\n",
    "    test = test.drop(columns=[\"sii\"])\n",
    "\n",
    "if \"id\" in train.columns:\n",
    "    train = train.drop(columns=[\"id\"])\n",
    "    test = test.drop(columns=[\"id\"])\n",
    "\n",
    "working_dir = '/kaggle/working'\n",
    "clean_kaggle_working_directory(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2567afe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:30.658317Z",
     "iopub.status.busy": "2024-12-22T10:48:30.657946Z",
     "iopub.status.idle": "2024-12-22T10:48:30.662509Z",
     "shell.execute_reply": "2024-12-22T10:48:30.661731Z"
    },
    "papermill": {
     "duration": 0.063908,
     "end_time": "2024-12-22T10:48:30.664382",
     "exception": false,
     "start_time": "2024-12-22T10:48:30.600474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train2, test2, target2 = train, test, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d8f2fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:30.782653Z",
     "iopub.status.busy": "2024-12-22T10:48:30.781818Z",
     "iopub.status.idle": "2024-12-22T10:48:30.788085Z",
     "shell.execute_reply": "2024-12-22T10:48:30.787112Z"
    },
    "papermill": {
     "duration": 0.065756,
     "end_time": "2024-12-22T10:48:30.790036",
     "exception": false,
     "start_time": "2024-12-22T10:48:30.724280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2736, 123)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d49d3ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:30.905350Z",
     "iopub.status.busy": "2024-12-22T10:48:30.904454Z",
     "iopub.status.idle": "2024-12-22T10:48:30.915501Z",
     "shell.execute_reply": "2024-12-22T10:48:30.912730Z"
    },
    "papermill": {
     "duration": 0.070842,
     "end_time": "2024-12-22T10:48:30.917395",
     "exception": false,
     "start_time": "2024-12-22T10:48:30.846553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "TARGET_FEATURE = 'sii'\n",
    "ID_COLUMN = 'id'\n",
    "\n",
    "# Automatically infer numeric and categorical features\n",
    "NUMERIC_FEATURES = train.select_dtypes(include=['int64', 'float64']).columns.difference([TARGET_FEATURE, ID_COLUMN]).tolist()\n",
    "CATEGORICAL_FEATURES = train.select_dtypes(include=['object', 'category']).columns.difference([TARGET_FEATURE, ID_COLUMN]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad85cbd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:31.033257Z",
     "iopub.status.busy": "2024-12-22T10:48:31.032494Z",
     "iopub.status.idle": "2024-12-22T10:48:31.038589Z",
     "shell.execute_reply": "2024-12-22T10:48:31.037464Z"
    },
    "papermill": {
     "duration": 0.067164,
     "end_time": "2024-12-22T10:48:31.040541",
     "exception": false,
     "start_time": "2024-12-22T10:48:30.973377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "637fa200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:31.155327Z",
     "iopub.status.busy": "2024-12-22T10:48:31.154945Z",
     "iopub.status.idle": "2024-12-22T10:48:36.393958Z",
     "shell.execute_reply": "2024-12-22T10:48:36.392915Z"
    },
    "papermill": {
     "duration": 5.299829,
     "end_time": "2024-12-22T10:48:36.396315",
     "exception": false,
     "start_time": "2024-12-22T10:48:31.096486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old ver\n",
    "FTTransformer_Params = {\n",
    "    'numerical_features': NUMERIC_FEATURES,\n",
    "    'categorical_features': CATEGORICAL_FEATURES,\n",
    "    'embedding_dim': 64,\n",
    "    'depth': 4,\n",
    "    'heads': 8,\n",
    "    'attn_dropout': 0.4,\n",
    "    'ff_dropout': 0.4,\n",
    "    'out_dim': 1,\n",
    "    'out_activation': 'relu',\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss': tf.keras.losses.MeanSquaredError(),\n",
    "    'metrics': [tf.keras.metrics.RootMeanSquaredError()],\n",
    "    'epochs': 100,\n",
    "    'patience': 5,\n",
    "    'batch_size': 256,\n",
    "    'shuffle': True,\n",
    "    'seed': 64\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66443802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:36.512157Z",
     "iopub.status.busy": "2024-12-22T10:48:36.511781Z",
     "iopub.status.idle": "2024-12-22T10:48:36.518147Z",
     "shell.execute_reply": "2024-12-22T10:48:36.517196Z"
    },
    "papermill": {
     "duration": 0.065977,
     "end_time": "2024-12-22T10:48:36.519915",
     "exception": false,
     "start_time": "2024-12-22T10:48:36.453938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'auto',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'CPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b47d3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:36.633950Z",
     "iopub.status.busy": "2024-12-22T10:48:36.633606Z",
     "iopub.status.idle": "2024-12-22T10:48:36.637772Z",
     "shell.execute_reply": "2024-12-22T10:48:36.636946Z"
    },
    "papermill": {
     "duration": 0.06356,
     "end_time": "2024-12-22T10:48:36.639727",
     "exception": false,
     "start_time": "2024-12-22T10:48:36.576167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights=[4.0,5.0,4.0,3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef3a52ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:36.755014Z",
     "iopub.status.busy": "2024-12-22T10:48:36.754084Z",
     "iopub.status.idle": "2024-12-22T10:48:36.758625Z",
     "shell.execute_reply": "2024-12-22T10:48:36.757777Z"
    },
    "papermill": {
     "duration": 0.063759,
     "end_time": "2024-12-22T10:48:36.760349",
     "exception": false,
     "start_time": "2024-12-22T10:48:36.696590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_dict = {'LightGBM': Params, 'XGBoost': XGB_Params, 'CatBoost': CatBoost_Params, 'FTTransformer': FTTransformer_Params }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e853e33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:48:36.874158Z",
     "iopub.status.busy": "2024-12-22T10:48:36.873792Z",
     "iopub.status.idle": "2024-12-22T10:55:09.757316Z",
     "shell.execute_reply": "2024-12-22T10:55:09.756027Z"
    },
    "papermill": {
     "duration": 392.94353,
     "end_time": "2024-12-22T10:55:09.759662",
     "exception": false,
     "start_time": "2024-12-22T10:48:36.816132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with CV: Ensemble\n",
      "Training fold 1/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 15s 353ms/step - loss: 1.6394 - importances_loss: 0.9208 - output_loss: 0.7186 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.8477 - val_loss: 1.5459 - val_importances_loss: 0.9515 - val_output_loss: 0.5944 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7710\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 1.5305 - importances_loss: 0.9208 - output_loss: 0.6097 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7808 - val_loss: 1.5480 - val_importances_loss: 0.9515 - val_output_loss: 0.5965 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7723\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 1.5151 - importances_loss: 0.9208 - output_loss: 0.5943 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7709 - val_loss: 1.5282 - val_importances_loss: 0.9515 - val_output_loss: 0.5767 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7594\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 1.4949 - importances_loss: 0.9208 - output_loss: 0.5741 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7577 - val_loss: 1.4772 - val_importances_loss: 0.9515 - val_output_loss: 0.5257 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7251\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.4702 - importances_loss: 0.9208 - output_loss: 0.5494 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7412 - val_loss: 1.5808 - val_importances_loss: 0.9515 - val_output_loss: 0.6293 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7933\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 1.5051 - importances_loss: 0.9208 - output_loss: 0.5843 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7644 - val_loss: 1.4719 - val_importances_loss: 0.9515 - val_output_loss: 0.5204 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7214\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 1.4728 - importances_loss: 0.9208 - output_loss: 0.5520 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7430 - val_loss: 1.5058 - val_importances_loss: 0.9515 - val_output_loss: 0.5543 - val_importances_root_mean_squared_error: 0.9754 - val_output_root_mean_squared_error: 0.7445\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 1.4584 - importances_loss: 0.9208 - output_loss: 0.5376 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7332 - val_loss: 1.4814 - val_importances_loss: 0.9515 - val_output_loss: 0.5299 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.7279\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 229ms/step - loss: 1.4472 - importances_loss: 0.9208 - output_loss: 0.5264 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7255 - val_loss: 1.4471 - val_importances_loss: 0.9515 - val_output_loss: 0.4956 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.7040\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 1.4189 - importances_loss: 0.9208 - output_loss: 0.4981 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7057 - val_loss: 1.4332 - val_importances_loss: 0.9515 - val_output_loss: 0.4817 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6940\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 217ms/step - loss: 1.4061 - importances_loss: 0.9208 - output_loss: 0.4853 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6966 - val_loss: 1.4215 - val_importances_loss: 0.9515 - val_output_loss: 0.4700 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6856\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 1.4086 - importances_loss: 0.9208 - output_loss: 0.4878 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6984 - val_loss: 1.4170 - val_importances_loss: 0.9515 - val_output_loss: 0.4655 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6823\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 1.4110 - importances_loss: 0.9208 - output_loss: 0.4901 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.7001 - val_loss: 1.4411 - val_importances_loss: 0.9515 - val_output_loss: 0.4895 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6997\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 1.4046 - importances_loss: 0.9208 - output_loss: 0.4838 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6955 - val_loss: 1.4225 - val_importances_loss: 0.9515 - val_output_loss: 0.4710 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6863\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 1.3960 - importances_loss: 0.9208 - output_loss: 0.4751 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6893 - val_loss: 1.4147 - val_importances_loss: 0.9515 - val_output_loss: 0.4632 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6806\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 1.3892 - importances_loss: 0.9208 - output_loss: 0.4684 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6844 - val_loss: 1.4186 - val_importances_loss: 0.9515 - val_output_loss: 0.4670 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6834\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 1.3877 - importances_loss: 0.9208 - output_loss: 0.4669 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6833 - val_loss: 1.4092 - val_importances_loss: 0.9515 - val_output_loss: 0.4576 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6765\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 1.3936 - importances_loss: 0.9208 - output_loss: 0.4728 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6876 - val_loss: 1.4056 - val_importances_loss: 0.9516 - val_output_loss: 0.4540 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6738\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.3849 - importances_loss: 0.9208 - output_loss: 0.4641 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6812 - val_loss: 1.4094 - val_importances_loss: 0.9516 - val_output_loss: 0.4578 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6766\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 236ms/step - loss: 1.3761 - importances_loss: 0.9208 - output_loss: 0.4552 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6747 - val_loss: 1.4094 - val_importances_loss: 0.9516 - val_output_loss: 0.4578 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6766\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 1.3766 - importances_loss: 0.9208 - output_loss: 0.4557 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6751 - val_loss: 1.3937 - val_importances_loss: 0.9515 - val_output_loss: 0.4421 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6649\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 1.3765 - importances_loss: 0.9208 - output_loss: 0.4556 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6750 - val_loss: 1.4044 - val_importances_loss: 0.9516 - val_output_loss: 0.4529 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6730\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 236ms/step - loss: 1.3757 - importances_loss: 0.9209 - output_loss: 0.4549 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6744 - val_loss: 1.3943 - val_importances_loss: 0.9516 - val_output_loss: 0.4428 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6654\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.3647 - importances_loss: 0.9209 - output_loss: 0.4439 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6662 - val_loss: 1.4670 - val_importances_loss: 0.9515 - val_output_loss: 0.5154 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.7179\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 224ms/step - loss: 1.3893 - importances_loss: 0.9208 - output_loss: 0.4684 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6844 - val_loss: 1.4030 - val_importances_loss: 0.9516 - val_output_loss: 0.4515 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6719\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 233ms/step - loss: 1.3907 - importances_loss: 0.9209 - output_loss: 0.4699 - importances_root_mean_squared_error: 0.9596 - output_root_mean_squared_error: 0.6855 - val_loss: 1.4265 - val_importances_loss: 0.9516 - val_output_loss: 0.4749 - val_importances_root_mean_squared_error: 0.9755 - val_output_root_mean_squared_error: 0.6891\n",
      "9/9 [==============================] - 1s 73ms/step\n",
      "3/3 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "Fold 1 - Train QWK: 0.7274, Validation QWK: 0.3649\n",
      "Training fold 2/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 9s 370ms/step - loss: 1.6550 - importances_loss: 0.9245 - output_loss: 0.7305 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.8547 - val_loss: 1.6558 - val_importances_loss: 1.0151 - val_output_loss: 0.6407 - val_importances_root_mean_squared_error: 1.0075 - val_output_root_mean_squared_error: 0.8004\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 228ms/step - loss: 1.5399 - importances_loss: 0.9245 - output_loss: 0.6155 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7845 - val_loss: 1.6500 - val_importances_loss: 1.0151 - val_output_loss: 0.6348 - val_importances_root_mean_squared_error: 1.0075 - val_output_root_mean_squared_error: 0.7968\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 1.5273 - importances_loss: 0.9245 - output_loss: 0.6029 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7765 - val_loss: 1.6686 - val_importances_loss: 1.0151 - val_output_loss: 0.6535 - val_importances_root_mean_squared_error: 1.0075 - val_output_root_mean_squared_error: 0.8084\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 1.5336 - importances_loss: 0.9245 - output_loss: 0.6091 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7805 - val_loss: 1.6474 - val_importances_loss: 1.0151 - val_output_loss: 0.6323 - val_importances_root_mean_squared_error: 1.0075 - val_output_root_mean_squared_error: 0.7951\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 1.5193 - importances_loss: 0.9245 - output_loss: 0.5949 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7713 - val_loss: 1.6428 - val_importances_loss: 1.0151 - val_output_loss: 0.6276 - val_importances_root_mean_squared_error: 1.0075 - val_output_root_mean_squared_error: 0.7922\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 224ms/step - loss: 1.5114 - importances_loss: 0.9245 - output_loss: 0.5870 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7662 - val_loss: 1.6308 - val_importances_loss: 1.0151 - val_output_loss: 0.6157 - val_importances_root_mean_squared_error: 1.0075 - val_output_root_mean_squared_error: 0.7846\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 233ms/step - loss: 1.4765 - importances_loss: 0.9245 - output_loss: 0.5520 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7430 - val_loss: 1.6244 - val_importances_loss: 1.0152 - val_output_loss: 0.6092 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7805\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.4605 - importances_loss: 0.9245 - output_loss: 0.5360 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7321 - val_loss: 1.5643 - val_importances_loss: 1.0152 - val_output_loss: 0.5491 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7410\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 233ms/step - loss: 1.4453 - importances_loss: 0.9246 - output_loss: 0.5207 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.7216 - val_loss: 1.5473 - val_importances_loss: 1.0153 - val_output_loss: 0.5321 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7294\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 1.4174 - importances_loss: 0.9246 - output_loss: 0.4928 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.7020 - val_loss: 1.5381 - val_importances_loss: 1.0153 - val_output_loss: 0.5228 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7230\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 1.4118 - importances_loss: 0.9246 - output_loss: 0.4872 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6980 - val_loss: 1.5717 - val_importances_loss: 1.0153 - val_output_loss: 0.5565 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7460\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 1.4176 - importances_loss: 0.9246 - output_loss: 0.4931 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.7022 - val_loss: 1.5566 - val_importances_loss: 1.0153 - val_output_loss: 0.5413 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7358\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 1.4095 - importances_loss: 0.9246 - output_loss: 0.4849 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6964 - val_loss: 1.5439 - val_importances_loss: 1.0153 - val_output_loss: 0.5286 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7271\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.4038 - importances_loss: 0.9246 - output_loss: 0.4793 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6923 - val_loss: 1.5360 - val_importances_loss: 1.0153 - val_output_loss: 0.5207 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7216\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 1.4045 - importances_loss: 0.9246 - output_loss: 0.4799 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6927 - val_loss: 1.5345 - val_importances_loss: 1.0152 - val_output_loss: 0.5193 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7206\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 1.3959 - importances_loss: 0.9246 - output_loss: 0.4714 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6866 - val_loss: 1.5422 - val_importances_loss: 1.0152 - val_output_loss: 0.5270 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7259\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 1.4057 - importances_loss: 0.9246 - output_loss: 0.4812 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6937 - val_loss: 1.5293 - val_importances_loss: 1.0152 - val_output_loss: 0.5141 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7170\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 1.3928 - importances_loss: 0.9246 - output_loss: 0.4683 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6843 - val_loss: 1.5226 - val_importances_loss: 1.0153 - val_output_loss: 0.5074 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7123\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.3974 - importances_loss: 0.9246 - output_loss: 0.4728 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6876 - val_loss: 1.5312 - val_importances_loss: 1.0153 - val_output_loss: 0.5160 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7183\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 1.4023 - importances_loss: 0.9246 - output_loss: 0.4777 - importances_root_mean_squared_error: 0.9615 - output_root_mean_squared_error: 0.6911 - val_loss: 1.5107 - val_importances_loss: 1.0153 - val_output_loss: 0.4954 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7039\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 1.3934 - importances_loss: 0.9246 - output_loss: 0.4688 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6847 - val_loss: 1.5809 - val_importances_loss: 1.0152 - val_output_loss: 0.5657 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7521\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 1.4175 - importances_loss: 0.9246 - output_loss: 0.4929 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.7020 - val_loss: 1.5690 - val_importances_loss: 1.0152 - val_output_loss: 0.5538 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7442\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 1.3972 - importances_loss: 0.9246 - output_loss: 0.4726 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6875 - val_loss: 1.5126 - val_importances_loss: 1.0153 - val_output_loss: 0.4974 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7052\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 1.3794 - importances_loss: 0.9246 - output_loss: 0.4548 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6744 - val_loss: 1.5094 - val_importances_loss: 1.0153 - val_output_loss: 0.4941 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7029\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.3964 - importances_loss: 0.9246 - output_loss: 0.4718 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6869 - val_loss: 1.5113 - val_importances_loss: 1.0153 - val_output_loss: 0.4960 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7043\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 235ms/step - loss: 1.3924 - importances_loss: 0.9246 - output_loss: 0.4678 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6840 - val_loss: 1.5077 - val_importances_loss: 1.0153 - val_output_loss: 0.4924 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7017\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 1.3750 - importances_loss: 0.9246 - output_loss: 0.4504 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6711 - val_loss: 1.5206 - val_importances_loss: 1.0153 - val_output_loss: 0.5054 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7109\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.3747 - importances_loss: 0.9246 - output_loss: 0.4501 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6709 - val_loss: 1.5067 - val_importances_loss: 1.0153 - val_output_loss: 0.4915 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7011\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 242ms/step - loss: 1.3654 - importances_loss: 0.9246 - output_loss: 0.4408 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6639 - val_loss: 1.4862 - val_importances_loss: 1.0153 - val_output_loss: 0.4709 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.6863\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 224ms/step - loss: 1.3772 - importances_loss: 0.9246 - output_loss: 0.4526 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6727 - val_loss: 1.5494 - val_importances_loss: 1.0152 - val_output_loss: 0.5341 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7308\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 1.3830 - importances_loss: 0.9246 - output_loss: 0.4584 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6771 - val_loss: 1.5231 - val_importances_loss: 1.0153 - val_output_loss: 0.5078 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7126\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 1.3829 - importances_loss: 0.9246 - output_loss: 0.4583 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6770 - val_loss: 1.4930 - val_importances_loss: 1.0153 - val_output_loss: 0.4778 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.6912\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.3753 - importances_loss: 0.9246 - output_loss: 0.4507 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6713 - val_loss: 1.4870 - val_importances_loss: 1.0153 - val_output_loss: 0.4717 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.6868\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 1.3719 - importances_loss: 0.9246 - output_loss: 0.4472 - importances_root_mean_squared_error: 0.9616 - output_root_mean_squared_error: 0.6688 - val_loss: 1.5149 - val_importances_loss: 1.0153 - val_output_loss: 0.4997 - val_importances_root_mean_squared_error: 1.0076 - val_output_root_mean_squared_error: 0.7069\n",
      "9/9 [==============================] - 1s 75ms/step\n",
      "3/3 [==============================] - 1s 56ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "Fold 2 - Train QWK: 0.7433, Validation QWK: 0.4220\n",
      "Training fold 3/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 9s 335ms/step - loss: 1.7078 - importances_loss: 0.9204 - output_loss: 0.7874 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.8874 - val_loss: 1.4746 - val_importances_loss: 0.8727 - val_output_loss: 0.6019 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7758\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 1.5841 - importances_loss: 0.9204 - output_loss: 0.6637 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.8147 - val_loss: 1.4566 - val_importances_loss: 0.8727 - val_output_loss: 0.5840 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7642\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 1.5349 - importances_loss: 0.9204 - output_loss: 0.6146 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7839 - val_loss: 1.4549 - val_importances_loss: 0.8727 - val_output_loss: 0.5822 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7630\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 1.5094 - importances_loss: 0.9204 - output_loss: 0.5891 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7675 - val_loss: 1.4616 - val_importances_loss: 0.8727 - val_output_loss: 0.5889 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7674\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 1.4953 - importances_loss: 0.9204 - output_loss: 0.5749 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7583 - val_loss: 1.4334 - val_importances_loss: 0.8727 - val_output_loss: 0.5608 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7489\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 241ms/step - loss: 1.4642 - importances_loss: 0.9204 - output_loss: 0.5438 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7374 - val_loss: 1.4132 - val_importances_loss: 0.8727 - val_output_loss: 0.5405 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7352\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 1.4437 - importances_loss: 0.9204 - output_loss: 0.5233 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7234 - val_loss: 1.4047 - val_importances_loss: 0.8727 - val_output_loss: 0.5320 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7294\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.4508 - importances_loss: 0.9204 - output_loss: 0.5304 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7283 - val_loss: 1.3907 - val_importances_loss: 0.8727 - val_output_loss: 0.5180 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7197\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 237ms/step - loss: 1.4349 - importances_loss: 0.9204 - output_loss: 0.5144 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7172 - val_loss: 1.3956 - val_importances_loss: 0.8728 - val_output_loss: 0.5228 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7230\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 240ms/step - loss: 1.4210 - importances_loss: 0.9205 - output_loss: 0.5005 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.7075 - val_loss: 1.4228 - val_importances_loss: 0.8728 - val_output_loss: 0.5500 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7416\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 1.4094 - importances_loss: 0.9205 - output_loss: 0.4889 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6992 - val_loss: 1.3938 - val_importances_loss: 0.8728 - val_output_loss: 0.5210 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7218\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 1.4072 - importances_loss: 0.9205 - output_loss: 0.4867 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6976 - val_loss: 1.3684 - val_importances_loss: 0.8728 - val_output_loss: 0.4957 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7040\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 246ms/step - loss: 1.4040 - importances_loss: 0.9205 - output_loss: 0.4835 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6954 - val_loss: 1.3693 - val_importances_loss: 0.8728 - val_output_loss: 0.4965 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7046\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 240ms/step - loss: 1.4052 - importances_loss: 0.9205 - output_loss: 0.4848 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6963 - val_loss: 1.4162 - val_importances_loss: 0.8728 - val_output_loss: 0.5435 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7372\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 1.4021 - importances_loss: 0.9205 - output_loss: 0.4817 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6940 - val_loss: 1.3886 - val_importances_loss: 0.8728 - val_output_loss: 0.5159 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7182\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 249ms/step - loss: 1.4036 - importances_loss: 0.9205 - output_loss: 0.4831 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6951 - val_loss: 1.3643 - val_importances_loss: 0.8728 - val_output_loss: 0.4916 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7011\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 246ms/step - loss: 1.4058 - importances_loss: 0.9205 - output_loss: 0.4853 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6966 - val_loss: 1.3579 - val_importances_loss: 0.8728 - val_output_loss: 0.4851 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6965\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 1.3919 - importances_loss: 0.9205 - output_loss: 0.4714 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6866 - val_loss: 1.3555 - val_importances_loss: 0.8728 - val_output_loss: 0.4827 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6948\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 1.3845 - importances_loss: 0.9205 - output_loss: 0.4640 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6812 - val_loss: 1.3562 - val_importances_loss: 0.8728 - val_output_loss: 0.4834 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6953\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 1.3898 - importances_loss: 0.9205 - output_loss: 0.4693 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6851 - val_loss: 1.3628 - val_importances_loss: 0.8728 - val_output_loss: 0.4900 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7000\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 241ms/step - loss: 1.3956 - importances_loss: 0.9205 - output_loss: 0.4751 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6893 - val_loss: 1.3539 - val_importances_loss: 0.8728 - val_output_loss: 0.4811 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6936\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 1.3866 - importances_loss: 0.9205 - output_loss: 0.4661 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6827 - val_loss: 1.3633 - val_importances_loss: 0.8728 - val_output_loss: 0.4905 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7004\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 3s 269ms/step - loss: 1.3864 - importances_loss: 0.9205 - output_loss: 0.4659 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6826 - val_loss: 1.3686 - val_importances_loss: 0.8728 - val_output_loss: 0.4958 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7041\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 229ms/step - loss: 1.3665 - importances_loss: 0.9205 - output_loss: 0.4460 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6679 - val_loss: 1.3651 - val_importances_loss: 0.8728 - val_output_loss: 0.4923 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7016\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 1.3723 - importances_loss: 0.9205 - output_loss: 0.4518 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6722 - val_loss: 1.3461 - val_importances_loss: 0.8728 - val_output_loss: 0.4733 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6880\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 241ms/step - loss: 1.3616 - importances_loss: 0.9205 - output_loss: 0.4410 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6641 - val_loss: 1.3274 - val_importances_loss: 0.8728 - val_output_loss: 0.4546 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6742\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 224ms/step - loss: 1.3547 - importances_loss: 0.9205 - output_loss: 0.4342 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6589 - val_loss: 1.3272 - val_importances_loss: 0.8728 - val_output_loss: 0.4544 - val_importances_root_mean_squared_error: 0.9343 - val_output_root_mean_squared_error: 0.6741\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 230ms/step - loss: 1.3611 - importances_loss: 0.9205 - output_loss: 0.4406 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6638 - val_loss: 1.3557 - val_importances_loss: 0.8728 - val_output_loss: 0.4828 - val_importances_root_mean_squared_error: 0.9343 - val_output_root_mean_squared_error: 0.6949\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 245ms/step - loss: 1.3713 - importances_loss: 0.9205 - output_loss: 0.4508 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6714 - val_loss: 1.3313 - val_importances_loss: 0.8728 - val_output_loss: 0.4585 - val_importances_root_mean_squared_error: 0.9343 - val_output_root_mean_squared_error: 0.6771\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 1.3941 - importances_loss: 0.9205 - output_loss: 0.4735 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6881 - val_loss: 1.3619 - val_importances_loss: 0.8728 - val_output_loss: 0.4890 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.6993\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 1.3870 - importances_loss: 0.9205 - output_loss: 0.4665 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6830 - val_loss: 1.3675 - val_importances_loss: 0.8728 - val_output_loss: 0.4947 - val_importances_root_mean_squared_error: 0.9342 - val_output_root_mean_squared_error: 0.7034\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 1.3786 - importances_loss: 0.9205 - output_loss: 0.4581 - importances_root_mean_squared_error: 0.9594 - output_root_mean_squared_error: 0.6768 - val_loss: 1.3435 - val_importances_loss: 0.8728 - val_output_loss: 0.4706 - val_importances_root_mean_squared_error: 0.9343 - val_output_root_mean_squared_error: 0.6860\n",
      "9/9 [==============================] - 2s 77ms/step\n",
      "3/3 [==============================] - 1s 58ms/step\n",
      "1/1 [==============================] - 0s 355ms/step\n",
      "Fold 3 - Train QWK: 0.7494, Validation QWK: 0.4144\n",
      "Training fold 4/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 9s 329ms/step - loss: 1.7327 - importances_loss: 0.9222 - output_loss: 0.8105 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.9003 - val_loss: 1.8584 - val_importances_loss: 0.9040 - val_output_loss: 0.9545 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.9770\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.6103 - importances_loss: 0.9222 - output_loss: 0.6881 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.8295 - val_loss: 1.5723 - val_importances_loss: 0.9040 - val_output_loss: 0.6684 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.8175\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 1.5486 - importances_loss: 0.9222 - output_loss: 0.6264 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7914 - val_loss: 1.5116 - val_importances_loss: 0.9040 - val_output_loss: 0.6077 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.7795\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.5219 - importances_loss: 0.9222 - output_loss: 0.5997 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7744 - val_loss: 1.4489 - val_importances_loss: 0.9040 - val_output_loss: 0.5449 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.7382\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 1.4766 - importances_loss: 0.9222 - output_loss: 0.5544 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7446 - val_loss: 1.4096 - val_importances_loss: 0.9040 - val_output_loss: 0.5057 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.7111\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 1.4636 - importances_loss: 0.9222 - output_loss: 0.5414 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7358 - val_loss: 1.3897 - val_importances_loss: 0.9040 - val_output_loss: 0.4857 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.6969\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 237ms/step - loss: 1.4390 - importances_loss: 0.9222 - output_loss: 0.5168 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7189 - val_loss: 1.3917 - val_importances_loss: 0.9040 - val_output_loss: 0.4877 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.6984\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 1.4339 - importances_loss: 0.9222 - output_loss: 0.5116 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7153 - val_loss: 1.3788 - val_importances_loss: 0.9041 - val_output_loss: 0.4747 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.6890\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 3s 270ms/step - loss: 1.3996 - importances_loss: 0.9223 - output_loss: 0.4773 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6909 - val_loss: 1.3551 - val_importances_loss: 0.9041 - val_output_loss: 0.4511 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.6716\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 257ms/step - loss: 1.4121 - importances_loss: 0.9223 - output_loss: 0.4898 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6999 - val_loss: 1.4038 - val_importances_loss: 0.9041 - val_output_loss: 0.4997 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.7069\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 3s 283ms/step - loss: 1.4360 - importances_loss: 0.9223 - output_loss: 0.5137 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7168 - val_loss: 1.3944 - val_importances_loss: 0.9041 - val_output_loss: 0.4903 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.7002\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 250ms/step - loss: 1.4088 - importances_loss: 0.9223 - output_loss: 0.4864 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6975 - val_loss: 1.3727 - val_importances_loss: 0.9042 - val_output_loss: 0.4686 - val_importances_root_mean_squared_error: 0.9509 - val_output_root_mean_squared_error: 0.6845\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 251ms/step - loss: 1.4082 - importances_loss: 0.9223 - output_loss: 0.4859 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6970 - val_loss: 1.3563 - val_importances_loss: 0.9041 - val_output_loss: 0.4522 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.6725\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 245ms/step - loss: 1.3963 - importances_loss: 0.9223 - output_loss: 0.4740 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6885 - val_loss: 1.3631 - val_importances_loss: 0.9041 - val_output_loss: 0.4590 - val_importances_root_mean_squared_error: 0.9508 - val_output_root_mean_squared_error: 0.6775\n",
      "9/9 [==============================] - 2s 78ms/step\n",
      "3/3 [==============================] - 1s 62ms/step\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "Fold 4 - Train QWK: 0.7373, Validation QWK: 0.3353\n",
      "Training fold 5/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 9s 341ms/step - loss: 1.6305 - importances_loss: 0.9222 - output_loss: 0.7084 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.8416 - val_loss: 1.6391 - val_importances_loss: 0.9948 - val_output_loss: 0.6443 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.8027\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.5285 - importances_loss: 0.9222 - output_loss: 0.6063 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7787 - val_loss: 1.6303 - val_importances_loss: 0.9948 - val_output_loss: 0.6355 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7972\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 242ms/step - loss: 1.5212 - importances_loss: 0.9222 - output_loss: 0.5991 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7740 - val_loss: 1.6236 - val_importances_loss: 0.9948 - val_output_loss: 0.6287 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7929\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 1.5092 - importances_loss: 0.9222 - output_loss: 0.5870 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7662 - val_loss: 1.6132 - val_importances_loss: 0.9948 - val_output_loss: 0.6183 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7863\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 212ms/step - loss: 1.4919 - importances_loss: 0.9222 - output_loss: 0.5698 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7548 - val_loss: 1.5612 - val_importances_loss: 0.9948 - val_output_loss: 0.5664 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7526\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 1.4577 - importances_loss: 0.9222 - output_loss: 0.5355 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7318 - val_loss: 1.5374 - val_importances_loss: 0.9949 - val_output_loss: 0.5425 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7366\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 1.4260 - importances_loss: 0.9222 - output_loss: 0.5038 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.7098 - val_loss: 1.4926 - val_importances_loss: 0.9949 - val_output_loss: 0.4978 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7055\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 1.4094 - importances_loss: 0.9222 - output_loss: 0.4872 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6980 - val_loss: 1.4687 - val_importances_loss: 0.9949 - val_output_loss: 0.4738 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.6883\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 238ms/step - loss: 1.3972 - importances_loss: 0.9222 - output_loss: 0.4750 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6892 - val_loss: 1.4543 - val_importances_loss: 0.9949 - val_output_loss: 0.4594 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6778\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.3893 - importances_loss: 0.9222 - output_loss: 0.4671 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6834 - val_loss: 1.4959 - val_importances_loss: 0.9949 - val_output_loss: 0.5010 - val_importances_root_mean_squared_error: 0.9974 - val_output_root_mean_squared_error: 0.7078\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 212ms/step - loss: 1.3953 - importances_loss: 0.9222 - output_loss: 0.4731 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6878 - val_loss: 1.4686 - val_importances_loss: 0.9949 - val_output_loss: 0.4737 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6882\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 1.3811 - importances_loss: 0.9222 - output_loss: 0.4588 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6774 - val_loss: 1.4395 - val_importances_loss: 0.9949 - val_output_loss: 0.4446 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6668\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 1.3804 - importances_loss: 0.9222 - output_loss: 0.4582 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6769 - val_loss: 1.4514 - val_importances_loss: 0.9949 - val_output_loss: 0.4565 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6756\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 212ms/step - loss: 1.3774 - importances_loss: 0.9223 - output_loss: 0.4551 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6746 - val_loss: 1.4366 - val_importances_loss: 0.9949 - val_output_loss: 0.4417 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6646\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 1.3840 - importances_loss: 0.9223 - output_loss: 0.4617 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6795 - val_loss: 1.4509 - val_importances_loss: 0.9949 - val_output_loss: 0.4559 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6752\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 1.3765 - importances_loss: 0.9223 - output_loss: 0.4542 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6740 - val_loss: 1.4372 - val_importances_loss: 0.9950 - val_output_loss: 0.4423 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6650\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 1.3818 - importances_loss: 0.9223 - output_loss: 0.4595 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6779 - val_loss: 1.4668 - val_importances_loss: 0.9949 - val_output_loss: 0.4719 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6869\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 1.3834 - importances_loss: 0.9223 - output_loss: 0.4611 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6791 - val_loss: 1.4582 - val_importances_loss: 0.9949 - val_output_loss: 0.4632 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6806\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 1.3819 - importances_loss: 0.9223 - output_loss: 0.4597 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6780 - val_loss: 1.4349 - val_importances_loss: 0.9950 - val_output_loss: 0.4399 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6633\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 1.3870 - importances_loss: 0.9223 - output_loss: 0.4648 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6817 - val_loss: 1.4292 - val_importances_loss: 0.9950 - val_output_loss: 0.4342 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6590\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 212ms/step - loss: 1.3730 - importances_loss: 0.9223 - output_loss: 0.4507 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6714 - val_loss: 1.4293 - val_importances_loss: 0.9949 - val_output_loss: 0.4343 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6590\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.3688 - importances_loss: 0.9223 - output_loss: 0.4465 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6682 - val_loss: 1.4338 - val_importances_loss: 0.9949 - val_output_loss: 0.4388 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6624\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 217ms/step - loss: 1.3657 - importances_loss: 0.9223 - output_loss: 0.4434 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6659 - val_loss: 1.4253 - val_importances_loss: 0.9949 - val_output_loss: 0.4303 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6560\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 1.3686 - importances_loss: 0.9223 - output_loss: 0.4464 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6681 - val_loss: 1.4128 - val_importances_loss: 0.9949 - val_output_loss: 0.4179 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6464\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 1.3637 - importances_loss: 0.9223 - output_loss: 0.4414 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6644 - val_loss: 1.4260 - val_importances_loss: 0.9949 - val_output_loss: 0.4311 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6565\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 212ms/step - loss: 1.3618 - importances_loss: 0.9223 - output_loss: 0.4396 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6630 - val_loss: 1.4176 - val_importances_loss: 0.9949 - val_output_loss: 0.4226 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6501\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 1.3719 - importances_loss: 0.9223 - output_loss: 0.4496 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6705 - val_loss: 1.4353 - val_importances_loss: 0.9949 - val_output_loss: 0.4404 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6636\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 1.3571 - importances_loss: 0.9223 - output_loss: 0.4349 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6594 - val_loss: 1.4072 - val_importances_loss: 0.9949 - val_output_loss: 0.4122 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6421\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 1.3565 - importances_loss: 0.9223 - output_loss: 0.4342 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6589 - val_loss: 1.4150 - val_importances_loss: 0.9949 - val_output_loss: 0.4200 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6481\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 1.3663 - importances_loss: 0.9223 - output_loss: 0.4440 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6663 - val_loss: 1.4028 - val_importances_loss: 0.9949 - val_output_loss: 0.4078 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6386\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 1.3507 - importances_loss: 0.9223 - output_loss: 0.4285 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6546 - val_loss: 1.3998 - val_importances_loss: 0.9949 - val_output_loss: 0.4049 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6363\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 228ms/step - loss: 1.3553 - importances_loss: 0.9223 - output_loss: 0.4331 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6581 - val_loss: 1.4004 - val_importances_loss: 0.9949 - val_output_loss: 0.4055 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6368\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 1.3565 - importances_loss: 0.9223 - output_loss: 0.4343 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6590 - val_loss: 1.4180 - val_importances_loss: 0.9949 - val_output_loss: 0.4231 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6505\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 1.3566 - importances_loss: 0.9223 - output_loss: 0.4343 - importances_root_mean_squared_error: 0.9603 - output_root_mean_squared_error: 0.6590 - val_loss: 1.4223 - val_importances_loss: 0.9949 - val_output_loss: 0.4273 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6537\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 1.3617 - importances_loss: 0.9223 - output_loss: 0.4394 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6629 - val_loss: 1.4539 - val_importances_loss: 0.9949 - val_output_loss: 0.4589 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6774\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 1.3625 - importances_loss: 0.9223 - output_loss: 0.4402 - importances_root_mean_squared_error: 0.9604 - output_root_mean_squared_error: 0.6635 - val_loss: 1.4245 - val_importances_loss: 0.9950 - val_output_loss: 0.4296 - val_importances_root_mean_squared_error: 0.9975 - val_output_root_mean_squared_error: 0.6554\n",
      "9/9 [==============================] - 1s 75ms/step\n",
      "3/3 [==============================] - 1s 58ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "Fold 5 - Train QWK: 0.7442, Validation QWK: 0.3174\n",
      "Mean Train QWK --> 0.7403\n",
      "Mean Validation QWK ---> 0.3708\n",
      "KappaOPtimizer.x = [0.56461546 0.90441001 2.80785218]\n",
      "----> || Optimized QWK SCORE :: 0.440\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Train model and make predictions using cross-validation\n",
    "predictions, trained_models = train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method='voting', weights=weights\n",
    "\n",
    "# Generate submission\n",
    "final_predictions = next(iter(predictions.values()))\n",
    "submission = generate_submission_file(final_predictions, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116c4a1",
   "metadata": {
    "papermill": {
     "duration": 0.136301,
     "end_time": "2024-12-22T10:55:10.043872",
     "exception": false,
     "start_time": "2024-12-22T10:55:09.907571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec7a23",
   "metadata": {
    "papermill": {
     "duration": 0.135042,
     "end_time": "2024-12-22T10:55:10.314343",
     "exception": false,
     "start_time": "2024-12-22T10:55:10.179301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Tabnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf3faf35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:55:10.584041Z",
     "iopub.status.busy": "2024-12-22T10:55:10.583253Z",
     "iopub.status.idle": "2024-12-22T10:55:10.601419Z",
     "shell.execute_reply": "2024-12-22T10:55:10.600454Z"
    },
    "papermill": {
     "duration": 0.155533,
     "end_time": "2024-12-22T10:55:10.603580",
     "exception": false,
     "start_time": "2024-12-22T10:55:10.448047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New: TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d227e35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:55:10.927265Z",
     "iopub.status.busy": "2024-12-22T10:55:10.926319Z",
     "iopub.status.idle": "2024-12-22T10:55:10.932775Z",
     "shell.execute_reply": "2024-12-22T10:55:10.931861Z"
    },
    "papermill": {
     "duration": 0.194836,
     "end_time": "2024-12-22T10:55:10.934724",
     "exception": false,
     "start_time": "2024-12-22T10:55:10.739888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ab770e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:55:11.208323Z",
     "iopub.status.busy": "2024-12-22T10:55:11.207419Z",
     "iopub.status.idle": "2024-12-22T10:55:11.214251Z",
     "shell.execute_reply": "2024-12-22T10:55:11.213284Z"
    },
    "papermill": {
     "duration": 0.147762,
     "end_time": "2024-12-22T10:55:11.216154",
     "exception": false,
     "start_time": "2024-12-22T10:55:11.068392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params2 = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params2 = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'auto',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params2 = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'CPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3d50092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:55:11.483734Z",
     "iopub.status.busy": "2024-12-22T10:55:11.483217Z",
     "iopub.status.idle": "2024-12-22T10:55:11.498714Z",
     "shell.execute_reply": "2024-12-22T10:55:11.492134Z"
    },
    "papermill": {
     "duration": 0.152726,
     "end_time": "2024-12-22T10:55:11.501267",
     "exception": false,
     "start_time": "2024-12-22T10:55:11.348541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_dict2 = {'LightGBM': Params2, 'XGBoost': XGB_Params2, 'CatBoost': CatBoost_Params2, 'TabNet': TabNet_Params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20bb50ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:55:11.780591Z",
     "iopub.status.busy": "2024-12-22T10:55:11.779785Z",
     "iopub.status.idle": "2024-12-22T10:55:11.788505Z",
     "shell.execute_reply": "2024-12-22T10:55:11.784267Z"
    },
    "papermill": {
     "duration": 0.15323,
     "end_time": "2024-12-22T10:55:11.793604",
     "exception": false,
     "start_time": "2024-12-22T10:55:11.640374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights2 = [4.0,5.0,4.0,4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c3c06d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:55:12.063648Z",
     "iopub.status.busy": "2024-12-22T10:55:12.063314Z",
     "iopub.status.idle": "2024-12-22T10:56:43.634696Z",
     "shell.execute_reply": "2024-12-22T10:56:43.633566Z"
    },
    "papermill": {
     "duration": 91.708783,
     "end_time": "2024-12-22T10:56:43.636878",
     "exception": false,
     "start_time": "2024-12-22T10:55:11.928095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with CV: Ensemble\n",
      "Training fold 1/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "epoch 0  | loss: 2.39934 | valid_mse: 51.45601|  0:00:00s\n",
      "\n",
      "Epoch 0: valid_mse improved from inf to 51.4560\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 1  | loss: 1.41375 | valid_mse: 26.73382|  0:00:00s\n",
      "\n",
      "Epoch 1: valid_mse improved from 51.4560 to 26.7338\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 2  | loss: 1.01562 | valid_mse: 11.47039|  0:00:00s\n",
      "\n",
      "Epoch 2: valid_mse improved from 26.7338 to 11.4704\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 3  | loss: 0.85514 | valid_mse: 3.77951 |  0:00:00s\n",
      "\n",
      "Epoch 3: valid_mse improved from 11.4704 to 3.7795\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 4  | loss: 0.75604 | valid_mse: 2.08962 |  0:00:01s\n",
      "\n",
      "Epoch 4: valid_mse improved from 3.7795 to 2.0896\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 5  | loss: 0.68189 | valid_mse: 1.45567 |  0:00:01s\n",
      "\n",
      "Epoch 5: valid_mse improved from 2.0896 to 1.4557\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 6  | loss: 0.66739 | valid_mse: 1.22872 |  0:00:01s\n",
      "\n",
      "Epoch 6: valid_mse improved from 1.4557 to 1.2287\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 7  | loss: 0.63355 | valid_mse: 1.3844  |  0:00:01s\n",
      "epoch 8  | loss: 0.61318 | valid_mse: 1.54716 |  0:00:01s\n",
      "epoch 9  | loss: 0.57159 | valid_mse: 1.21527 |  0:00:02s\n",
      "\n",
      "Epoch 9: valid_mse improved from 1.2287 to 1.2153\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 10 | loss: 0.57668 | valid_mse: 1.10822 |  0:00:02s\n",
      "\n",
      "Epoch 10: valid_mse improved from 1.2153 to 1.1082\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 11 | loss: 0.56408 | valid_mse: 1.34309 |  0:00:02s\n",
      "epoch 12 | loss: 0.54632 | valid_mse: 2.43754 |  0:00:02s\n",
      "epoch 13 | loss: 0.54636 | valid_mse: 4.08269 |  0:00:02s\n",
      "epoch 14 | loss: 0.5422  | valid_mse: 4.3816  |  0:00:02s\n",
      "epoch 15 | loss: 0.52566 | valid_mse: 3.86032 |  0:00:03s\n",
      "epoch 16 | loss: 0.52456 | valid_mse: 2.95774 |  0:00:03s\n",
      "epoch 17 | loss: 0.53051 | valid_mse: 2.84062 |  0:00:03s\n",
      "epoch 18 | loss: 0.51141 | valid_mse: 2.90208 |  0:00:03s\n",
      "epoch 19 | loss: 0.51665 | valid_mse: 3.03708 |  0:00:03s\n",
      "epoch 20 | loss: 0.50132 | valid_mse: 2.35172 |  0:00:03s\n",
      "epoch 21 | loss: 0.5191  | valid_mse: 1.95015 |  0:00:03s\n",
      "epoch 22 | loss: 0.51479 | valid_mse: 1.86585 |  0:00:04s\n",
      "epoch 23 | loss: 0.51901 | valid_mse: 1.78165 |  0:00:04s\n",
      "epoch 24 | loss: 0.51232 | valid_mse: 2.11288 |  0:00:04s\n",
      "epoch 25 | loss: 0.50377 | valid_mse: 2.17089 |  0:00:04s\n",
      "epoch 26 | loss: 0.50006 | valid_mse: 1.98776 |  0:00:04s\n",
      "epoch 27 | loss: 0.5052  | valid_mse: 1.72545 |  0:00:04s\n",
      "epoch 28 | loss: 0.5123  | valid_mse: 1.40564 |  0:00:04s\n",
      "epoch 29 | loss: 0.4977  | valid_mse: 1.28086 |  0:00:05s\n",
      "epoch 30 | loss: 0.50612 | valid_mse: 1.58418 |  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_valid_mse = 1.10822\n",
      "Fold 1 - Train QWK: 0.6624, Validation QWK: 0.3234\n",
      "Training fold 2/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "epoch 0  | loss: 2.42625 | valid_mse: 59.50043|  0:00:00s\n",
      "\n",
      "Epoch 0: valid_mse improved from inf to 59.5004\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 1  | loss: 1.46786 | valid_mse: 15.36389|  0:00:00s\n",
      "\n",
      "Epoch 1: valid_mse improved from 59.5004 to 15.3639\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 2  | loss: 1.05315 | valid_mse: 7.77786 |  0:00:00s\n",
      "\n",
      "Epoch 2: valid_mse improved from 15.3639 to 7.7779\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 3  | loss: 0.91564 | valid_mse: 5.23295 |  0:00:00s\n",
      "\n",
      "Epoch 3: valid_mse improved from 7.7779 to 5.2330\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 4  | loss: 0.76264 | valid_mse: 3.68357 |  0:00:00s\n",
      "\n",
      "Epoch 4: valid_mse improved from 5.2330 to 3.6836\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 5  | loss: 0.65438 | valid_mse: 3.64585 |  0:00:01s\n",
      "\n",
      "Epoch 5: valid_mse improved from 3.6836 to 3.6459\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 6  | loss: 0.6366  | valid_mse: 4.16623 |  0:00:01s\n",
      "epoch 7  | loss: 0.62077 | valid_mse: 4.84004 |  0:00:01s\n",
      "epoch 8  | loss: 0.59033 | valid_mse: 3.96907 |  0:00:01s\n",
      "epoch 9  | loss: 0.57722 | valid_mse: 2.57342 |  0:00:01s\n",
      "\n",
      "Epoch 9: valid_mse improved from 3.6459 to 2.5734\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 10 | loss: 0.55648 | valid_mse: 1.82039 |  0:00:01s\n",
      "\n",
      "Epoch 10: valid_mse improved from 2.5734 to 1.8204\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 11 | loss: 0.5627  | valid_mse: 1.30577 |  0:00:02s\n",
      "\n",
      "Epoch 11: valid_mse improved from 1.8204 to 1.3058\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 12 | loss: 0.52661 | valid_mse: 1.30587 |  0:00:02s\n",
      "epoch 13 | loss: 0.53392 | valid_mse: 1.37762 |  0:00:02s\n",
      "epoch 14 | loss: 0.52777 | valid_mse: 1.15724 |  0:00:02s\n",
      "\n",
      "Epoch 14: valid_mse improved from 1.3058 to 1.1572\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 15 | loss: 0.52182 | valid_mse: 0.96102 |  0:00:02s\n",
      "\n",
      "Epoch 15: valid_mse improved from 1.1572 to 0.9610\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 16 | loss: 0.51433 | valid_mse: 0.8734  |  0:00:02s\n",
      "\n",
      "Epoch 16: valid_mse improved from 0.9610 to 0.8734\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 17 | loss: 0.50871 | valid_mse: 0.91731 |  0:00:03s\n",
      "epoch 18 | loss: 0.50948 | valid_mse: 0.99023 |  0:00:03s\n",
      "epoch 19 | loss: 0.52076 | valid_mse: 0.89615 |  0:00:03s\n",
      "epoch 20 | loss: 0.51639 | valid_mse: 0.92807 |  0:00:03s\n",
      "epoch 21 | loss: 0.51556 | valid_mse: 0.88469 |  0:00:03s\n",
      "epoch 22 | loss: 0.51577 | valid_mse: 0.92728 |  0:00:03s\n",
      "epoch 23 | loss: 0.51972 | valid_mse: 0.99796 |  0:00:03s\n",
      "epoch 24 | loss: 0.50455 | valid_mse: 0.93752 |  0:00:04s\n",
      "epoch 25 | loss: 0.50957 | valid_mse: 0.88134 |  0:00:04s\n",
      "epoch 26 | loss: 0.5063  | valid_mse: 0.89328 |  0:00:04s\n",
      "epoch 27 | loss: 0.50166 | valid_mse: 0.91868 |  0:00:04s\n",
      "epoch 28 | loss: 0.48622 | valid_mse: 0.84023 |  0:00:04s\n",
      "\n",
      "Epoch 28: valid_mse improved from 0.8734 to 0.8402\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 29 | loss: 0.49234 | valid_mse: 0.82885 |  0:00:04s\n",
      "\n",
      "Epoch 29: valid_mse improved from 0.8402 to 0.8289\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 30 | loss: 0.48659 | valid_mse: 0.84699 |  0:00:05s\n",
      "epoch 31 | loss: 0.49167 | valid_mse: 0.77012 |  0:00:05s\n",
      "\n",
      "Epoch 31: valid_mse improved from 0.8289 to 0.7701\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 32 | loss: 0.4809  | valid_mse: 0.69513 |  0:00:05s\n",
      "\n",
      "Epoch 32: valid_mse improved from 0.7701 to 0.6951\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 33 | loss: 0.48832 | valid_mse: 0.69573 |  0:00:05s\n",
      "epoch 34 | loss: 0.49594 | valid_mse: 0.73664 |  0:00:05s\n",
      "epoch 35 | loss: 0.48719 | valid_mse: 0.82291 |  0:00:06s\n",
      "epoch 36 | loss: 0.4845  | valid_mse: 0.81967 |  0:00:06s\n",
      "epoch 37 | loss: 0.47993 | valid_mse: 0.79356 |  0:00:06s\n",
      "epoch 38 | loss: 0.4773  | valid_mse: 0.84672 |  0:00:06s\n",
      "epoch 39 | loss: 0.49338 | valid_mse: 0.80468 |  0:00:06s\n",
      "epoch 40 | loss: 0.47762 | valid_mse: 0.71755 |  0:00:06s\n",
      "epoch 41 | loss: 0.48006 | valid_mse: 0.71379 |  0:00:07s\n",
      "epoch 42 | loss: 0.47483 | valid_mse: 0.78924 |  0:00:07s\n",
      "epoch 43 | loss: 0.47439 | valid_mse: 0.79488 |  0:00:07s\n",
      "epoch 44 | loss: 0.47298 | valid_mse: 0.73833 |  0:00:07s\n",
      "epoch 45 | loss: 0.47128 | valid_mse: 0.72146 |  0:00:07s\n",
      "epoch 46 | loss: 0.46921 | valid_mse: 0.74146 |  0:00:08s\n",
      "epoch 47 | loss: 0.4765  | valid_mse: 0.79849 |  0:00:08s\n",
      "epoch 48 | loss: 0.4637  | valid_mse: 0.79884 |  0:00:08s\n",
      "epoch 49 | loss: 0.46969 | valid_mse: 0.7703  |  0:00:08s\n",
      "epoch 50 | loss: 0.47173 | valid_mse: 0.74856 |  0:00:08s\n",
      "epoch 51 | loss: 0.46659 | valid_mse: 0.76875 |  0:00:08s\n",
      "epoch 52 | loss: 0.46322 | valid_mse: 0.74775 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 32 and best_valid_mse = 0.69513\n",
      "Fold 2 - Train QWK: 0.6724, Validation QWK: 0.3778\n",
      "Training fold 3/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "epoch 0  | loss: 2.34845 | valid_mse: 103.67482|  0:00:00s\n",
      "\n",
      "Epoch 0: valid_mse improved from inf to 103.6748\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 1  | loss: 1.49917 | valid_mse: 84.03341|  0:00:00s\n",
      "\n",
      "Epoch 1: valid_mse improved from 103.6748 to 84.0334\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 2  | loss: 1.10803 | valid_mse: 54.16368|  0:00:00s\n",
      "\n",
      "Epoch 2: valid_mse improved from 84.0334 to 54.1637\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 3  | loss: 0.91974 | valid_mse: 46.20956|  0:00:00s\n",
      "\n",
      "Epoch 3: valid_mse improved from 54.1637 to 46.2096\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 4  | loss: 0.91213 | valid_mse: 16.07924|  0:00:01s\n",
      "\n",
      "Epoch 4: valid_mse improved from 46.2096 to 16.0792\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 5  | loss: 0.79521 | valid_mse: 3.7401  |  0:00:01s\n",
      "\n",
      "Epoch 5: valid_mse improved from 16.0792 to 3.7401\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 6  | loss: 0.6797  | valid_mse: 1.02841 |  0:00:01s\n",
      "\n",
      "Epoch 6: valid_mse improved from 3.7401 to 1.0284\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 7  | loss: 0.62898 | valid_mse: 0.99911 |  0:00:01s\n",
      "\n",
      "Epoch 7: valid_mse improved from 1.0284 to 0.9991\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 8  | loss: 0.62023 | valid_mse: 0.87417 |  0:00:01s\n",
      "\n",
      "Epoch 8: valid_mse improved from 0.9991 to 0.8742\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 9  | loss: 0.608   | valid_mse: 0.89064 |  0:00:02s\n",
      "epoch 10 | loss: 0.59295 | valid_mse: 0.89794 |  0:00:02s\n",
      "epoch 11 | loss: 0.57288 | valid_mse: 0.82443 |  0:00:02s\n",
      "\n",
      "Epoch 11: valid_mse improved from 0.8742 to 0.8244\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 12 | loss: 0.5643  | valid_mse: 0.83377 |  0:00:02s\n",
      "epoch 13 | loss: 0.55573 | valid_mse: 2.65453 |  0:00:02s\n",
      "epoch 14 | loss: 0.54899 | valid_mse: 1.47663 |  0:00:03s\n",
      "epoch 15 | loss: 0.54772 | valid_mse: 1.0478  |  0:00:03s\n",
      "epoch 16 | loss: 0.54256 | valid_mse: 0.73187 |  0:00:03s\n",
      "\n",
      "Epoch 16: valid_mse improved from 0.8244 to 0.7319\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 17 | loss: 0.53588 | valid_mse: 0.79562 |  0:00:03s\n",
      "epoch 18 | loss: 0.52447 | valid_mse: 0.81362 |  0:00:03s\n",
      "epoch 19 | loss: 0.51966 | valid_mse: 0.76586 |  0:00:03s\n",
      "epoch 20 | loss: 0.52285 | valid_mse: 0.76904 |  0:00:04s\n",
      "epoch 21 | loss: 0.51311 | valid_mse: 0.74864 |  0:00:04s\n",
      "epoch 22 | loss: 0.52278 | valid_mse: 0.82125 |  0:00:04s\n",
      "epoch 23 | loss: 0.50479 | valid_mse: 1.00867 |  0:00:04s\n",
      "epoch 24 | loss: 0.50995 | valid_mse: 1.36372 |  0:00:04s\n",
      "epoch 25 | loss: 0.4996  | valid_mse: 1.76303 |  0:00:04s\n",
      "epoch 26 | loss: 0.5044  | valid_mse: 1.81214 |  0:00:04s\n",
      "epoch 27 | loss: 0.51691 | valid_mse: 1.13546 |  0:00:05s\n",
      "epoch 28 | loss: 0.51786 | valid_mse: 1.00292 |  0:00:05s\n",
      "epoch 29 | loss: 0.49546 | valid_mse: 0.85394 |  0:00:05s\n",
      "epoch 30 | loss: 0.51039 | valid_mse: 0.77008 |  0:00:05s\n",
      "epoch 31 | loss: 0.49688 | valid_mse: 0.72568 |  0:00:05s\n",
      "\n",
      "Epoch 31: valid_mse improved from 0.7319 to 0.7257\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 32 | loss: 0.50526 | valid_mse: 0.83545 |  0:00:06s\n",
      "epoch 33 | loss: 0.4917  | valid_mse: 0.89696 |  0:00:06s\n",
      "epoch 34 | loss: 0.50007 | valid_mse: 0.88539 |  0:00:06s\n",
      "epoch 35 | loss: 0.49829 | valid_mse: 0.76981 |  0:00:06s\n",
      "epoch 36 | loss: 0.48969 | valid_mse: 0.70517 |  0:00:06s\n",
      "\n",
      "Epoch 36: valid_mse improved from 0.7257 to 0.7052\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 37 | loss: 0.48121 | valid_mse: 0.64774 |  0:00:06s\n",
      "\n",
      "Epoch 37: valid_mse improved from 0.7052 to 0.6477\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 38 | loss: 0.48139 | valid_mse: 0.671   |  0:00:07s\n",
      "epoch 39 | loss: 0.48437 | valid_mse: 0.62963 |  0:00:07s\n",
      "\n",
      "Epoch 39: valid_mse improved from 0.6477 to 0.6296\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 40 | loss: 0.48418 | valid_mse: 0.59213 |  0:00:07s\n",
      "\n",
      "Epoch 40: valid_mse improved from 0.6296 to 0.5921\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 41 | loss: 0.47954 | valid_mse: 0.60618 |  0:00:07s\n",
      "epoch 42 | loss: 0.47555 | valid_mse: 0.64116 |  0:00:07s\n",
      "epoch 43 | loss: 0.46549 | valid_mse: 0.64321 |  0:00:08s\n",
      "epoch 44 | loss: 0.46466 | valid_mse: 0.63128 |  0:00:08s\n",
      "epoch 45 | loss: 0.46673 | valid_mse: 0.65507 |  0:00:08s\n",
      "epoch 46 | loss: 0.46512 | valid_mse: 0.6631  |  0:00:09s\n",
      "epoch 47 | loss: 0.46338 | valid_mse: 0.668   |  0:00:09s\n",
      "epoch 48 | loss: 0.46409 | valid_mse: 0.62165 |  0:00:09s\n",
      "epoch 49 | loss: 0.46187 | valid_mse: 0.59622 |  0:00:09s\n",
      "epoch 50 | loss: 0.45599 | valid_mse: 0.611   |  0:00:09s\n",
      "epoch 51 | loss: 0.46177 | valid_mse: 0.65395 |  0:00:09s\n",
      "epoch 52 | loss: 0.45842 | valid_mse: 0.62222 |  0:00:10s\n",
      "epoch 53 | loss: 0.45358 | valid_mse: 0.60266 |  0:00:10s\n",
      "epoch 54 | loss: 0.44507 | valid_mse: 0.58754 |  0:00:10s\n",
      "\n",
      "Epoch 54: valid_mse improved from 0.5921 to 0.5875\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 55 | loss: 0.45182 | valid_mse: 0.60115 |  0:00:10s\n",
      "epoch 56 | loss: 0.46002 | valid_mse: 0.59389 |  0:00:10s\n",
      "epoch 57 | loss: 0.4649  | valid_mse: 0.56527 |  0:00:10s\n",
      "\n",
      "Epoch 57: valid_mse improved from 0.5875 to 0.5653\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 58 | loss: 0.45382 | valid_mse: 0.54621 |  0:00:11s\n",
      "\n",
      "Epoch 58: valid_mse improved from 0.5653 to 0.5462\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 59 | loss: 0.44934 | valid_mse: 0.54259 |  0:00:11s\n",
      "\n",
      "Epoch 59: valid_mse improved from 0.5462 to 0.5426\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 60 | loss: 0.45207 | valid_mse: 0.5496  |  0:00:11s\n",
      "epoch 61 | loss: 0.45318 | valid_mse: 0.59425 |  0:00:11s\n",
      "epoch 62 | loss: 0.45364 | valid_mse: 0.5691  |  0:00:11s\n",
      "epoch 63 | loss: 0.44946 | valid_mse: 0.53374 |  0:00:11s\n",
      "\n",
      "Epoch 63: valid_mse improved from 0.5426 to 0.5337\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 64 | loss: 0.44948 | valid_mse: 0.54564 |  0:00:12s\n",
      "epoch 65 | loss: 0.43849 | valid_mse: 0.54626 |  0:00:12s\n",
      "epoch 66 | loss: 0.44031 | valid_mse: 0.55607 |  0:00:12s\n",
      "epoch 67 | loss: 0.45364 | valid_mse: 0.56521 |  0:00:12s\n",
      "epoch 68 | loss: 0.44365 | valid_mse: 0.55857 |  0:00:12s\n",
      "epoch 69 | loss: 0.4368  | valid_mse: 0.54821 |  0:00:12s\n",
      "epoch 70 | loss: 0.44707 | valid_mse: 0.56715 |  0:00:13s\n",
      "epoch 71 | loss: 0.44039 | valid_mse: 0.55123 |  0:00:13s\n",
      "epoch 72 | loss: 0.43614 | valid_mse: 0.52601 |  0:00:13s\n",
      "\n",
      "Epoch 72: valid_mse improved from 0.5337 to 0.5260\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 73 | loss: 0.44405 | valid_mse: 0.53039 |  0:00:14s\n",
      "epoch 74 | loss: 0.43453 | valid_mse: 0.54996 |  0:00:14s\n",
      "epoch 75 | loss: 0.43097 | valid_mse: 0.55553 |  0:00:14s\n",
      "epoch 76 | loss: 0.43679 | valid_mse: 0.53712 |  0:00:14s\n",
      "epoch 77 | loss: 0.43678 | valid_mse: 0.52857 |  0:00:14s\n",
      "epoch 78 | loss: 0.43077 | valid_mse: 0.53664 |  0:00:14s\n",
      "epoch 79 | loss: 0.42865 | valid_mse: 0.53211 |  0:00:14s\n",
      "epoch 80 | loss: 0.42437 | valid_mse: 0.51236 |  0:00:15s\n",
      "\n",
      "Epoch 80: valid_mse improved from 0.5260 to 0.5124\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 81 | loss: 0.42616 | valid_mse: 0.50553 |  0:00:15s\n",
      "\n",
      "Epoch 81: valid_mse improved from 0.5124 to 0.5055\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 82 | loss: 0.42848 | valid_mse: 0.51461 |  0:00:15s\n",
      "epoch 83 | loss: 0.42128 | valid_mse: 0.49207 |  0:00:15s\n",
      "\n",
      "Epoch 83: valid_mse improved from 0.5055 to 0.4921\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 84 | loss: 0.421   | valid_mse: 0.4885  |  0:00:15s\n",
      "\n",
      "Epoch 84: valid_mse improved from 0.4921 to 0.4885\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 85 | loss: 0.42502 | valid_mse: 0.51106 |  0:00:16s\n",
      "epoch 86 | loss: 0.42673 | valid_mse: 0.52126 |  0:00:16s\n",
      "epoch 87 | loss: 0.41944 | valid_mse: 0.5139  |  0:00:16s\n",
      "epoch 88 | loss: 0.41781 | valid_mse: 0.48877 |  0:00:16s\n",
      "epoch 89 | loss: 0.42552 | valid_mse: 0.47439 |  0:00:16s\n",
      "\n",
      "Epoch 89: valid_mse improved from 0.4885 to 0.4744\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 90 | loss: 0.43075 | valid_mse: 0.48005 |  0:00:16s\n",
      "epoch 91 | loss: 0.42651 | valid_mse: 0.48772 |  0:00:17s\n",
      "epoch 92 | loss: 0.42115 | valid_mse: 0.48238 |  0:00:17s\n",
      "epoch 93 | loss: 0.42344 | valid_mse: 0.48784 |  0:00:17s\n",
      "epoch 94 | loss: 0.42272 | valid_mse: 0.49368 |  0:00:17s\n",
      "epoch 95 | loss: 0.41988 | valid_mse: 0.5065  |  0:00:17s\n",
      "epoch 96 | loss: 0.42289 | valid_mse: 0.49363 |  0:00:17s\n",
      "epoch 97 | loss: 0.41563 | valid_mse: 0.4846  |  0:00:17s\n",
      "epoch 98 | loss: 0.42976 | valid_mse: 0.51369 |  0:00:18s\n",
      "epoch 99 | loss: 0.43524 | valid_mse: 0.54529 |  0:00:18s\n",
      "epoch 100| loss: 0.42589 | valid_mse: 0.54623 |  0:00:18s\n",
      "epoch 101| loss: 0.43546 | valid_mse: 0.51262 |  0:00:18s\n",
      "epoch 102| loss: 0.42765 | valid_mse: 0.48551 |  0:00:18s\n",
      "epoch 103| loss: 0.4315  | valid_mse: 0.49351 |  0:00:18s\n",
      "epoch 104| loss: 0.44117 | valid_mse: 0.49173 |  0:00:19s\n",
      "epoch 105| loss: 0.43239 | valid_mse: 0.48334 |  0:00:19s\n",
      "epoch 106| loss: 0.43865 | valid_mse: 0.49163 |  0:00:19s\n",
      "epoch 107| loss: 0.42735 | valid_mse: 0.5022  |  0:00:19s\n",
      "epoch 108| loss: 0.43006 | valid_mse: 0.49573 |  0:00:19s\n",
      "epoch 109| loss: 0.42845 | valid_mse: 0.48451 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_valid_mse = 0.47439\n",
      "Fold 3 - Train QWK: 0.7124, Validation QWK: 0.3568\n",
      "Training fold 4/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "epoch 0  | loss: 2.49083 | valid_mse: 79.94446|  0:00:00s\n",
      "\n",
      "Epoch 0: valid_mse improved from inf to 79.9445\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 1  | loss: 1.43372 | valid_mse: 56.39555|  0:00:00s\n",
      "\n",
      "Epoch 1: valid_mse improved from 79.9445 to 56.3956\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 2  | loss: 0.93375 | valid_mse: 35.44988|  0:00:00s\n",
      "\n",
      "Epoch 2: valid_mse improved from 56.3956 to 35.4499\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 3  | loss: 0.86623 | valid_mse: 12.06088|  0:00:00s\n",
      "\n",
      "Epoch 3: valid_mse improved from 35.4499 to 12.0609\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 4  | loss: 0.76565 | valid_mse: 4.27918 |  0:00:01s\n",
      "\n",
      "Epoch 4: valid_mse improved from 12.0609 to 4.2792\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 5  | loss: 0.69178 | valid_mse: 3.04006 |  0:00:01s\n",
      "\n",
      "Epoch 5: valid_mse improved from 4.2792 to 3.0401\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 6  | loss: 0.62227 | valid_mse: 1.67216 |  0:00:01s\n",
      "\n",
      "Epoch 6: valid_mse improved from 3.0401 to 1.6722\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 7  | loss: 0.61381 | valid_mse: 2.35278 |  0:00:01s\n",
      "epoch 8  | loss: 0.58289 | valid_mse: 1.82935 |  0:00:01s\n",
      "epoch 9  | loss: 0.56543 | valid_mse: 1.88299 |  0:00:02s\n",
      "epoch 10 | loss: 0.54203 | valid_mse: 1.25072 |  0:00:02s\n",
      "\n",
      "Epoch 10: valid_mse improved from 1.6722 to 1.2507\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 11 | loss: 0.53528 | valid_mse: 1.39855 |  0:00:02s\n",
      "epoch 12 | loss: 0.54322 | valid_mse: 1.71166 |  0:00:02s\n",
      "epoch 13 | loss: 0.52961 | valid_mse: 1.90211 |  0:00:02s\n",
      "epoch 14 | loss: 0.51825 | valid_mse: 2.42873 |  0:00:03s\n",
      "epoch 15 | loss: 0.5331  | valid_mse: 3.57476 |  0:00:03s\n",
      "epoch 16 | loss: 0.53066 | valid_mse: 3.38675 |  0:00:03s\n",
      "epoch 17 | loss: 0.51027 | valid_mse: 3.90075 |  0:00:03s\n",
      "epoch 18 | loss: 0.49448 | valid_mse: 3.91328 |  0:00:03s\n",
      "epoch 19 | loss: 0.50968 | valid_mse: 2.90572 |  0:00:03s\n",
      "epoch 20 | loss: 0.50006 | valid_mse: 1.91538 |  0:00:04s\n",
      "epoch 21 | loss: 0.50866 | valid_mse: 1.67515 |  0:00:04s\n",
      "epoch 22 | loss: 0.50747 | valid_mse: 1.93254 |  0:00:04s\n",
      "epoch 23 | loss: 0.49109 | valid_mse: 1.92623 |  0:00:04s\n",
      "epoch 24 | loss: 0.50885 | valid_mse: 1.68365 |  0:00:04s\n",
      "epoch 25 | loss: 0.49712 | valid_mse: 1.58652 |  0:00:05s\n",
      "epoch 26 | loss: 0.49242 | valid_mse: 2.20642 |  0:00:05s\n",
      "epoch 27 | loss: 0.49732 | valid_mse: 2.74556 |  0:00:05s\n",
      "epoch 28 | loss: 0.48577 | valid_mse: 2.63603 |  0:00:05s\n",
      "epoch 29 | loss: 0.48828 | valid_mse: 2.17305 |  0:00:05s\n",
      "epoch 30 | loss: 0.48441 | valid_mse: 1.92313 |  0:00:05s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_valid_mse = 1.25072\n",
      "Fold 4 - Train QWK: 0.6690, Validation QWK: 0.2428\n",
      "Training fold 5/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "epoch 0  | loss: 2.31117 | valid_mse: 56.92822|  0:00:00s\n",
      "\n",
      "Epoch 0: valid_mse improved from inf to 56.9282\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 1  | loss: 1.46327 | valid_mse: 21.08238|  0:00:00s\n",
      "\n",
      "Epoch 1: valid_mse improved from 56.9282 to 21.0824\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 2  | loss: 1.13931 | valid_mse: 10.74614|  0:00:00s\n",
      "\n",
      "Epoch 2: valid_mse improved from 21.0824 to 10.7461\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 3  | loss: 0.89454 | valid_mse: 8.38869 |  0:00:00s\n",
      "\n",
      "Epoch 3: valid_mse improved from 10.7461 to 8.3887\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 4  | loss: 0.75224 | valid_mse: 35.57783|  0:00:00s\n",
      "epoch 5  | loss: 0.66439 | valid_mse: 23.66044|  0:00:01s\n",
      "epoch 6  | loss: 0.6315  | valid_mse: 15.35128|  0:00:01s\n",
      "epoch 7  | loss: 0.59477 | valid_mse: 9.73375 |  0:00:01s\n",
      "epoch 8  | loss: 0.58484 | valid_mse: 9.17425 |  0:00:01s\n",
      "epoch 9  | loss: 0.55286 | valid_mse: 8.21222 |  0:00:01s\n",
      "\n",
      "Epoch 9: valid_mse improved from 8.3887 to 8.2122\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 10 | loss: 0.55464 | valid_mse: 7.99758 |  0:00:01s\n",
      "\n",
      "Epoch 10: valid_mse improved from 8.2122 to 7.9976\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 11 | loss: 0.52768 | valid_mse: 7.77378 |  0:00:02s\n",
      "\n",
      "Epoch 11: valid_mse improved from 7.9976 to 7.7738\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 12 | loss: 0.52397 | valid_mse: 8.81259 |  0:00:02s\n",
      "epoch 13 | loss: 0.51986 | valid_mse: 10.9524 |  0:00:02s\n",
      "epoch 14 | loss: 0.50824 | valid_mse: 12.07195|  0:00:02s\n",
      "epoch 15 | loss: 0.5099  | valid_mse: 9.65799 |  0:00:02s\n",
      "epoch 16 | loss: 0.50462 | valid_mse: 7.17593 |  0:00:03s\n",
      "\n",
      "Epoch 16: valid_mse improved from 7.7738 to 7.1759\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 17 | loss: 0.49386 | valid_mse: 5.52936 |  0:00:03s\n",
      "\n",
      "Epoch 17: valid_mse improved from 7.1759 to 5.5294\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 18 | loss: 0.47198 | valid_mse: 5.51413 |  0:00:03s\n",
      "\n",
      "Epoch 18: valid_mse improved from 5.5294 to 5.5141\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 19 | loss: 0.48001 | valid_mse: 5.36838 |  0:00:03s\n",
      "\n",
      "Epoch 19: valid_mse improved from 5.5141 to 5.3684\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 20 | loss: 0.48557 | valid_mse: 4.46908 |  0:00:03s\n",
      "\n",
      "Epoch 20: valid_mse improved from 5.3684 to 4.4691\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 21 | loss: 0.48655 | valid_mse: 1.7854  |  0:00:04s\n",
      "\n",
      "Epoch 21: valid_mse improved from 4.4691 to 1.7854\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 22 | loss: 0.47037 | valid_mse: 1.92035 |  0:00:04s\n",
      "epoch 23 | loss: 0.47336 | valid_mse: 2.07996 |  0:00:04s\n",
      "epoch 24 | loss: 0.45701 | valid_mse: 2.28118 |  0:00:04s\n",
      "epoch 25 | loss: 0.46734 | valid_mse: 2.37284 |  0:00:04s\n",
      "epoch 26 | loss: 0.45231 | valid_mse: 2.20537 |  0:00:04s\n",
      "epoch 27 | loss: 0.45853 | valid_mse: 2.16523 |  0:00:05s\n",
      "epoch 28 | loss: 0.46282 | valid_mse: 2.2844  |  0:00:05s\n",
      "epoch 29 | loss: 0.45361 | valid_mse: 2.23401 |  0:00:05s\n",
      "epoch 30 | loss: 0.4506  | valid_mse: 1.9466  |  0:00:05s\n",
      "epoch 31 | loss: 0.45671 | valid_mse: 1.87602 |  0:00:05s\n",
      "epoch 32 | loss: 0.45541 | valid_mse: 2.12675 |  0:00:06s\n",
      "epoch 33 | loss: 0.45448 | valid_mse: 2.31295 |  0:00:06s\n",
      "epoch 34 | loss: 0.46365 | valid_mse: 2.32559 |  0:00:06s\n",
      "epoch 35 | loss: 0.45447 | valid_mse: 2.0369  |  0:00:06s\n",
      "epoch 36 | loss: 0.44767 | valid_mse: 1.80237 |  0:00:06s\n",
      "epoch 37 | loss: 0.45576 | valid_mse: 1.79456 |  0:00:06s\n",
      "epoch 38 | loss: 0.44853 | valid_mse: 1.91565 |  0:00:07s\n",
      "epoch 39 | loss: 0.44652 | valid_mse: 1.91269 |  0:00:07s\n",
      "epoch 40 | loss: 0.44877 | valid_mse: 1.75991 |  0:00:07s\n",
      "\n",
      "Epoch 40: valid_mse improved from 1.7854 to 1.7599\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 41 | loss: 0.44336 | valid_mse: 1.64371 |  0:00:07s\n",
      "\n",
      "Epoch 41: valid_mse improved from 1.7599 to 1.6437\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 42 | loss: 0.44847 | valid_mse: 1.45261 |  0:00:07s\n",
      "\n",
      "Epoch 42: valid_mse improved from 1.6437 to 1.4526\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 43 | loss: 0.43408 | valid_mse: 1.50456 |  0:00:08s\n",
      "epoch 44 | loss: 0.43822 | valid_mse: 1.39899 |  0:00:08s\n",
      "\n",
      "Epoch 44: valid_mse improved from 1.4526 to 1.3990\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 45 | loss: 0.4388  | valid_mse: 1.22086 |  0:00:08s\n",
      "\n",
      "Epoch 45: valid_mse improved from 1.3990 to 1.2209\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 46 | loss: 0.43046 | valid_mse: 1.1803  |  0:00:08s\n",
      "\n",
      "Epoch 46: valid_mse improved from 1.2209 to 1.1803\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 47 | loss: 0.43156 | valid_mse: 1.13104 |  0:00:08s\n",
      "\n",
      "Epoch 47: valid_mse improved from 1.1803 to 1.1310\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 48 | loss: 0.43251 | valid_mse: 1.10845 |  0:00:09s\n",
      "\n",
      "Epoch 48: valid_mse improved from 1.1310 to 1.1084\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 49 | loss: 0.42917 | valid_mse: 1.27262 |  0:00:09s\n",
      "epoch 50 | loss: 0.4249  | valid_mse: 1.40528 |  0:00:09s\n",
      "epoch 51 | loss: 0.42531 | valid_mse: 1.29174 |  0:00:09s\n",
      "epoch 52 | loss: 0.42755 | valid_mse: 1.12797 |  0:00:09s\n",
      "epoch 53 | loss: 0.43063 | valid_mse: 1.04652 |  0:00:09s\n",
      "\n",
      "Epoch 53: valid_mse improved from 1.1084 to 1.0465\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 54 | loss: 0.43172 | valid_mse: 1.02076 |  0:00:10s\n",
      "\n",
      "Epoch 54: valid_mse improved from 1.0465 to 1.0208\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 55 | loss: 0.43656 | valid_mse: 1.00554 |  0:00:10s\n",
      "\n",
      "Epoch 55: valid_mse improved from 1.0208 to 1.0055\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 56 | loss: 0.43849 | valid_mse: 1.05441 |  0:00:10s\n",
      "epoch 57 | loss: 0.44088 | valid_mse: 1.05472 |  0:00:10s\n",
      "epoch 58 | loss: 0.4299  | valid_mse: 0.9654  |  0:00:10s\n",
      "\n",
      "Epoch 58: valid_mse improved from 1.0055 to 0.9654\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 59 | loss: 0.4368  | valid_mse: 0.97043 |  0:00:11s\n",
      "epoch 60 | loss: 0.43457 | valid_mse: 0.9924  |  0:00:11s\n",
      "epoch 61 | loss: 0.43452 | valid_mse: 1.00441 |  0:00:11s\n",
      "epoch 62 | loss: 0.44017 | valid_mse: 0.95544 |  0:00:11s\n",
      "\n",
      "Epoch 62: valid_mse improved from 0.9654 to 0.9554\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 63 | loss: 0.43999 | valid_mse: 0.95552 |  0:00:11s\n",
      "epoch 64 | loss: 0.43153 | valid_mse: 0.9295  |  0:00:11s\n",
      "\n",
      "Epoch 64: valid_mse improved from 0.9554 to 0.9295\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 65 | loss: 0.43621 | valid_mse: 0.88998 |  0:00:12s\n",
      "\n",
      "Epoch 65: valid_mse improved from 0.9295 to 0.8900\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 66 | loss: 0.44345 | valid_mse: 0.87165 |  0:00:12s\n",
      "\n",
      "Epoch 66: valid_mse improved from 0.8900 to 0.8716\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 67 | loss: 0.42058 | valid_mse: 0.88172 |  0:00:12s\n",
      "epoch 68 | loss: 0.43516 | valid_mse: 0.83312 |  0:00:12s\n",
      "\n",
      "Epoch 68: valid_mse improved from 0.8716 to 0.8331\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 69 | loss: 0.42264 | valid_mse: 0.74395 |  0:00:13s\n",
      "\n",
      "Epoch 69: valid_mse improved from 0.8331 to 0.7439\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 70 | loss: 0.4244  | valid_mse: 0.73306 |  0:00:13s\n",
      "\n",
      "Epoch 70: valid_mse improved from 0.7439 to 0.7331\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 71 | loss: 0.42673 | valid_mse: 0.75001 |  0:00:13s\n",
      "epoch 72 | loss: 0.43194 | valid_mse: 0.74279 |  0:00:13s\n",
      "epoch 73 | loss: 0.4186  | valid_mse: 0.70274 |  0:00:13s\n",
      "\n",
      "Epoch 73: valid_mse improved from 0.7331 to 0.7027\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 74 | loss: 0.42284 | valid_mse: 0.69598 |  0:00:13s\n",
      "\n",
      "Epoch 74: valid_mse improved from 0.7027 to 0.6960\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 75 | loss: 0.41637 | valid_mse: 0.72782 |  0:00:14s\n",
      "epoch 76 | loss: 0.42133 | valid_mse: 0.76061 |  0:00:14s\n",
      "epoch 77 | loss: 0.41666 | valid_mse: 0.74753 |  0:00:14s\n",
      "epoch 78 | loss: 0.42007 | valid_mse: 0.66996 |  0:00:14s\n",
      "\n",
      "Epoch 78: valid_mse improved from 0.6960 to 0.6700\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 79 | loss: 0.4121  | valid_mse: 0.66055 |  0:00:14s\n",
      "\n",
      "Epoch 79: valid_mse improved from 0.6700 to 0.6606\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 80 | loss: 0.41432 | valid_mse: 0.70359 |  0:00:15s\n",
      "epoch 81 | loss: 0.4111  | valid_mse: 0.71414 |  0:00:15s\n",
      "epoch 82 | loss: 0.41866 | valid_mse: 0.68644 |  0:00:15s\n",
      "epoch 83 | loss: 0.41131 | valid_mse: 0.66238 |  0:00:15s\n",
      "epoch 84 | loss: 0.40794 | valid_mse: 0.69103 |  0:00:15s\n",
      "epoch 85 | loss: 0.40429 | valid_mse: 0.71677 |  0:00:15s\n",
      "epoch 86 | loss: 0.40586 | valid_mse: 0.66707 |  0:00:16s\n",
      "epoch 87 | loss: 0.40869 | valid_mse: 0.62893 |  0:00:16s\n",
      "\n",
      "Epoch 87: valid_mse improved from 0.6606 to 0.6289\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 88 | loss: 0.40051 | valid_mse: 0.65372 |  0:00:16s\n",
      "epoch 89 | loss: 0.40503 | valid_mse: 0.7033  |  0:00:16s\n",
      "epoch 90 | loss: 0.40989 | valid_mse: 0.71107 |  0:00:16s\n",
      "epoch 91 | loss: 0.41163 | valid_mse: 0.64819 |  0:00:16s\n",
      "epoch 92 | loss: 0.40217 | valid_mse: 0.62907 |  0:00:17s\n",
      "epoch 93 | loss: 0.40505 | valid_mse: 0.65171 |  0:00:17s\n",
      "epoch 94 | loss: 0.39704 | valid_mse: 0.68125 |  0:00:17s\n",
      "epoch 95 | loss: 0.41157 | valid_mse: 0.65402 |  0:00:17s\n",
      "epoch 96 | loss: 0.40367 | valid_mse: 0.61427 |  0:00:17s\n",
      "\n",
      "Epoch 96: valid_mse improved from 0.6289 to 0.6143\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 97 | loss: 0.4024  | valid_mse: 0.60202 |  0:00:18s\n",
      "\n",
      "Epoch 97: valid_mse improved from 0.6143 to 0.6020\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 98 | loss: 0.39949 | valid_mse: 0.61623 |  0:00:18s\n",
      "epoch 99 | loss: 0.40239 | valid_mse: 0.62966 |  0:00:18s\n",
      "epoch 100| loss: 0.40021 | valid_mse: 0.61705 |  0:00:18s\n",
      "epoch 101| loss: 0.40475 | valid_mse: 0.60575 |  0:00:18s\n",
      "epoch 102| loss: 0.3961  | valid_mse: 0.6212  |  0:00:18s\n",
      "epoch 103| loss: 0.40762 | valid_mse: 0.61663 |  0:00:19s\n",
      "epoch 104| loss: 0.40235 | valid_mse: 0.56757 |  0:00:19s\n",
      "\n",
      "Epoch 104: valid_mse improved from 0.6020 to 0.5676\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 105| loss: 0.4029  | valid_mse: 0.54866 |  0:00:19s\n",
      "\n",
      "Epoch 105: valid_mse improved from 0.5676 to 0.5487\n",
      "Successfully saved model at best_tabnet_model.pt.zip\n",
      "epoch 106| loss: 0.39624 | valid_mse: 0.58654 |  0:00:19s\n",
      "epoch 107| loss: 0.39794 | valid_mse: 0.59839 |  0:00:19s\n",
      "epoch 108| loss: 0.39005 | valid_mse: 0.57269 |  0:00:20s\n",
      "epoch 109| loss: 0.39453 | valid_mse: 0.56274 |  0:00:20s\n",
      "epoch 110| loss: 0.39275 | valid_mse: 0.57589 |  0:00:20s\n",
      "epoch 111| loss: 0.38553 | valid_mse: 0.59971 |  0:00:20s\n",
      "epoch 112| loss: 0.38394 | valid_mse: 0.58435 |  0:00:20s\n",
      "epoch 113| loss: 0.37733 | valid_mse: 0.58599 |  0:00:20s\n",
      "epoch 114| loss: 0.37797 | valid_mse: 0.58438 |  0:00:21s\n",
      "epoch 115| loss: 0.37924 | valid_mse: 0.58581 |  0:00:21s\n",
      "epoch 116| loss: 0.37524 | valid_mse: 0.59296 |  0:00:21s\n",
      "epoch 117| loss: 0.37689 | valid_mse: 0.60449 |  0:00:21s\n",
      "epoch 118| loss: 0.37451 | valid_mse: 0.60669 |  0:00:21s\n",
      "epoch 119| loss: 0.3749  | valid_mse: 0.6033  |  0:00:21s\n",
      "epoch 120| loss: 0.37469 | valid_mse: 0.59528 |  0:00:22s\n",
      "epoch 121| loss: 0.36958 | valid_mse: 0.5934  |  0:00:22s\n",
      "epoch 122| loss: 0.36659 | valid_mse: 0.59113 |  0:00:22s\n",
      "epoch 123| loss: 0.37318 | valid_mse: 0.58142 |  0:00:22s\n",
      "epoch 124| loss: 0.36896 | valid_mse: 0.5844  |  0:00:22s\n",
      "epoch 125| loss: 0.36345 | valid_mse: 0.58078 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 105 and best_valid_mse = 0.54866\n",
      "Fold 5 - Train QWK: 0.7239, Validation QWK: 0.3000\n",
      "Mean Train QWK --> 0.6880\n",
      "Mean Validation QWK ---> 0.3201\n",
      "KappaOPtimizer.x = [0.63362002 0.95505364 2.41873108]\n",
      "----> || Optimized QWK SCORE :: 0.427\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Train model and make predictions using cross-validation\n",
    "predictions2, trained_models2 = train_all_models_with_cv(train2, test2, target2, best_params_dict2, ensemble_method='voting', weights=weights2)\n",
    "\n",
    "# Generate submission\n",
    "final_predictions2 = next(iter(predictions.values()))\n",
    "submission2 = generate_submission_file(final_predictions2, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccc14b",
   "metadata": {
    "papermill": {
     "duration": 0.142181,
     "end_time": "2024-12-22T10:56:43.971873",
     "exception": false,
     "start_time": "2024-12-22T10:56:43.829692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c5afed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:56:44.304562Z",
     "iopub.status.busy": "2024-12-22T10:56:44.303514Z",
     "iopub.status.idle": "2024-12-22T10:56:51.110880Z",
     "shell.execute_reply": "2024-12-22T10:56:51.109697Z"
    },
    "papermill": {
     "duration": 6.996929,
     "end_time": "2024-12-22T10:56:51.113030",
     "exception": false,
     "start_time": "2024-12-22T10:56:44.116101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after feature engineering:  (3960, 99)\n",
      "Test shape after feature engineering:  (20, 77)\n",
      "Train shape after removing columns:  (3960, 65)\n",
      "Test shape after removing columns:  (20, 64)\n",
      "Final features included in train: ['sii', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-Height', 'Physical-Weight', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age', 'BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight', 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight', 'Sex_BMI', 'Sex_HeartRate', 'Age_WaistCirc', 'BMI_FitnessMaxStage', 'Weight_GripStrengthDominant', 'Weight_GripStrengthNonDominant', 'HeartRate_FitnessTime', 'Age_PushUp', 'FFMI_Age', 'InternetUse_SleepDisturbance', 'CGAS_BMI', 'CGAS_FitnessMaxStage']\n",
      "Deleted: /kaggle/working/best_tabnet_model.pt.zip\n",
      "Deleted: /kaggle/working/__notebook__.ipynb\n",
      "Deleted: /kaggle/working/catboost_info\n",
      "Deleted: /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "n_folds = 5\n",
    "\n",
    "# Set up logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Limit verbosity\n",
    "\n",
    "# Paths\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "sample_path = '/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv'\n",
    "time_series_train = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n",
    "time_series_test = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n",
    "\n",
    "# Toggle for using time-series data\n",
    "use_time_series = False  # Set to False to skip time-series data\n",
    "\n",
    "# Preprocess data\n",
    "train3, test3, sample3 = preprocess_csv_data(train_path, test_path, sample_path)\n",
    "\n",
    "if use_time_series:\n",
    "    train_ts3, test_ts3 = preprocess_time_series_data(time_series_train, time_series_test, use_autoencoder=True, use_imputer=True, impute_method=\"mean\")\n",
    "else:\n",
    "    train_ts3, test_ts3 = None, None\n",
    "train3, test3 = merge_csv_and_time_series(train3, test3, train_ts3, test_ts3, use_time_series=use_time_series, use_numeric_imputation=True, numeric_impute_method=\"knn\")\n",
    "\n",
    "train3 = train3.dropna(subset=['sii'])\n",
    "\n",
    "# Target and features\n",
    "target3 = train3[\"sii\"]\n",
    "train3 = train3.drop(columns=[\"sii\"])  # Drop `sii` from features\n",
    "\n",
    "\n",
    "# Ensure `sii` is not in test data\n",
    "if \"sii\" in test3.columns:\n",
    "    test3 = test3.drop(columns=[\"sii\"])\n",
    "\n",
    "if \"id\" in train3.columns:\n",
    "    train3 = train3.drop(columns=[\"id\"])\n",
    "    test3 = test3.drop(columns=[\"id\"])\n",
    "\n",
    "working_dir = '/kaggle/working'\n",
    "clean_kaggle_working_directory(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c76f899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:56:51.421587Z",
     "iopub.status.busy": "2024-12-22T10:56:51.420916Z",
     "iopub.status.idle": "2024-12-22T10:56:51.427541Z",
     "shell.execute_reply": "2024-12-22T10:56:51.426588Z"
    },
    "papermill": {
     "duration": 0.174604,
     "end_time": "2024-12-22T10:56:51.429466",
     "exception": false,
     "start_time": "2024-12-22T10:56:51.254862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params3 = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params3 = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'auto',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params3 = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'CPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b03c0edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:56:51.720198Z",
     "iopub.status.busy": "2024-12-22T10:56:51.719829Z",
     "iopub.status.idle": "2024-12-22T10:56:51.724275Z",
     "shell.execute_reply": "2024-12-22T10:56:51.723355Z"
    },
    "papermill": {
     "duration": 0.152251,
     "end_time": "2024-12-22T10:56:51.726206",
     "exception": false,
     "start_time": "2024-12-22T10:56:51.573955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_dict3 = {'LightGBM': Params3, 'XGBoost': XGB_Params3, 'CatBoost': CatBoost_Params3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2e751ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:56:52.015262Z",
     "iopub.status.busy": "2024-12-22T10:56:52.014249Z",
     "iopub.status.idle": "2024-12-22T10:58:10.071148Z",
     "shell.execute_reply": "2024-12-22T10:58:10.069916Z"
    },
    "papermill": {
     "duration": 78.203899,
     "end_time": "2024-12-22T10:58:10.073479",
     "exception": false,
     "start_time": "2024-12-22T10:56:51.869580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with CV: Ensemble\n",
      "Training fold 1/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 1 - Train QWK: 0.5183, Validation QWK: 0.3435\n",
      "Training fold 2/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 2 - Train QWK: 0.4806, Validation QWK: 0.4508\n",
      "Training fold 3/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 3 - Train QWK: 0.5718, Validation QWK: 0.3856\n",
      "Training fold 4/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 4 - Train QWK: 0.5938, Validation QWK: 0.3079\n",
      "Training fold 5/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 5 - Train QWK: 0.5466, Validation QWK: 0.3549\n",
      "Mean Train QWK --> 0.5422\n",
      "Mean Validation QWK ---> 0.3685\n",
      "KappaOPtimizer.x = [0.53873933 0.96465826 2.86979107]\n",
      "----> || Optimized QWK SCORE :: 0.461\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Train model and make predictions using cross-validation\n",
    "predictions3, trained_models3 = train_all_models_with_cv(train3, test3, target3, best_params_dict3, ensemble_method='stacking')\n",
    "\n",
    "# Generate submission\n",
    "final_predictions3 = next(iter(predictions.values()))\n",
    "submission3 = generate_submission_file(final_predictions3, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1775c9",
   "metadata": {
    "papermill": {
     "duration": 0.144473,
     "end_time": "2024-12-22T10:58:10.366909",
     "exception": false,
     "start_time": "2024-12-22T10:58:10.222436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **FINAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcf152ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:58:10.714353Z",
     "iopub.status.busy": "2024-12-22T10:58:10.713943Z",
     "iopub.status.idle": "2024-12-22T10:58:10.734782Z",
     "shell.execute_reply": "2024-12-22T10:58:10.733545Z"
    },
    "papermill": {
     "duration": 0.176451,
     "end_time": "2024-12-22T10:58:10.736713",
     "exception": false,
     "start_time": "2024-12-22T10:58:10.560262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Final_Submission.csv'\n"
     ]
    }
   ],
   "source": [
    "sub1 = submission\n",
    "sub2 = submission2\n",
    "sub3 = submission3\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "770651c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:58:11.031736Z",
     "iopub.status.busy": "2024-12-22T10:58:11.031352Z",
     "iopub.status.idle": "2024-12-22T10:58:11.044162Z",
     "shell.execute_reply": "2024-12-22T10:58:11.043220Z"
    },
    "papermill": {
     "duration": 0.164466,
     "end_time": "2024-12-22T10:58:11.046040",
     "exception": false,
     "start_time": "2024-12-22T10:58:10.881574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    0\n",
       "6   0038ba98    1\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    0\n",
       "11  00abe655    0\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    1\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6310681,
     "sourceId": 10210719,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30407,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1842.992846,
   "end_time": "2024-12-22T10:58:15.225118",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-22T10:27:32.232272",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
