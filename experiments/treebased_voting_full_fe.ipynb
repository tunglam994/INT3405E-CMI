{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdd5716",
   "metadata": {
    "papermill": {
     "duration": 0.007832,
     "end_time": "2024-12-19T17:00:14.038690",
     "exception": false,
     "start_time": "2024-12-19T17:00:14.030858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook demonstrates a complete machine learning pipeline, including:\n",
    "- Preprocessing CSV and time-series data.\n",
    "- Using Optuna to tune hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "- Training multiple models with optimized parameters.\n",
    "- Combining predictions using an ensemble method.\n",
    "- Generating a Kaggle submission.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1e570",
   "metadata": {
    "papermill": {
     "duration": 0.007142,
     "end_time": "2024-12-19T17:00:14.053969",
     "exception": false,
     "start_time": "2024-12-19T17:00:14.046827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# **Table of Contents**\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries and Utilities](#libraries-and-utilities)\n",
    "3. [Preprocessing](#preprocessing)\n",
    "    - [Preprocessing CSV Data](#preprocessing-csv-data)\n",
    "    - [Preprocessing Time-Series Data](#preprocessing-time-series-data)\n",
    "    - [Merging Preprocessed Data](#merging-preprocessed-data)\n",
    "4. [Hyperparameter Tuning with Optuna](#hyperparameter-tuning)\n",
    "    - [LightGBM](#lightgbm)\n",
    "    - [XGBoost](#xgboost)\n",
    "    - [CatBoost](#catboost)\n",
    "5. [Model Training](#model-training)\n",
    "6. [Submission](#submission)\n",
    "7. [Main Pipeline](#main-pipeline)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631bd0f",
   "metadata": {
    "papermill": {
     "duration": 0.006457,
     "end_time": "2024-12-19T17:00:14.067156",
     "exception": false,
     "start_time": "2024-12-19T17:00:14.060699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"libraries-and-utilities\"></a>\n",
    "# **Libraries and Utilities**\n",
    "Import the required libraries, including Optuna for hyperparameter optimization.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f4c996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:14.082699Z",
     "iopub.status.busy": "2024-12-19T17:00:14.081949Z",
     "iopub.status.idle": "2024-12-19T17:00:34.099778Z",
     "shell.execute_reply": "2024-12-19T17:00:34.098829Z"
    },
    "papermill": {
     "duration": 20.028102,
     "end_time": "2024-12-19T17:00:34.101924",
     "exception": false,
     "start_time": "2024-12-19T17:00:14.073822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afbe63b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.119320Z",
     "iopub.status.busy": "2024-12-19T17:00:34.118732Z",
     "iopub.status.idle": "2024-12-19T17:00:34.124190Z",
     "shell.execute_reply": "2024-12-19T17:00:34.123360Z"
    },
    "papermill": {
     "duration": 0.015905,
     "end_time": "2024-12-19T17:00:34.125936",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.110031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423e0be",
   "metadata": {
    "papermill": {
     "duration": 0.006387,
     "end_time": "2024-12-19T17:00:34.139126",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.132739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"preprocessing\"></a>\n",
    "# **Preprocessing**\n",
    "This section contains the preprocessing functions for CSV and time-series data, as well as merging them.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0eed84",
   "metadata": {
    "papermill": {
     "duration": 0.006357,
     "end_time": "2024-12-19T17:00:34.152259",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.145902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Imputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f268b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.166829Z",
     "iopub.status.busy": "2024-12-19T17:00:34.166512Z",
     "iopub.status.idle": "2024-12-19T17:00:34.178589Z",
     "shell.execute_reply": "2024-12-19T17:00:34.177732Z"
    },
    "papermill": {
     "duration": 0.021375,
     "end_time": "2024-12-19T17:00:34.180226",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.158851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute(df, method=\"knn\", n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Impute missing values in a DataFrame using the specified method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with missing values.\n",
    "        method (str): Imputation method. Options: \"knn\", \"mean\", \"median\", \"cat\".\n",
    "        n_neighbors (int): Number of neighbors for KNNImputer (if method=\"knn\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Handle categorical columns if method is \"cat\"\n",
    "    if method == \"cat\":\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if len(categorical_cols) > 0:\n",
    "            cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "            df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "        return df\n",
    "\n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\", \"float64\", \"int64\"]).columns.tolist()\n",
    "    \n",
    "    # Remove 'sii' column if present\n",
    "    if \"sii\" in numeric_cols:\n",
    "        numeric_cols.remove(\"sii\")\n",
    "    \n",
    "    # If no numeric columns exist, return the original DataFrame\n",
    "    if not numeric_cols:\n",
    "        print(\"No numeric columns found for imputation. Returning original DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Initialize the appropriate imputer\n",
    "    if method == \"knn\":\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    elif method == \"mean\":\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "    elif method == \"median\":\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown imputation method: {method}\")\n",
    "\n",
    "    # Perform imputation on numeric columns\n",
    "    imputed_data = imputer.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # Create a new DataFrame with imputed numeric data\n",
    "    df_imputed = pd.DataFrame(imputed_data, columns=numeric_cols, index=df.index)\n",
    "    \n",
    "    # Combine imputed numeric data with non-numeric columns\n",
    "    for col in df.columns:\n",
    "        if col not in numeric_cols:\n",
    "            df_imputed[col] = df[col]\n",
    "    \n",
    "    return df_imputed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4d4f0",
   "metadata": {
    "papermill": {
     "duration": 0.006561,
     "end_time": "2024-12-19T17:00:34.193584",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.187023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess CSV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56997ce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.207908Z",
     "iopub.status.busy": "2024-12-19T17:00:34.207625Z",
     "iopub.status.idle": "2024-12-19T17:00:34.216474Z",
     "shell.execute_reply": "2024-12-19T17:00:34.215644Z"
    },
    "papermill": {
     "duration": 0.017879,
     "end_time": "2024-12-19T17:00:34.218053",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.200174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "\n",
    "    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n",
    "    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n",
    "    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n",
    "    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n",
    "    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n",
    "    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n",
    "    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n",
    "    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n",
    "    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n",
    "    df['FFMI_Age'] = df['BIA-BIA_FFMI'] * df['Basic_Demos-Age']\n",
    "    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n",
    "    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n",
    "    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3a1f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.232173Z",
     "iopub.status.busy": "2024-12-19T17:00:34.231927Z",
     "iopub.status.idle": "2024-12-19T17:00:34.239341Z",
     "shell.execute_reply": "2024-12-19T17:00:34.238542Z"
    },
    "papermill": {
     "duration": 0.016264,
     "end_time": "2024-12-19T17:00:34.240928",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.224664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    input_length = len(df)\n",
    "    df = df.drop(df[df['Physical-BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Systolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] > 160].index)\n",
    "\n",
    "    children = df[df['Basic_Demos-Age'] <= 12]\n",
    "    df = df.drop(children[children['FGC-FGC_CU'] > 80].index)\n",
    "    df = df.drop(children[children['FGC-FGC_GSND'] > 80].index)\n",
    "\n",
    "    df = df.drop(df[df['BIA-BIA_BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMC'] > 1000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMR'] > 40000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_DEE'] > 60000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ECW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_FFM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ICW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LDM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LST'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_SMM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_TBW'] > 2000].index)\n",
    "    output_length = len(df)\n",
    "    print (input_length, output_length)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071c98f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.255290Z",
     "iopub.status.busy": "2024-12-19T17:00:34.254807Z",
     "iopub.status.idle": "2024-12-19T17:00:34.260730Z",
     "shell.execute_reply": "2024-12-19T17:00:34.259883Z"
    },
    "papermill": {
     "duration": 0.014675,
     "end_time": "2024-12-19T17:00:34.262227",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.247552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_csv_data(train_path, test_path, sample_path):\n",
    "    \"\"\"\n",
    "    Preprocess CSV data with proper handling of missing columns.\n",
    "    Args:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        test_path (str): Path to the test CSV file.\n",
    "        sample_path (str): Path to the sample submission CSV file\n",
    "    Returns:\n",
    "        tuple: Preprocessed training DataFrame, test DataFrame, and sample submission.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    sample = pd.read_csv(sample_path)\n",
    "\n",
    "    # # remove outlier from train\n",
    "    # train = remove_outliers(train)\n",
    "    # print(\"Train shape after removing outlier: \", train.shape)\n",
    "\n",
    "    # feature engineering for both train and test\n",
    "    train = csv_feature_engineering(train)\n",
    "    test = csv_feature_engineering(test)\n",
    "    print(\"Train shape after feature engineering: \", train.shape)\n",
    "    print(\"Test shape after feature engineering: \", test.shape)\n",
    "\n",
    "    # Only use columns in both train and test\n",
    "    # Ensure that the columns in `train` and `test` match\n",
    "    # Remove some columns\n",
    "    common_columns = test.columns.to_list() \n",
    "    remove_columns = ['BIA-BIA_LDM', 'Physical-Waist_Circumference', 'FGC-FGC_SRL', 'FGC-FGC_GSND', 'BIA-BIA_ECW', 'Physical-BMI', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_FFM', 'BIA-BIA_TBW', 'BIA-BIA_BMR', 'BIA-BIA_ICW', 'BIA-BIA_DEE']\n",
    "    common_columns = [col for col in common_columns if col not in remove_columns]\n",
    "    \n",
    "    train = train[[\"sii\"] + common_columns]\n",
    "    test = test[common_columns]\n",
    "    print(\"Train shape after removing columns: \", train.shape)\n",
    "    print(\"Test shape after removing columns: \", test.shape)\n",
    "\n",
    "    return train, test, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510d259",
   "metadata": {
    "papermill": {
     "duration": 0.006335,
     "end_time": "2024-12-19T17:00:34.275209",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.268874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a01214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.289988Z",
     "iopub.status.busy": "2024-12-19T17:00:34.289408Z",
     "iopub.status.idle": "2024-12-19T17:00:34.294539Z",
     "shell.execute_reply": "2024-12-19T17:00:34.293722Z"
    },
    "papermill": {
     "duration": 0.014118,
     "end_time": "2024-12-19T17:00:34.296079",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.281961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_kaggle_working_directory(working_dir):\n",
    "    \"\"\"\n",
    "    Cleans up all files and folders in the specified Kaggle working directory.\n",
    "\n",
    "    Args:\n",
    "        working_dir (str): Path to the Kaggle working directory.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for item in os.listdir(working_dir):\n",
    "        item_path = os.path.join(working_dir, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)  # Remove file or symbolic link\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  # Remove directory\n",
    "            print(f\"Deleted: {item_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663e5260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.310606Z",
     "iopub.status.busy": "2024-12-19T17:00:34.310323Z",
     "iopub.status.idle": "2024-12-19T17:00:34.329272Z",
     "shell.execute_reply": "2024-12-19T17:00:34.328458Z"
    },
    "papermill": {
     "duration": 0.028073,
     "end_time": "2024-12-19T17:00:34.330835",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.302762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering_ts(worn_data):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the time-series data for a single participant.\n",
    "    Returns a DataFrame with all derived features for this participant.\n",
    "    \"\"\"\n",
    "    mvpa_threshold = 0.1\n",
    "    vig_threshold = 0.5\n",
    "    window_size = 12\n",
    "\n",
    "    # Filter worn data\n",
    "    worn_data = worn_data[worn_data['non-wear_flag'] == 0].copy()\n",
    "\n",
    "    # Time of day conversions\n",
    "    worn_data['time_of_day_hours'] = worn_data['time_of_day'] / 1e9 / 3600\n",
    "    worn_data['day_time'] = worn_data['relative_date_PCIAT'] + (worn_data['time_of_day_hours'] / 24)\n",
    "    worn_data['day_period'] = np.where(\n",
    "        (worn_data['time_of_day_hours'] >= 8) & (worn_data['time_of_day_hours'] < 21),\n",
    "        'day', 'night'\n",
    "    )\n",
    "\n",
    "    # Time differences\n",
    "    worn_data['time_diff'] = (worn_data['day_time'].diff() * 86400).round(0)\n",
    "    worn_data['measurement_after_gap'] = worn_data['time_diff'] > 5\n",
    "\n",
    "    # Classify activity levels\n",
    "    worn_data['activity_type'] = pd.cut(\n",
    "        worn_data['enmo'],\n",
    "        bins=[-np.inf, mvpa_threshold, vig_threshold, np.inf],\n",
    "        labels=['low', 'moderate', 'vigorous']\n",
    "    )\n",
    "\n",
    "    # Aggregate activity periods\n",
    "    activity_group = (\n",
    "        (worn_data['activity_type'] != worn_data['activity_type'].shift()) |\n",
    "        (worn_data['measurement_after_gap'])\n",
    "    ).cumsum()\n",
    "\n",
    "    activity_periods = worn_data.groupby(activity_group).agg(\n",
    "        min=('day_time', 'min'),\n",
    "        max=('day_time', 'max'),\n",
    "        activity_type=('activity_type', 'first')\n",
    "    )\n",
    "    activity_periods['duration_sec'] = (activity_periods['max'] - activity_periods['min']) * 86400 + 5\n",
    "    activity_periods = activity_periods[activity_periods['duration_sec'] >= 60]\n",
    "\n",
    "    # Add day and transition number\n",
    "    activity_periods['day'] = activity_periods['min'].astype(int)\n",
    "    activity_periods['transition_num'] = (\n",
    "        activity_periods.groupby('day')['activity_type']\n",
    "        .apply(lambda x: (x != x.shift()).cumsum())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Calculate activity-level summary statistics\n",
    "    activity_summary = {}\n",
    "    for act_type in ['low', 'moderate']:\n",
    "        activity_data = activity_periods[activity_periods['activity_type'] == act_type]\n",
    "        if activity_data.empty:\n",
    "            for stat in ['median', 'max', 'std']:\n",
    "                activity_summary[f'{act_type}_duration_{stat}'] = 0\n",
    "                activity_summary[f'{act_type}_count_periods_{stat}'] = 0\n",
    "            continue\n",
    "        stats = activity_data.groupby('day').agg(\n",
    "            total_duration=('duration_sec', 'sum'),\n",
    "            count_periods=('duration_sec', 'size')\n",
    "        ).agg(['median', 'max', 'std'])\n",
    "\n",
    "        for stat in ['median', 'max', 'std']:\n",
    "            activity_summary[f'{act_type}_duration_{stat}'] = stats.loc[stat, 'total_duration']\n",
    "            activity_summary[f'{act_type}_count_periods_{stat}'] = stats.loc[stat, 'count_periods']\n",
    "\n",
    "    # Add daily transition statistics\n",
    "    daily_transitions = activity_periods.groupby('day')['transition_num'].max()\n",
    "    trans_stats = daily_transitions.agg(['median', 'max', 'std'])\n",
    "    for stat in ['median', 'max', 'std']:\n",
    "        activity_summary[f'transitions_{stat}'] = trans_stats[stat]\n",
    "\n",
    "    # Hourly activity features\n",
    "    hourly_activity = worn_data.groupby(\n",
    "        [worn_data['relative_date_PCIAT'].astype(int),\n",
    "         worn_data['time_of_day_hours'].astype(int),\n",
    "         worn_data['day_period']]\n",
    "    )['enmo'].agg(['mean', 'max'])\n",
    "\n",
    "    features = hourly_activity['mean'].groupby(\n",
    "        ['relative_date_PCIAT', 'day_period']\n",
    "    ).agg(\n",
    "        std_across_hours='std',\n",
    "        peak_hour=lambda x: x.idxmax()[1] if not x.empty else 0,\n",
    "        entropy=lambda x: -(x / x.sum() * np.log(x / x.sum() + 1e-9)).sum() if not x.empty else 0\n",
    "    )\n",
    "\n",
    "    # Day and night features\n",
    "    if 'day' in features.index.get_level_values('day_period'):\n",
    "        day_features = features.xs('day', level='day_period').agg(['median', 'max', 'std']).stack().to_frame().T\n",
    "        day_features.columns = [f'{stat}_{feature}_day' for stat, feature in day_features.columns]\n",
    "    else:\n",
    "        day_features = pd.DataFrame(columns=[f'{stat}_{feature}_day' for stat in ['median', 'max', 'std'] for feature in ['std_across_hours', 'peak_hour', 'entropy']])\n",
    "\n",
    "    if 'night' in features.index.get_level_values('day_period'):\n",
    "        night_features = features.xs('night', level='day_period').agg(['median', 'max', 'std']).stack().to_frame().T\n",
    "        night_features.columns = [f'{stat}_{feature}_night' for stat, feature in night_features.columns]\n",
    "    else:\n",
    "        night_features = pd.DataFrame(columns=[f'{stat}_{feature}_night' for stat in ['median', 'max', 'std'] for feature in ['std_across_hours', 'peak_hour', 'entropy']])\n",
    "\n",
    "    def merge_mvpa_groups(df, allowed_gap=60, merge_gap=60):\n",
    "        last_mvpa_time = df['day_time'].where(df['is_mvpa']).ffill().shift()\n",
    "        mvpa_time_diff = ((df['day_time'] - last_mvpa_time) * 86400).round(0)\n",
    "        mvpa_group = (\n",
    "            (df['is_mvpa'] != df['is_mvpa'].shift()) |\n",
    "            (df['time_diff'] >= allowed_gap)\n",
    "        ).cumsum()\n",
    "        is_mvpa_start = (\n",
    "            (mvpa_group != mvpa_group.shift()) &\n",
    "            df['is_mvpa']\n",
    "        )\n",
    "        group_increment = is_mvpa_start & (\n",
    "            (mvpa_time_diff >= merge_gap) | last_mvpa_time.isnull()\n",
    "        )\n",
    "        merged_group = group_increment.cumsum()\n",
    "        merged_group.loc[~df['is_mvpa']] = np.nan\n",
    "        return merged_group\n",
    "    \n",
    "    # Calculate daily MVPA statistics\n",
    "    worn_data['is_mvpa'] = worn_data['enmo'] > mvpa_threshold\n",
    "    worn_data['mvpa_merged_group'] = merge_mvpa_groups(worn_data)\n",
    "\n",
    "    mvpa_periods = worn_data[worn_data['is_mvpa']].groupby('mvpa_merged_group')['day_time'].agg(['min', 'max'])\n",
    "    mvpa_periods['duration_sec'] = (mvpa_periods['max'] - mvpa_periods['min']) * 86400\n",
    "    mvpa_periods = mvpa_periods[mvpa_periods['duration_sec'] >= 60]\n",
    "\n",
    "    mvpa_periods['day'] = mvpa_periods['min'].astype(int)\n",
    "    daily_stats = mvpa_periods.groupby('day').agg(\n",
    "        total_duration=('duration_sec', 'sum'),\n",
    "        count_periods=('duration_sec', 'size')\n",
    "    )\n",
    "\n",
    "    # Extract daily stats features\n",
    "    daily_stats_features = daily_stats.agg(\n",
    "        ['median', 'max', 'std']\n",
    "    ).unstack().to_frame().T\n",
    "\n",
    "    daily_stats_features.columns = [\n",
    "        f'{stat}_{feature}' for feature, stat in daily_stats_features.columns\n",
    "    ]\n",
    "\n",
    "    # Combine all features\n",
    "    combined_features = pd.concat(\n",
    "        [pd.DataFrame([activity_summary]), day_features, night_features, daily_stats_features],\n",
    "        axis=1\n",
    "    )\n",
    "    # combined_features.fillna(0, inplace=True)  # Ensure no missing values\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b088de76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.345499Z",
     "iopub.status.busy": "2024-12-19T17:00:34.345201Z",
     "iopub.status.idle": "2024-12-19T17:00:34.351996Z",
     "shell.execute_reply": "2024-12-19T17:00:34.351172Z"
    },
    "papermill": {
     "duration": 0.015696,
     "end_time": "2024-12-19T17:00:34.353530",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.337834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file_and_engineer_features(filename, dirname, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single participant's Parquet file, apply feature engineering, and save features to disk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Parquet file\n",
    "        file_path = os.path.join(dirname, filename, 'part-0.parquet')\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Add participant ID\n",
    "        participant_id = filename.split('=')[1]\n",
    "        df['id'] = participant_id\n",
    "\n",
    "        # Apply feature engineering\n",
    "        features = feature_engineering_ts(df)\n",
    "\n",
    "        # Add participant ID to features\n",
    "        features['id'] = participant_id\n",
    "\n",
    "        # Save the engineered features to disk\n",
    "        output_file = os.path.join(output_dir, f\"{participant_id}_features.parquet\")\n",
    "        features.to_parquet(output_file, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "\n",
    "def load_time_series(dirname, output_dir) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all Parquet files, save engineered features to disk, and return the combined features.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process participants one by one\n",
    "    for filename in tqdm(os.listdir(dirname), desc=\"Processing participants\"):\n",
    "        process_file_and_engineer_features(filename, dirname, output_dir)\n",
    "\n",
    "    # Combine all saved features into a single DataFrame\n",
    "    feature_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\"_features.parquet\")]\n",
    "    combined_features = pd.concat([pd.read_parquet(f) for f in tqdm(feature_files, desc=\"Combining features\")], ignore_index=True)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f814395",
   "metadata": {
    "papermill": {
     "duration": 0.006541,
     "end_time": "2024-12-19T17:00:34.367135",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.360594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43051a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.381786Z",
     "iopub.status.busy": "2024-12-19T17:00:34.381490Z",
     "iopub.status.idle": "2024-12-19T17:00:34.397365Z",
     "shell.execute_reply": "2024-12-19T17:00:34.396572Z"
    },
    "papermill": {
     "duration": 0.025328,
     "end_time": "2024-12-19T17:00:34.399179",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.373851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sparse Autoencoder\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, sparsity_weight=1e-5):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()  # Outputs in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_data(data, scaler_type=\"MinMaxScaler\"):\n",
    "    \"\"\"\n",
    "    Prepares data for model training, ensuring only numeric columns are scaled.\n",
    "    Returns PyTorch tensor.\n",
    "    \"\"\"\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    if scaler_type == \"RobustScaler\":\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numeric columns\n",
    "    data_scaled = scaler.fit_transform(data[numeric_cols])\n",
    "    return torch.tensor(data_scaled, dtype=torch.float32), scaler\n",
    "\n",
    "\n",
    "# PCA function\n",
    "def apply_pca(data, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    return data_pca, pca\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "# Training function\n",
    "def perform_sparse_autoencoder(data, epochs=100, batch_size=32, learning_rate=0.001, patience=10, scaler_type=\"MinMaxScaler\", use_pca=False, sparsity_weight=1e-5):\n",
    "    # Preprocess data\n",
    "    if use_pca:\n",
    "        data, pca = apply_pca(data)\n",
    "\n",
    "    data_tensor, scaler = prepare_data(data, scaler_type=scaler_type)\n",
    "    train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Ensure data is PyTorch tensors\n",
    "    assert isinstance(train_data, torch.Tensor), \"train_data must be a PyTorch tensor\"\n",
    "    assert isinstance(val_data, torch.Tensor), \"val_data must be a PyTorch tensor\"\n",
    "\n",
    "    # DataLoader setup\n",
    "    train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = SparseAutoencoder(input_dim=data_tensor.shape[1], sparsity_weight=sparsity_weight)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, outputs = model(batch)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Sparsity penalty\n",
    "            l1_penalty = torch.mean(torch.abs(encoded))\n",
    "            loss += sparsity_weight * l1_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[0].to(device)\n",
    "                _, outputs = model(batch)\n",
    "                loss = criterion(outputs, batch)\n",
    "                val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        stopper(val_loss)\n",
    "        if stopper.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Return encoded features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = model(data_tensor.to(device))\n",
    "    encoded_features = encoded_features.cpu().numpy()\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=[f\"feature_{i}\" for i in range(encoded_features.shape[1])])\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09b0d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.414481Z",
     "iopub.status.busy": "2024-12-19T17:00:34.414163Z",
     "iopub.status.idle": "2024-12-19T17:00:34.423461Z",
     "shell.execute_reply": "2024-12-19T17:00:34.422721Z"
    },
    "papermill": {
     "duration": 0.018934,
     "end_time": "2024-12-19T17:00:34.425134",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.406200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    # Keep only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    # Scale the numeric data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_numeric)\n",
    "    \n",
    "    # Convert to a PyTorch tensor\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    # Define the autoencoder model\n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    # Set up the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    # Extract encoded data\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    # Return the encoded data as a DataFrame\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2ab6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.440063Z",
     "iopub.status.busy": "2024-12-19T17:00:34.439827Z",
     "iopub.status.idle": "2024-12-19T17:00:34.445993Z",
     "shell.execute_reply": "2024-12-19T17:00:34.445102Z"
    },
    "papermill": {
     "duration": 0.015592,
     "end_time": "2024-12-19T17:00:34.447854",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.432262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_time_series_data(train_ts_path, test_ts_path, use_autoencoder=False, use_imputer=False, impute_method=\"mean\", outdir='/kaggle/working/intermediate_results'):\n",
    "    \"\"\"\n",
    "    Preprocess time-series data, including feature engineering and optional autoencoder-based encoding.\n",
    "    Args:\n",
    "        train_ts_path (str): Path to training time-series data.\n",
    "        test_ts_path (str): Path to test time-series data.\n",
    "        use_autoencoder (bool): Whether to encode features with a sparse autoencoder.\n",
    "        use_imputer (bool): Whether to impute missing values.\n",
    "        impute_method (str): Imputation method (\"mean\", \"knn\", etc.).\n",
    "    Returns:\n",
    "        tuple: Preprocessed training and test time-series DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_ts = load_time_series(train_ts_path, '/kaggle/working/final')\n",
    "    test_ts = load_time_series(test_ts_path, '/kaggle/working/final_test')\n",
    "\n",
    "\n",
    "    # Impute missing values\n",
    "    if use_imputer:\n",
    "        train_ts = impute(train_ts, method=impute_method)\n",
    "        test_ts = impute(test_ts, method=impute_method)\n",
    "\n",
    "    # Encode features with an autoencoder\n",
    "    if use_autoencoder:\n",
    "        train_ts_encoded = perform_autoencoder(\n",
    "            train_ts, encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        test_ts_encoded = perform_autoencoder(\n",
    "            test_ts, encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        train_ts_encoded['id'] = train_ts[\"id\"]\n",
    "        test_ts_encoded['id'] = test_ts[\"id\"]\n",
    "\n",
    "        return train_ts_encoded, test_ts_encoded\n",
    "\n",
    "    return train_ts, test_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d088397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.462960Z",
     "iopub.status.busy": "2024-12-19T17:00:34.462300Z",
     "iopub.status.idle": "2024-12-19T17:00:34.470430Z",
     "shell.execute_reply": "2024-12-19T17:00:34.469582Z"
    },
    "papermill": {
     "duration": 0.017391,
     "end_time": "2024-12-19T17:00:34.472148",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.454757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=False, use_numeric_imputation=False, numeric_impute_method=\"knn\"):\n",
    "    \"\"\"\n",
    "    Merge CSV and time-series data into a unified dataset and fill missing values using KNNImputer.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data from CSV.\n",
    "        test (pd.DataFrame): Test data from CSV.\n",
    "        train_ts (pd.DataFrame): Aggregated training time-series data.\n",
    "        test_ts (pd.DataFrame): Aggregated test time-series data.\n",
    "        use_time_series (bool): Whether to include time-series data in the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Merged training and test DataFrames.\n",
    "    \"\"\"\n",
    "    featuresCols = train.columns.to_list()\n",
    "    if use_time_series:\n",
    "        # Merge time-series data with train and test data on 'id'\n",
    "        train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "        test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "    # Drop 'id' column after merging\n",
    "    train = train.drop('id', axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    if use_time_series:\n",
    "        # Feature selection\n",
    "        time_series_cols = train_ts.columns.tolist()\n",
    "        time_series_cols.remove(\"id\")\n",
    "\n",
    "    if np.any(np.isinf(train)):\n",
    "        train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if np.any(np.isinf(test)):\n",
    "        test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if use_numeric_imputation:\n",
    "        train = impute(train, method=numeric_impute_method)\n",
    "        test = impute(test, method=numeric_impute_method)\n",
    "\n",
    "    if use_time_series:\n",
    "        featuresCols += time_series_cols\n",
    "\n",
    "    # Dynamically filter features based on available columns\n",
    "    featuresCols = [col for col in featuresCols if col in train.columns]\n",
    "    print(\"Final features included in train:\", featuresCols)\n",
    "\n",
    "    # Filter features and drop rows with missing target 'sii'\n",
    "    train = train[featuresCols]\n",
    "    train = train.dropna(subset=['sii'])\n",
    "\n",
    "    featuresCols.remove('sii')\n",
    "    test = test[featuresCols]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cc0a3",
   "metadata": {
    "papermill": {
     "duration": 0.006604,
     "end_time": "2024-12-19T17:00:34.485441",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.478837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"hyperparameter-tuning\"></a>\n",
    "# **Hyperparameter Tuning with Optuna**\n",
    "This section includes functions to optimize hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752459e",
   "metadata": {
    "papermill": {
     "duration": 0.006503,
     "end_time": "2024-12-19T17:00:34.498655",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.492152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"lightgbm\"></a>\n",
    "### **LightGBM Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f888a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.513143Z",
     "iopub.status.busy": "2024-12-19T17:00:34.512615Z",
     "iopub.status.idle": "2024-12-19T17:00:34.519600Z",
     "shell.execute_reply": "2024-12-19T17:00:34.518746Z"
    },
    "papermill": {
     "duration": 0.016023,
     "end_time": "2024-12-19T17:00:34.521288",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.505265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_lightgbm(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n",
    "            \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "            \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "            \"random_state\": 42,\n",
    "            \"n_estimators\": 200\n",
    "        }\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\"\n",
    "            # early_stopping_rounds=50\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for LightGBM:\", study.best_value)\n",
    "    print(\"Best Params for LightGBM:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7a8cd",
   "metadata": {
    "papermill": {
     "duration": 0.006494,
     "end_time": "2024-12-19T17:00:34.535624",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.529130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "### **XGBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e5d5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.550275Z",
     "iopub.status.busy": "2024-12-19T17:00:34.549766Z",
     "iopub.status.idle": "2024-12-19T17:00:34.557803Z",
     "shell.execute_reply": "2024-12-19T17:00:34.556959Z"
    },
    "papermill": {
     "duration": 0.017215,
     "end_time": "2024-12-19T17:00:34.559481",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.542266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_xgboost(train, target, n_trials=50):\n",
    "    import optuna\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    def is_gpu_available():\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    use_gpu = is_gpu_available()\n",
    "    tree_method = \"gpu_hist\" if use_gpu else \"hist\"\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-5, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-5, 10.0),\n",
    "            \"tree_method\": tree_method,\n",
    "        }\n",
    "\n",
    "        # Preprocess the training data to remove non-numeric columns\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\",\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "\n",
    "        # Predict and evaluate\n",
    "        preds = model.predict(X_valid)\n",
    "        rounded_preds = np.round(preds).astype(int)  # Ensure preds are rounded here\n",
    "        score = quadratic_weighted_kappa(y_valid, rounded_preds)\n",
    "        return score\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for XGBoost:\", study.best_value)\n",
    "    print(\"Best Params for XGBoost:\", study.best_params)\n",
    "\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83749144",
   "metadata": {
    "papermill": {
     "duration": 0.0064,
     "end_time": "2024-12-19T17:00:34.572454",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.566054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"catboost\"></a>\n",
    "### **CatBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ded23c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.586847Z",
     "iopub.status.busy": "2024-12-19T17:00:34.586534Z",
     "iopub.status.idle": "2024-12-19T17:00:34.593418Z",
     "shell.execute_reply": "2024-12-19T17:00:34.592587Z"
    },
    "papermill": {
     "duration": 0.016081,
     "end_time": "2024-12-19T17:00:34.595072",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.578991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_catboost(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n",
    "            \"iterations\": 200,\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for CatBoost:\", study.best_value)\n",
    "    print(\"Best Params for CatBoost:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef20e8",
   "metadata": {
    "papermill": {
     "duration": 0.006286,
     "end_time": "2024-12-19T17:00:34.607913",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.601627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Prepare Model Optimized Hyperparameter**\n",
    "Create a dictionary of models' best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dea79ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.622472Z",
     "iopub.status.busy": "2024-12-19T17:00:34.621923Z",
     "iopub.status.idle": "2024-12-19T17:00:34.626589Z",
     "shell.execute_reply": "2024-12-19T17:00:34.625898Z"
    },
    "papermill": {
     "duration": 0.01352,
     "end_time": "2024-12-19T17:00:34.628214",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.614694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def models_best_params(use_lightgbm=False, use_xgboost=False, use_catboost= False):\n",
    "    best_params_dict = {}\n",
    "    if use_lightgbm:\n",
    "        best_params_lgbm = optimize_lightgbm(train, target, n_trials=50)\n",
    "        best_params_dict['LightGBM'] = best_params_lgbm\n",
    "    if use_xgboost:\n",
    "        best_params_xgb = optimize_xgboost(train, target, n_trials=50)\n",
    "        best_params_dict['XGBoost'] = best_params_xgb\n",
    "    if use_catboost:\n",
    "        best_params_catboost = optimize_catboost(train, target, n_trials=50)\n",
    "        best_params_dict['CatBoost'] = best_params_catboost\n",
    "    return best_params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2638790",
   "metadata": {
    "papermill": {
     "duration": 0.006458,
     "end_time": "2024-12-19T17:00:34.676438",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.669980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"model-training\"></a>\n",
    "# **Model Training**\n",
    "Train multiple models with the optimized hyperparameters.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04a509b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.690746Z",
     "iopub.status.busy": "2024-12-19T17:00:34.690460Z",
     "iopub.status.idle": "2024-12-19T17:00:34.702616Z",
     "shell.execute_reply": "2024-12-19T17:00:34.701805Z"
    },
    "papermill": {
     "duration": 0.021325,
     "end_time": "2024-12-19T17:00:34.704343",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.683018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method=None, weights=None, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train models using cross-validation, optionally using Voting or Stacking.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data.\n",
    "        test (pd.DataFrame): Test data.\n",
    "        target (pd.Series): Target variable.\n",
    "        best_params_dict (dict): Optimized hyperparameters for each model.\n",
    "        ensemble_method (str): 'voting', 'stacking', or None for individual models.\n",
    "        n_splits (int): Number of CV splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: Aggregated predictions for each model or ensemble.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    # target_binned = np.digitize(target, bins=np.linspace(target.min(), target.max(), 5))\n",
    "    trained_models = {}\n",
    "\n",
    "    models = []\n",
    "    for model_name, params in best_params_dict.items():\n",
    "        if model_name == \"LightGBM\":\n",
    "            model = LGBMRegressor(**params, verbose=-1)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**params)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model = CatBoostRegressor(**params)\n",
    "        elif model_name == \"FTTransformer\":\n",
    "            model = FTTransformerWrapper(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        models.append((model_name, model))\n",
    "\n",
    "    if ensemble_method == 'voting':\n",
    "        if weights:\n",
    "            ensemble_model = VotingRegressor(estimators=models, weights=weights)\n",
    "        # Create a Voting Regressor\n",
    "        else:\n",
    "            ensemble_model = VotingRegressor(estimators=models)\n",
    "    elif ensemble_method == 'stacking':\n",
    "        # Create a Stacking Regressor\n",
    "        ensemble_model = StackingRegressor(estimators=models, final_estimator=LinearRegression())\n",
    "\n",
    "    for model_name, model in ([('Ensemble', ensemble_model)] if ensemble_method else models):\n",
    "        print(f\"Training model with CV: {model_name}\")\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        # fold_predictions = np.zeros(test.shape[0])\n",
    "\n",
    "        train_S = []\n",
    "        test_S = []\n",
    "        \n",
    "        oof_non_rounded = np.zeros(len(target), dtype=float) \n",
    "        oof_rounded = np.zeros(len(target), dtype=int) \n",
    "        test_preds = np.zeros((len(test), n_splits))\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(train, target)):\n",
    "            print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "            X_train, X_valid = train.iloc[train_idx], train.iloc[test_idx]\n",
    "            y_train, y_valid = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_valid)\n",
    "\n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "    \n",
    "            train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "            val_kappa = quadratic_weighted_kappa(y_valid, y_val_pred_rounded)\n",
    "    \n",
    "            train_S.append(train_kappa)\n",
    "            test_S.append(val_kappa)\n",
    "            \n",
    "            test_preds[:, fold] = model.predict(test)\n",
    "            \n",
    "            print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "            # clear_output(wait=True)\n",
    "\n",
    "        print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "        print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(target, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "\n",
    "        print(\"KappaOPtimizer.x =\",  KappaOPtimizer.x)\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(target, oof_tuned)\n",
    "    \n",
    "        print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n",
    "    \n",
    "        tpm = test_preds.mean(axis=1)\n",
    "        tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "        predictions[model_name] = tpTuned\n",
    "        trained_models[model_name] = model\n",
    "\n",
    "    return predictions, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5a8ed",
   "metadata": {
    "papermill": {
     "duration": 0.006349,
     "end_time": "2024-12-19T17:00:34.717643",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.711294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"submission\"></a>\n",
    "# **Submission**\n",
    "Generate and save the final submission file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d35223be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.731759Z",
     "iopub.status.busy": "2024-12-19T17:00:34.731489Z",
     "iopub.status.idle": "2024-12-19T17:00:34.736023Z",
     "shell.execute_reply": "2024-12-19T17:00:34.734963Z"
    },
    "papermill": {
     "duration": 0.013473,
     "end_time": "2024-12-19T17:00:34.737604",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.724131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_submission_file(predictions, sample):\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"sii\": np.round(predictions).astype(int)\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved!\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2191256",
   "metadata": {
    "papermill": {
     "duration": 0.006401,
     "end_time": "2024-12-19T17:00:34.750835",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.744434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"main-pipeline\"></a>\n",
    "# **Main Pipeline**\n",
    "Run the entire pipeline.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98b3b76e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:00:34.765098Z",
     "iopub.status.busy": "2024-12-19T17:00:34.764867Z",
     "iopub.status.idle": "2024-12-19T17:05:59.034419Z",
     "shell.execute_reply": "2024-12-19T17:05:59.033575Z"
    },
    "papermill": {
     "duration": 324.279035,
     "end_time": "2024-12-19T17:05:59.036357",
     "exception": false,
     "start_time": "2024-12-19T17:00:34.757322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after feature engineering:  (3960, 99)\n",
      "Test shape after feature engineering:  (20, 77)\n",
      "Train shape after removing columns:  (3960, 65)\n",
      "Test shape after removing columns:  (20, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing participants: 100%|██████████| 996/996 [04:58<00:00,  3.33it/s]\n",
      "Combining features: 100%|██████████| 996/996 [00:03<00:00, 282.23it/s]\n",
      "Processing participants: 100%|██████████| 2/2 [00:00<00:00,  4.01it/s]\n",
      "Combining features: 100%|██████████| 2/2 [00:00<00:00, 238.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.4159]\n",
      "Epoch [20/100], Loss: 0.3702]\n",
      "Epoch [30/100], Loss: 0.3523]\n",
      "Epoch [40/100], Loss: 0.3414]\n",
      "Epoch [50/100], Loss: 0.3223]\n",
      "Epoch [60/100], Loss: 0.3199]\n",
      "Epoch [70/100], Loss: 0.3194]\n",
      "Epoch [80/100], Loss: 0.3187]\n",
      "Epoch [90/100], Loss: 0.3156]\n",
      "Epoch [100/100], Loss: 0.3182]\n",
      "Epoch [10/100], Loss: 1.1226]\n",
      "Epoch [20/100], Loss: 0.8168]\n",
      "Epoch [30/100], Loss: 0.5205]\n",
      "Epoch [40/100], Loss: 0.4617]\n",
      "Epoch [50/100], Loss: 0.4615]\n",
      "Epoch [60/100], Loss: 0.4615]\n",
      "Epoch [70/100], Loss: 0.4615]\n",
      "Epoch [80/100], Loss: 0.4615]\n",
      "Epoch [90/100], Loss: 0.4615]\n",
      "Epoch [100/100], Loss: 0.4615]\n",
      "Final features included in train: ['sii', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-Height', 'Physical-Weight', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age', 'BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight', 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight', 'Sex_BMI', 'Sex_HeartRate', 'Age_WaistCirc', 'BMI_FitnessMaxStage', 'Weight_GripStrengthDominant', 'Weight_GripStrengthNonDominant', 'HeartRate_FitnessTime', 'Age_PushUp', 'FFMI_Age', 'InternetUse_SleepDisturbance', 'CGAS_BMI', 'CGAS_FitnessMaxStage', 'Enc_1', 'Enc_2', 'Enc_3', 'Enc_4', 'Enc_5', 'Enc_6', 'Enc_7', 'Enc_8', 'Enc_9', 'Enc_10', 'Enc_11', 'Enc_12', 'Enc_13', 'Enc_14', 'Enc_15', 'Enc_16', 'Enc_17', 'Enc_18', 'Enc_19', 'Enc_20', 'Enc_21', 'Enc_22', 'Enc_23', 'Enc_24', 'Enc_25', 'Enc_26', 'Enc_27', 'Enc_28', 'Enc_29', 'Enc_30', 'Enc_31', 'Enc_32', 'Enc_33', 'Enc_34', 'Enc_35', 'Enc_36', 'Enc_37', 'Enc_38', 'Enc_39', 'Enc_40', 'Enc_41', 'Enc_42', 'Enc_43', 'Enc_44', 'Enc_45', 'Enc_46', 'Enc_47', 'Enc_48', 'Enc_49', 'Enc_50', 'Enc_51', 'Enc_52', 'Enc_53', 'Enc_54', 'Enc_55', 'Enc_56', 'Enc_57', 'Enc_58', 'Enc_59', 'Enc_60']\n",
      "Deleted: /kaggle/working/final_test\n",
      "Deleted: /kaggle/working/__notebook__.ipynb\n",
      "Deleted: /kaggle/working/final\n"
     ]
    }
   ],
   "source": [
    "# Set up logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Limit verbosity\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "n_folds = 5\n",
    "\n",
    "# Paths\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "sample_path = '/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv'\n",
    "time_series_train = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n",
    "time_series_test = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n",
    "\n",
    "# Toggle for using time-series data\n",
    "use_time_series = True  # Set to False to skip time-series data\n",
    "\n",
    "# Preprocess data\n",
    "train, test, sample = preprocess_csv_data(train_path, test_path, sample_path)\n",
    "\n",
    "if use_time_series:\n",
    "    train_ts, test_ts = preprocess_time_series_data(time_series_train, time_series_test, use_autoencoder=True, use_imputer=True, impute_method=\"mean\")\n",
    "else:\n",
    "    train_ts, test_ts = None, None\n",
    "train, test = merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=use_time_series, use_numeric_imputation=True, numeric_impute_method=\"knn\")\n",
    "\n",
    "train = train.dropna(subset=['sii'])\n",
    "\n",
    "# Target and features\n",
    "target = train[\"sii\"]\n",
    "train = train.drop(columns=[\"sii\"])  # Drop `sii` from features\n",
    "\n",
    "\n",
    "# Ensure `sii` is not in test data\n",
    "if \"sii\" in test.columns:\n",
    "    test = test.drop(columns=[\"sii\"])\n",
    "\n",
    "if \"id\" in train.columns:\n",
    "    train = train.drop(columns=[\"id\"])\n",
    "    test = test.drop(columns=[\"id\"])\n",
    "\n",
    "working_dir = '/kaggle/working'\n",
    "clean_kaggle_working_directory(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e8576",
   "metadata": {
    "papermill": {
     "duration": 0.049504,
     "end_time": "2024-12-19T17:05:59.289862",
     "exception": false,
     "start_time": "2024-12-19T17:05:59.240358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Check features for FTTransformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea9dedc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:05:59.389322Z",
     "iopub.status.busy": "2024-12-19T17:05:59.388998Z",
     "iopub.status.idle": "2024-12-19T17:05:59.394892Z",
     "shell.execute_reply": "2024-12-19T17:05:59.394046Z"
    },
    "papermill": {
     "duration": 0.057829,
     "end_time": "2024-12-19T17:05:59.396681",
     "exception": false,
     "start_time": "2024-12-19T17:05:59.338852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'gpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3f6da19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:05:59.498131Z",
     "iopub.status.busy": "2024-12-19T17:05:59.497816Z",
     "iopub.status.idle": "2024-12-19T17:05:59.502204Z",
     "shell.execute_reply": "2024-12-19T17:05:59.501291Z"
    },
    "papermill": {
     "duration": 0.056668,
     "end_time": "2024-12-19T17:05:59.503888",
     "exception": false,
     "start_time": "2024-12-19T17:05:59.447220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_dict = {'LightGBM': Params, 'XGBoost': XGB_Params, 'CatBoost': CatBoost_Params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1af5bbcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:05:59.605819Z",
     "iopub.status.busy": "2024-12-19T17:05:59.605305Z",
     "iopub.status.idle": "2024-12-19T17:06:33.996815Z",
     "shell.execute_reply": "2024-12-19T17:06:33.995898Z"
    },
    "papermill": {
     "duration": 34.446443,
     "end_time": "2024-12-19T17:06:33.999254",
     "exception": false,
     "start_time": "2024-12-19T17:05:59.552811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with CV: Ensemble\n",
      "Training fold 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.7778, Validation QWK: 0.3613\n",
      "Training fold 2/5...\n",
      "Fold 2 - Train QWK: 0.7801, Validation QWK: 0.3951\n",
      "Training fold 3/5...\n",
      "Fold 3 - Train QWK: 0.7880, Validation QWK: 0.3882\n",
      "Training fold 4/5...\n",
      "Fold 4 - Train QWK: 0.7854, Validation QWK: 0.3437\n",
      "Training fold 5/5...\n",
      "Fold 5 - Train QWK: 0.7795, Validation QWK: 0.3202\n",
      "Mean Train QWK --> 0.7822\n",
      "Mean Validation QWK ---> 0.3617\n",
      "KappaOPtimizer.x = [0.57500361 0.94511966 2.78413208]\n",
      "----> || Optimized QWK SCORE :: 0.456\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Train model and make predictions using cross-validation\n",
    "predictions, trained_models = train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method='voting')\n",
    "\n",
    "# Generate submission\n",
    "final_predictions = next(iter(predictions.values()))\n",
    "submission = generate_submission_file(final_predictions, sample)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 385.896706,
   "end_time": "2024-12-19T17:06:37.283004",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-19T17:00:11.386298",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
