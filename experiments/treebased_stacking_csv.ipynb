{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3969fb0",
   "metadata": {
    "papermill": {
     "duration": 0.014324,
     "end_time": "2024-12-18T08:45:20.635016",
     "exception": false,
     "start_time": "2024-12-18T08:45:20.620692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook demonstrates a complete machine learning pipeline, including:\n",
    "- Preprocessing CSV and time-series data.\n",
    "- Using Optuna to tune hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "- Training multiple models with optimized parameters.\n",
    "- Combining predictions using an ensemble method.\n",
    "- Generating a Kaggle submission.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a9cfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T13:21:14.156098Z",
     "iopub.status.busy": "2024-12-17T13:21:14.155355Z",
     "iopub.status.idle": "2024-12-17T13:21:14.164244Z",
     "shell.execute_reply": "2024-12-17T13:21:14.162688Z",
     "shell.execute_reply.started": "2024-12-17T13:21:14.156064Z"
    },
    "papermill": {
     "duration": 0.011803,
     "end_time": "2024-12-18T08:45:20.659373",
     "exception": false,
     "start_time": "2024-12-18T08:45:20.647570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Table of Contents**\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries and Utilities](#libraries-and-utilities)\n",
    "3. [Preprocessing](#preprocessing)\n",
    "    - [Preprocessing CSV Data](#preprocessing-csv-data)\n",
    "    - [Preprocessing Time-Series Data](#preprocessing-time-series-data)\n",
    "    - [Merging Preprocessed Data](#merging-preprocessed-data)\n",
    "4. [Hyperparameter Tuning with Optuna](#hyperparameter-tuning)\n",
    "    - [LightGBM](#lightgbm)\n",
    "    - [XGBoost](#xgboost)\n",
    "    - [CatBoost](#catboost)\n",
    "5. [Model Training](#model-training)\n",
    "6. [Ensemble Predictions](#ensemble-predictions)\n",
    "7. [Submission](#submission)\n",
    "8. [Main Pipeline](#main-pipeline)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822dd26",
   "metadata": {
    "papermill": {
     "duration": 0.011845,
     "end_time": "2024-12-18T08:45:20.684435",
     "exception": false,
     "start_time": "2024-12-18T08:45:20.672590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"libraries-and-utilities\"></a>\n",
    "# **Libraries and Utilities**\n",
    "Import the required libraries, including Optuna for hyperparameter optimization.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa622d7",
   "metadata": {
    "papermill": {
     "duration": 0.012033,
     "end_time": "2024-12-18T08:45:20.708594",
     "exception": false,
     "start_time": "2024-12-18T08:45:20.696561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Install packages and import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17125b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:45:20.796982Z",
     "iopub.status.busy": "2024-12-18T08:45:20.796457Z",
     "iopub.status.idle": "2024-12-18T08:47:33.913906Z",
     "shell.execute_reply": "2024-12-18T08:47:33.912612Z"
    },
    "papermill": {
     "duration": 133.132712,
     "end_time": "2024-12-18T08:47:33.916407",
     "exception": false,
     "start_time": "2024-12-18T08:45:20.783695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.20.3\r\n",
      "Uninstalling protobuf-3.20.3:\r\n",
      "  Successfully uninstalled protobuf-3.20.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: protobuf\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-3.19.6\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/tabtransformertf-0.0.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.0.2)\r\n",
      "Requirement already satisfied: tensorflow>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.3.5)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.21.6)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (4.64.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->tabtransformertf==0.0.8) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->tabtransformertf==0.0.8) (2022.7.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (1.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.51.1)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.19.6)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.6.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (59.8.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (15.0.6.1)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.4.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.14.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.4.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.2.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.2.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.8.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.29.0)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (23.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.3.0)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (23.1.21)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.16.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.38.4)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.28.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.6.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.8.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.6)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.35.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.4.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.2.3)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.9)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.2.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.2.4)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.11.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2022.12.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.2.2)\r\n",
      "Installing collected packages: tabtransformertf\r\n",
      "Successfully installed tabtransformertf-0.0.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.19.0) (23.0)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.19.0) (2.13.3)\r\n",
      "tensorflow-addons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf -y\n",
    "!pip install /kaggle/input/ft-transformer/protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install /kaggle/input/ft-transformer/tabtransformertf-0.0.8-py3-none-any.whl\n",
    "!pip install /kaggle/input/ft-transformer/tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560a188a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:33.945098Z",
     "iopub.status.busy": "2024-12-18T08:47:33.944760Z",
     "iopub.status.idle": "2024-12-18T08:47:41.288401Z",
     "shell.execute_reply": "2024-12-18T08:47:41.287557Z"
    },
    "papermill": {
     "duration": 7.360544,
     "end_time": "2024-12-18T08:47:41.290758",
     "exception": false,
     "start_time": "2024-12-18T08:47:33.930214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c4df7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:41.319825Z",
     "iopub.status.busy": "2024-12-18T08:47:41.318856Z",
     "iopub.status.idle": "2024-12-18T08:47:45.316631Z",
     "shell.execute_reply": "2024-12-18T08:47:45.315835Z"
    },
    "papermill": {
     "duration": 4.014194,
     "end_time": "2024-12-18T08:47:45.318806",
     "exception": false,
     "start_time": "2024-12-18T08:47:41.304612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # Linear algebra\n",
    "import pandas as pd  # Data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a92df",
   "metadata": {
    "papermill": {
     "duration": 0.013319,
     "end_time": "2024-12-18T08:47:45.346031",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.332712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc1aba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:45.374451Z",
     "iopub.status.busy": "2024-12-18T08:47:45.374150Z",
     "iopub.status.idle": "2024-12-18T08:47:45.380588Z",
     "shell.execute_reply": "2024-12-18T08:47:45.379642Z"
    },
    "papermill": {
     "duration": 0.022722,
     "end_time": "2024-12-18T08:47:45.382404",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.359682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa23eee",
   "metadata": {
    "papermill": {
     "duration": 0.013205,
     "end_time": "2024-12-18T08:47:45.408821",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.395616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "# **Preprocessing**\n",
    "This section contains the preprocessing functions for CSV and time-series data, as well as merging them.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0768af",
   "metadata": {
    "papermill": {
     "duration": 0.013346,
     "end_time": "2024-12-18T08:47:45.435724",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.422378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Imputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a76bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:45.463530Z",
     "iopub.status.busy": "2024-12-18T08:47:45.463061Z",
     "iopub.status.idle": "2024-12-18T08:47:45.479200Z",
     "shell.execute_reply": "2024-12-18T08:47:45.478560Z"
    },
    "papermill": {
     "duration": 0.031989,
     "end_time": "2024-12-18T08:47:45.481053",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.449064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute(df, method=\"knn\", n_neighbors=5):\n",
    "    # impute categorical columns\n",
    "    if method == \"cat\":\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "        return df\n",
    "\n",
    "    # get numeric columns instead of 'sii'\n",
    "    numeric_cols = df.select_dtypes(include=['number', 'float64', 'int64']).columns\n",
    "    numeric_cols = list(numeric_cols)\n",
    "    if \"sii\" in numeric_cols:\n",
    "        numeric_cols.remove(\"sii\")\n",
    "    numeric_cols = pd.Index(numeric_cols)\n",
    "    \n",
    "    if method == \"knn\":\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        # impute_data = imputer.fit_transform(df[numeric_cols])\n",
    "    if method == \"mean\":\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "    if method == \"median\":\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "    \n",
    "    imputed_data = imputer.fit_transform(df[numeric_cols])\n",
    "    df_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "    for col in df.columns:\n",
    "        if col not in numeric_cols:\n",
    "            df_imputed[col] = df[col]          \n",
    "    df = df_imputed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d730b1",
   "metadata": {
    "papermill": {
     "duration": 0.012955,
     "end_time": "2024-12-18T08:47:45.507330",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.494375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess CSV data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bf9fa",
   "metadata": {
    "papermill": {
     "duration": 0.012871,
     "end_time": "2024-12-18T08:47:45.533410",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.520539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Feature engineering for CSV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b261eed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:45.561173Z",
     "iopub.status.busy": "2024-12-18T08:47:45.560570Z",
     "iopub.status.idle": "2024-12-18T08:47:45.564627Z",
     "shell.execute_reply": "2024-12-18T08:47:45.563855Z"
    },
    "papermill": {
     "duration": 0.01973,
     "end_time": "2024-12-18T08:47:45.566269",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.546539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "sample_path = '/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv'\n",
    "time_series_train = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n",
    "time_series_test = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd74796d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:45.594153Z",
     "iopub.status.busy": "2024-12-18T08:47:45.593900Z",
     "iopub.status.idle": "2024-12-18T08:47:45.604492Z",
     "shell.execute_reply": "2024-12-18T08:47:45.603804Z"
    },
    "papermill": {
     "duration": 0.026976,
     "end_time": "2024-12-18T08:47:45.606304",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.579328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "\n",
    "    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n",
    "    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n",
    "    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n",
    "    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n",
    "    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n",
    "    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n",
    "    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n",
    "    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n",
    "    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n",
    "    df['FFMI_Age'] = df['BIA-BIA_FFMI'] * df['Basic_Demos-Age']\n",
    "    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n",
    "    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n",
    "    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98769f",
   "metadata": {
    "papermill": {
     "duration": 0.013081,
     "end_time": "2024-12-18T08:47:45.896370",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.883289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After we analyzed the dataset, we observed that there are lot of unreasonable values in the dataset. Therefore, we decided to prune some anomaly samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09784495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:45.924330Z",
     "iopub.status.busy": "2024-12-18T08:47:45.923739Z",
     "iopub.status.idle": "2024-12-18T08:47:45.932614Z",
     "shell.execute_reply": "2024-12-18T08:47:45.931825Z"
    },
    "papermill": {
     "duration": 0.024882,
     "end_time": "2024-12-18T08:47:45.934488",
     "exception": false,
     "start_time": "2024-12-18T08:47:45.909606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    input_length = len(df)\n",
    "    df = df.drop(df[df['Physical-BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Systolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] > 160].index)\n",
    "\n",
    "    children = df[df['Basic_Demos-Age'] <= 12]\n",
    "    df = df.drop(children[children['FGC-FGC_CU'] > 80].index)\n",
    "    df = df.drop(children[children['FGC-FGC_GSND'] > 80].index)\n",
    "\n",
    "    df = df.drop(df[df['BIA-BIA_BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMC'] > 1000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMR'] > 40000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_DEE'] > 60000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ECW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_FFM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ICW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LDM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LST'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_SMM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_TBW'] > 2000].index)\n",
    "    output_length = len(df)\n",
    "    print (input_length, output_length)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726e478",
   "metadata": {
    "papermill": {
     "duration": 0.013271,
     "end_time": "2024-12-18T08:47:46.097587",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.084316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Preprocess Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd10562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.125974Z",
     "iopub.status.busy": "2024-12-18T08:47:46.125105Z",
     "iopub.status.idle": "2024-12-18T08:47:46.132403Z",
     "shell.execute_reply": "2024-12-18T08:47:46.131440Z"
    },
    "papermill": {
     "duration": 0.023433,
     "end_time": "2024-12-18T08:47:46.134257",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.110824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_csv_data(train_path, test_path, sample_path):\n",
    "    \"\"\"\n",
    "    Preprocess CSV data with proper handling of missing columns.\n",
    "    Args:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        test_path (str): Path to the test CSV file.\n",
    "        sample_path (str): Path to the sample submission CSV file\n",
    "    Returns:\n",
    "        tuple: Preprocessed training DataFrame, test DataFrame, and sample submission.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    sample = pd.read_csv(sample_path)\n",
    "\n",
    "    # # remove outlier from train\n",
    "    # train = remove_outliers(train)\n",
    "    # print(\"Train shape after removing outlier: \", train.shape)\n",
    "\n",
    "    # feature engineering for both train and test\n",
    "    train = csv_feature_engineering(train)\n",
    "    test = csv_feature_engineering(test)\n",
    "    print(\"Train shape after feature engineering: \", train.shape)\n",
    "    print(\"Test shape after feature engineering: \", test.shape)\n",
    "\n",
    "    # Only use columns in both train and test\n",
    "    # Ensure that the columns in `train` and `test` match\n",
    "    # Remove some columns\n",
    "    common_columns = test.columns.to_list() \n",
    "    remove_columns = ['BIA-BIA_LDM', 'Physical-Waist_Circumference', 'FGC-FGC_SRL', 'FGC-FGC_GSND', 'BIA-BIA_ECW', 'Physical-BMI', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_FFM', 'BIA-BIA_TBW', 'BIA-BIA_BMR', 'BIA-BIA_ICW', 'BIA-BIA_DEE']\n",
    "    common_columns = [col for col in common_columns if col not in remove_columns]\n",
    "    \n",
    "    train = train[[\"sii\"] + common_columns]\n",
    "    test = test[common_columns]\n",
    "    print(\"Train shape after removing columns: \", train.shape)\n",
    "    print(\"Test shape after removing columns: \", test.shape)\n",
    "\n",
    "    return train, test, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a342660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.162654Z",
     "iopub.status.busy": "2024-12-18T08:47:46.162014Z",
     "iopub.status.idle": "2024-12-18T08:47:46.244195Z",
     "shell.execute_reply": "2024-12-18T08:47:46.243196Z"
    },
    "papermill": {
     "duration": 0.098315,
     "end_time": "2024-12-18T08:47:46.246054",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.147739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after feature engineering:  (3960, 99)\n",
      "Test shape after feature engineering:  (20, 77)\n",
      "Train shape after removing columns:  (3960, 65)\n",
      "Test shape after removing columns:  (20, 64)\n"
     ]
    }
   ],
   "source": [
    "train, test, sample = preprocess_csv_data(train_path, test_path, sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7fa1a",
   "metadata": {
    "papermill": {
     "duration": 0.013676,
     "end_time": "2024-12-18T08:47:46.273589",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.259913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0fc69d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.339400Z",
     "iopub.status.busy": "2024-12-18T08:47:46.338701Z",
     "iopub.status.idle": "2024-12-18T08:47:46.345871Z",
     "shell.execute_reply": "2024-12-18T08:47:46.345086Z"
    },
    "papermill": {
     "duration": 0.025519,
     "end_time": "2024-12-18T08:47:46.347670",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.322151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New version, nhưng chạy tận 10p??????\n",
    "def process_file(filename, dirname):\n",
    "    df= pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    data=extract_advanced_features(df)\n",
    "    array_1=data.values[0]\n",
    "    array_2=df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "    # Combine the two arrays\n",
    "    combined_array = np.concatenate((array_1, array_2[0]))\n",
    "    combined_tuple=(array_1,array_2[1])\n",
    "    return combined_tuple\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79149fc8",
   "metadata": {
    "papermill": {
     "duration": 0.013582,
     "end_time": "2024-12-18T08:47:46.374621",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.361039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Feature engineering for time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2949db4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.403073Z",
     "iopub.status.busy": "2024-12-18T08:47:46.402554Z",
     "iopub.status.idle": "2024-12-18T08:47:46.424728Z",
     "shell.execute_reply": "2024-12-18T08:47:46.423820Z"
    },
    "papermill": {
     "duration": 0.038391,
     "end_time": "2024-12-18T08:47:46.426608",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.388217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GLOBAL_TS_LENGTH=[]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def extract_advanced_features(data):\n",
    "    \"\"\"\n",
    "    Extract advanced features from actigraphy data for SII prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Input DataFrame with actigraphy measurements\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Single row DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    # Initial data preprocessing\n",
    "    data = data.copy()\n",
    "    data['timestamp'] = pd.to_datetime(data['relative_date_PCIAT'], unit='D') + pd.to_timedelta(data['time_of_day'])\n",
    "    data = data[data['non-wear_flag'] == 0]\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    data['magnitude'] = np.sqrt(data['X']**2 + data['Y']**2 + data['Z']**2)\n",
    "    data['velocity'] = data['magnitude']\n",
    "    data['distance'] = data['velocity'] * 5  # 5 seconds per observation\n",
    "    data['date'] = data['timestamp'].dt.date\n",
    "    hour = pd.to_datetime(data['time_of_day']).dt.hour\n",
    "    \n",
    "    # Calculate aggregated distances\n",
    "    distances = {\n",
    "        'daily': data.groupby('date')['distance'].sum(),\n",
    "        'monthly': data.groupby(data['timestamp'].dt.to_period('M'))['distance'].sum(),\n",
    "        'quarterly': data.groupby('quarter')['distance'].sum()\n",
    "    }\n",
    "    \n",
    "    # Initialize features dictionary\n",
    "    features = {}\n",
    "    \n",
    "    # Time masks for different periods\n",
    "    time_masks = {\n",
    "        'morning': (hour >= 6) & (hour < 12),\n",
    "        'afternoon': (hour >= 12) & (hour < 18),\n",
    "        'evening': (hour >= 18) & (hour < 22),\n",
    "        'night': (hour >= 22) | (hour < 6)\n",
    "    }\n",
    "    \n",
    "    # 1. Activity Pattern Features\n",
    "    for period, mask in time_masks.items():\n",
    "        features.update({\n",
    "            f'{period}_activity_mean': data.loc[mask, 'magnitude'].mean(),\n",
    "            f'{period}_activity_std': data.loc[mask, 'magnitude'].std(),\n",
    "            f'{period}_enmo_mean': data.loc[mask, 'enmo'].mean()\n",
    "        })\n",
    "    \n",
    "    # 2. Sleep Quality Features\n",
    "    sleep_hours = time_masks['night']\n",
    "    magnitude_threshold = data['magnitude'].mean() + data['magnitude'].std()\n",
    "    \n",
    "    features.update({\n",
    "        'sleep_movement_mean': data.loc[sleep_hours, 'magnitude'].mean(),\n",
    "        'sleep_movement_std': data.loc[sleep_hours, 'magnitude'].std(),\n",
    "        'sleep_disruption_count': len(data.loc[sleep_hours & (data['magnitude'] > \n",
    "            data['magnitude'].mean() + 2 * data['magnitude'].std())]),\n",
    "        'light_exposure_during_sleep': data.loc[sleep_hours, 'light'].mean(),\n",
    "        'sleep_position_changes': len(data.loc[sleep_hours & \n",
    "            (abs(data['anglez'].diff()) > 45)]),\n",
    "        'good_sleep_cycle': int(data.loc[sleep_hours, 'light'].mean() < 50)\n",
    "    })\n",
    "    \n",
    "    # 3. Activity Intensity Features\n",
    "    features.update({\n",
    "        'sedentary_time_ratio': (data['magnitude'] < magnitude_threshold * 0.5).mean(),\n",
    "        'moderate_activity_ratio': ((data['magnitude'] >= magnitude_threshold * 0.5) & \n",
    "            (data['magnitude'] < magnitude_threshold * 1.5)).mean(),\n",
    "        'vigorous_activity_ratio': (data['magnitude'] >= magnitude_threshold * 1.5).mean(),\n",
    "        'activity_peaks_per_day': len(data[data['magnitude'] > \n",
    "            data['magnitude'].quantile(0.95)]) / len(data.groupby('relative_date_PCIAT'))\n",
    "    })\n",
    "    \n",
    "    # 4. Circadian Rhythm Features\n",
    "    hourly_activity = data.groupby(hour)['magnitude'].mean()\n",
    "    features.update({\n",
    "        'circadian_regularity': hourly_activity.std() / hourly_activity.mean(),\n",
    "        'peak_activity_hour': hourly_activity.idxmax(),\n",
    "        'trough_activity_hour': hourly_activity.idxmin(),\n",
    "        'activity_range': hourly_activity.max() - hourly_activity.min()\n",
    "    })\n",
    "    \n",
    "    # 5-11. Additional Feature Groups\n",
    "    weekend_mask = data['weekday'].isin([6, 7])\n",
    "    \n",
    "    features.update({\n",
    "        # Movement Patterns\n",
    "        'movement_entropy': stats.entropy(pd.qcut(data['magnitude'], q=10, duplicates='drop').value_counts()),\n",
    "        'direction_changes': len(data[abs(data['anglez'].diff()) > 30]) / len(data),\n",
    "        'sustained_activity_periods': len(data[data['magnitude'].rolling(12).mean() > \n",
    "            magnitude_threshold]) / len(data),\n",
    "        \n",
    "        # Weekend vs Weekday\n",
    "        'weekend_activity_ratio': data.loc[weekend_mask, 'magnitude'].mean() / \n",
    "            data.loc[~weekend_mask, 'magnitude'].mean(),\n",
    "        'weekend_sleep_difference': data.loc[weekend_mask & sleep_hours, 'magnitude'].mean() - \n",
    "            data.loc[~weekend_mask & sleep_hours, 'magnitude'].mean(),\n",
    "        \n",
    "        # Non-wear Time\n",
    "        'wear_time_ratio': (data['non-wear_flag'] == 0).mean(),\n",
    "        'wear_consistency': len(data['non-wear_flag'].value_counts()),\n",
    "        'longest_wear_streak': data['non-wear_flag'].eq(0).astype(int).groupby(\n",
    "            data['non-wear_flag'].ne(0).cumsum()).sum().max(),\n",
    "        \n",
    "        # Device Usage\n",
    "        'screen_time_proxy': (data['light'] > data['light'].quantile(0.75)).mean(),\n",
    "        'dark_environment_ratio': (data['light'] < data['light'].quantile(0.25)).mean(),\n",
    "        'light_variation': data['light'].std() / data['light'].mean() if data['light'].mean() != 0 else 0,\n",
    "        \n",
    "        # Battery Usage\n",
    "        'battery_drain_rate': -np.polyfit(range(len(data)), data['battery_voltage'], 1)[0],\n",
    "        'battery_variability': data['battery_voltage'].std(),\n",
    "        'low_battery_time': (data['battery_voltage'] < data['battery_voltage'].quantile(0.1)).mean(),\n",
    "        \n",
    "        # Time-based\n",
    "        'days_monitored': data['relative_date_PCIAT'].nunique(),\n",
    "        'total_active_hours': len(data[data['magnitude'] > magnitude_threshold * 0.5]) * 5 / 3600,\n",
    "        'activity_regularity': data.groupby('weekday')['magnitude'].mean().std()\n",
    "    })\n",
    "    \n",
    "    # Variability Features for multiple columns\n",
    "    for col in ['X', 'Y', 'Z', 'enmo', 'anglez']:\n",
    "        features.update({\n",
    "            f'{col}_skewness': data[col].skew(),\n",
    "            f'{col}_kurtosis': data[col].kurtosis(),\n",
    "            f'{col}_trend': np.polyfit(range(len(data)), data[col], 1)[0]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame([features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98470866",
   "metadata": {
    "papermill": {
     "duration": 0.013283,
     "end_time": "2024-12-18T08:47:46.453573",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.440290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12227a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.482123Z",
     "iopub.status.busy": "2024-12-18T08:47:46.481842Z",
     "iopub.status.idle": "2024-12-18T08:47:46.491570Z",
     "shell.execute_reply": "2024-12-18T08:47:46.490674Z"
    },
    "papermill": {
     "duration": 0.026364,
     "end_time": "2024-12-18T08:47:46.493548",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.467184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cf6a4",
   "metadata": {
    "papermill": {
     "duration": 0.013297,
     "end_time": "2024-12-18T08:47:46.520440",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.507143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Preprocess function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec151060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.549056Z",
     "iopub.status.busy": "2024-12-18T08:47:46.548403Z",
     "iopub.status.idle": "2024-12-18T08:47:46.554877Z",
     "shell.execute_reply": "2024-12-18T08:47:46.554028Z"
    },
    "papermill": {
     "duration": 0.022771,
     "end_time": "2024-12-18T08:47:46.556848",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.534077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "def preprocess_time_series_data(train_ts_path, test_ts_path, use_autoencoder=False, use_imputer=False, impute_method=\"mean\"):\n",
    "    train_ts = load_time_series(train_ts_path)\n",
    "    test_ts = load_time_series(test_ts_path)\n",
    "    \n",
    "    if use_imputer:\n",
    "        train_ts = impute(train_ts, method=\"mean\")\n",
    "        test_ts = impute(test_ts, method=\"mean\")\n",
    "\n",
    "    if use_autoencoder:\n",
    "        df_train = train_ts.drop('id', axis=1)\n",
    "        df_test = test_ts.drop('id', axis=1)\n",
    "        \n",
    "        train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "        test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "        train_ts_encoded['id']=train_ts[\"id\"]\n",
    "        test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "        train_ts = train_ts_encoded\n",
    "        test_ts = test_ts_encoded\n",
    "\n",
    "    return train_ts, test_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77a50b",
   "metadata": {
    "papermill": {
     "duration": 0.01344,
     "end_time": "2024-12-18T08:47:46.584041",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.570601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Merge CSV and time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c630a093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.613333Z",
     "iopub.status.busy": "2024-12-18T08:47:46.612771Z",
     "iopub.status.idle": "2024-12-18T08:47:46.621670Z",
     "shell.execute_reply": "2024-12-18T08:47:46.620731Z"
    },
    "papermill": {
     "duration": 0.025439,
     "end_time": "2024-12-18T08:47:46.623617",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.598178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=False, use_numeric_imputation=False, numeric_impute_method=\"knn\"):\n",
    "    \"\"\"\n",
    "    Merge CSV and time-series data into a unified dataset and fill missing values using KNNImputer.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data from CSV.\n",
    "        test (pd.DataFrame): Test data from CSV.\n",
    "        train_ts (pd.DataFrame): Aggregated training time-series data.\n",
    "        test_ts (pd.DataFrame): Aggregated test time-series data.\n",
    "        use_time_series (bool): Whether to include time-series data in the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Merged training and test DataFrames.\n",
    "    \"\"\"\n",
    "    if use_time_series:\n",
    "        # Merge time-series data with train and test data on 'id'\n",
    "        train = pd.merge(train, train_ts, how=\"inner\", on='id')\n",
    "        test = pd.merge(test, test_ts, how=\"inner\", on='id')\n",
    "\n",
    "    # Drop 'id' column after merging\n",
    "    train = train.drop('id', axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    if use_time_series:\n",
    "        # Feature selection\n",
    "        time_series_cols = train_ts.columns.tolist()\n",
    "        time_series_cols.remove(\"id\")\n",
    "\n",
    "    if np.any(np.isinf(train)):\n",
    "        train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if np.any(np.isinf(test)):\n",
    "        test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if use_numeric_imputation:\n",
    "        train = impute(train, method=numeric_impute_method)\n",
    "        test = impute(test, method=numeric_impute_method)\n",
    "\n",
    "    featuresCols = train.columns.to_list()\n",
    "\n",
    "    if use_time_series:\n",
    "        featuresCols += time_series_cols\n",
    "\n",
    "    # Dynamically filter features based on available columns\n",
    "    featuresCols = [col for col in featuresCols if col in train.columns]\n",
    "    print(\"Final features included in train:\", featuresCols)\n",
    "\n",
    "    # Filter features and drop rows with missing target 'sii'\n",
    "    train = train[featuresCols]\n",
    "    train = train.dropna(subset=['sii'])\n",
    "\n",
    "    featuresCols.remove('sii')\n",
    "    test = test[featuresCols]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0826aba",
   "metadata": {
    "papermill": {
     "duration": 0.013622,
     "end_time": "2024-12-18T08:47:46.651008",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.637386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"hyperparameter-tuning\"></a>\n",
    "# **Hyperparameter Tuning with Optuna**\n",
    "This section includes functions to optimize hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23455c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:20:30.493712Z",
     "iopub.status.busy": "2024-12-17T14:20:30.492829Z",
     "iopub.status.idle": "2024-12-17T14:20:30.519198Z",
     "shell.execute_reply": "2024-12-17T14:20:30.517978Z",
     "shell.execute_reply.started": "2024-12-17T14:20:30.493676Z"
    },
    "papermill": {
     "duration": 0.013623,
     "end_time": "2024-12-18T08:47:46.678299",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.664676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"lightgbm\"></a>\n",
    "### **LightGBM Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "179f9560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.707322Z",
     "iopub.status.busy": "2024-12-18T08:47:46.706513Z",
     "iopub.status.idle": "2024-12-18T08:47:46.715107Z",
     "shell.execute_reply": "2024-12-18T08:47:46.714242Z"
    },
    "papermill": {
     "duration": 0.024974,
     "end_time": "2024-12-18T08:47:46.716898",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.691924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_lightgbm(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n",
    "            \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "            \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "            \"random_state\": 42,\n",
    "            \"n_estimators\": 200\n",
    "        }\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\"\n",
    "            # early_stopping_rounds=50\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for LightGBM:\", study.best_value)\n",
    "    print(\"Best Params for LightGBM:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a1ac5",
   "metadata": {
    "papermill": {
     "duration": 0.013392,
     "end_time": "2024-12-18T08:47:46.743861",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.730469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "### **XGBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba98239e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.772934Z",
     "iopub.status.busy": "2024-12-18T08:47:46.772095Z",
     "iopub.status.idle": "2024-12-18T08:47:46.782074Z",
     "shell.execute_reply": "2024-12-18T08:47:46.781020Z"
    },
    "papermill": {
     "duration": 0.02641,
     "end_time": "2024-12-18T08:47:46.783758",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.757348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_xgboost(train, target, n_trials=50):\n",
    "    import optuna\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    def is_gpu_available():\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    use_gpu = is_gpu_available()\n",
    "    tree_method = \"gpu_hist\" if use_gpu else \"hist\"\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-5, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-5, 10.0),\n",
    "            \"tree_method\": tree_method,\n",
    "        }\n",
    "\n",
    "        # Preprocess the training data to remove non-numeric columns\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\",\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "\n",
    "        # Predict and evaluate\n",
    "        preds = model.predict(X_valid)\n",
    "        rounded_preds = np.round(preds).astype(int)  # Ensure preds are rounded here\n",
    "        score = quadratic_weighted_kappa(y_valid, rounded_preds)\n",
    "        return score\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for XGBoost:\", study.best_value)\n",
    "    print(\"Best Params for XGBoost:\", study.best_params)\n",
    "\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a70f771",
   "metadata": {
    "papermill": {
     "duration": 0.01357,
     "end_time": "2024-12-18T08:47:46.811069",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.797499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"catboost\"></a>\n",
    "### **CatBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ff30e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.840125Z",
     "iopub.status.busy": "2024-12-18T08:47:46.839515Z",
     "iopub.status.idle": "2024-12-18T08:47:46.846951Z",
     "shell.execute_reply": "2024-12-18T08:47:46.846112Z"
    },
    "papermill": {
     "duration": 0.02378,
     "end_time": "2024-12-18T08:47:46.848670",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.824890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_catboost(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n",
    "            \"iterations\": 200,\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for CatBoost:\", study.best_value)\n",
    "    print(\"Best Params for CatBoost:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615409b",
   "metadata": {
    "papermill": {
     "duration": 0.013317,
     "end_time": "2024-12-18T08:47:46.875420",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.862103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Prepare Model Optimized Hyperparameter**\n",
    "Create a dictionary of models' best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "247b6025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.904019Z",
     "iopub.status.busy": "2024-12-18T08:47:46.903474Z",
     "iopub.status.idle": "2024-12-18T08:47:46.908732Z",
     "shell.execute_reply": "2024-12-18T08:47:46.907873Z"
    },
    "papermill": {
     "duration": 0.021862,
     "end_time": "2024-12-18T08:47:46.910875",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.889013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def models_best_params(use_lightgbm=False, use_xgboost=False, use_catboost= False):\n",
    "    best_params_dict = {}\n",
    "    if use_lightgbm:\n",
    "        best_params_lgbm = optimize_lightgbm(train, target, n_trials=50)\n",
    "        best_params_dict['LightGBM'] = best_params_lgbm\n",
    "    if use_xgboost:\n",
    "        best_params_xgb = optimize_xgboost(train, target, n_trials=50)\n",
    "        best_params_dict['XGBoost'] = best_params_xgb\n",
    "    if use_catboost:\n",
    "        best_params_catboost = optimize_catboost(train, target, n_trials=50)\n",
    "        best_params_dict['CatBoost'] = best_params_catboost\n",
    "    return best_params_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45c092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:22:26.997103Z",
     "iopub.status.busy": "2024-12-17T14:22:26.996719Z",
     "iopub.status.idle": "2024-12-17T14:22:27.003164Z",
     "shell.execute_reply": "2024-12-17T14:22:27.001924Z",
     "shell.execute_reply.started": "2024-12-17T14:22:26.997069Z"
    },
    "papermill": {
     "duration": 0.013398,
     "end_time": "2024-12-18T08:47:46.937954",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.924556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"model-training\"></a>\n",
    "# **Model Training**\n",
    "Train multiple models with the optimized hyperparameters.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2a14d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:46.966729Z",
     "iopub.status.busy": "2024-12-18T08:47:46.966422Z",
     "iopub.status.idle": "2024-12-18T08:47:46.979766Z",
     "shell.execute_reply": "2024-12-18T08:47:46.978824Z"
    },
    "papermill": {
     "duration": 0.029843,
     "end_time": "2024-12-18T08:47:46.981605",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.951762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method=None, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train models using cross-validation, optionally using Voting or Stacking.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data.\n",
    "        test (pd.DataFrame): Test data.\n",
    "        target (pd.Series): Target variable.\n",
    "        best_params_dict (dict): Optimized hyperparameters for each model.\n",
    "        ensemble_method (str): 'voting', 'stacking', or None for individual models.\n",
    "        n_splits (int): Number of CV splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of aggregated predictions for each model or ensemble method.         \n",
    "        dict: A dictionary of trained models.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    trained_models = {}\n",
    "\n",
    "    models = []\n",
    "    for model_name, params in best_params_dict.items():\n",
    "        if model_name == \"LightGBM\":\n",
    "            model = LGBMRegressor(**params)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**params)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model = CatBoostRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        models.append((model_name, model))\n",
    "\n",
    "    if ensemble_method == 'voting':\n",
    "        # Create a Voting Regressor\n",
    "        ensemble_model = VotingRegressor(estimators=models)\n",
    "    elif ensemble_method == 'stacking':\n",
    "        # Create a Stacking Regressor\n",
    "        ensemble_model = StackingRegressor(estimators=models, final_estimator=LinearRegression())\n",
    "\n",
    "    for model_name, model in ([('Ensemble', ensemble_model)] if ensemble_method else models):\n",
    "        print(f\"Training model with CV: {model_name}\")\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "\n",
    "        train_S = []\n",
    "        test_S = []\n",
    "        \n",
    "        oof_non_rounded = np.zeros(len(target), dtype=float) \n",
    "        oof_rounded = np.zeros(len(target), dtype=int) \n",
    "        test_preds = np.zeros((len(test), n_splits))\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(train, target)):\n",
    "            print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "            X_train, X_valid = train.iloc[train_idx], train.iloc[test_idx]\n",
    "            y_train, y_valid = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_valid)\n",
    "\n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "    \n",
    "            train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "            val_kappa = quadratic_weighted_kappa(y_valid, y_val_pred_rounded)\n",
    "    \n",
    "            train_S.append(train_kappa)\n",
    "            test_S.append(val_kappa)\n",
    "            \n",
    "            test_preds[:, fold] = model.predict(test)\n",
    "            \n",
    "            print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "            \n",
    "\n",
    "        print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "        print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(target, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "\n",
    "        print(\"KappaOPtimizer.x =\",  KappaOPtimizer.x)\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(target, oof_tuned)\n",
    "    \n",
    "        print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n",
    "    \n",
    "        tpm = test_preds.mean(axis=1)\n",
    "        tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "        predictions[model_name] = tpTuned\n",
    "        trained_models[model_name] = model\n",
    "\n",
    "    return predictions, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52ffeb",
   "metadata": {
    "papermill": {
     "duration": 0.013478,
     "end_time": "2024-12-18T08:47:47.009074",
     "exception": false,
     "start_time": "2024-12-18T08:47:46.995596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"submission\"></a>\n",
    "# **Submission**\n",
    "Generate and save the final submission file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10cc8868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:47.038180Z",
     "iopub.status.busy": "2024-12-18T08:47:47.037600Z",
     "iopub.status.idle": "2024-12-18T08:47:47.042630Z",
     "shell.execute_reply": "2024-12-18T08:47:47.041647Z"
    },
    "papermill": {
     "duration": 0.021634,
     "end_time": "2024-12-18T08:47:47.044583",
     "exception": false,
     "start_time": "2024-12-18T08:47:47.022949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_submission_file(predictions, sample):\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"sii\": np.round(predictions).astype(int)\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved!\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5057c",
   "metadata": {
    "papermill": {
     "duration": 0.013709,
     "end_time": "2024-12-18T08:47:47.071994",
     "exception": false,
     "start_time": "2024-12-18T08:47:47.058285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"main-pipeline\"></a>\n",
    "# **Main Pipeline**\n",
    "Run the entire pipeline.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05b9457e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:47.100558Z",
     "iopub.status.busy": "2024-12-18T08:47:47.100285Z",
     "iopub.status.idle": "2024-12-18T08:47:53.565653Z",
     "shell.execute_reply": "2024-12-18T08:47:53.564521Z"
    },
    "papermill": {
     "duration": 6.482269,
     "end_time": "2024-12-18T08:47:53.567842",
     "exception": false,
     "start_time": "2024-12-18T08:47:47.085573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after feature engineering:  (3960, 99)\n",
      "Test shape after feature engineering:  (20, 77)\n",
      "Train shape after removing columns:  (3960, 65)\n",
      "Test shape after removing columns:  (20, 64)\n",
      "Final features included in train: ['Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-Height', 'Physical-Weight', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age', 'BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight', 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight', 'Sex_BMI', 'Sex_HeartRate', 'Age_WaistCirc', 'BMI_FitnessMaxStage', 'Weight_GripStrengthDominant', 'Weight_GripStrengthNonDominant', 'HeartRate_FitnessTime', 'Age_PushUp', 'FFMI_Age', 'InternetUse_SleepDisturbance', 'CGAS_BMI', 'CGAS_FitnessMaxStage', 'sii']\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "n_folds = 5\n",
    "\n",
    "# Set up logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Limit verbosity\n",
    "\n",
    "# Paths\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "sample_path = '/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv'\n",
    "time_series_train = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n",
    "time_series_test = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n",
    "\n",
    "# Toggle for using time-series data\n",
    "use_time_series = False  # Set to False to skip time-series data\n",
    "\n",
    "# Preprocess data\n",
    "train, test, sample = preprocess_csv_data(train_path, test_path, sample_path)\n",
    "\n",
    "if use_time_series:\n",
    "    train_ts, test_ts = preprocess_time_series_data(time_series_train, time_series_test, use_autoencoder=True, use_imputer=True, impute_method=\"mean\")\n",
    "else:\n",
    "    train_ts, test_ts = None, None\n",
    "train, test = merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=use_time_series, use_numeric_imputation=True, numeric_impute_method=\"knn\")\n",
    "\n",
    "train = train.dropna(subset=['sii'])\n",
    "\n",
    "# Target and features\n",
    "target = train[\"sii\"]\n",
    "train = train.drop(columns=[\"sii\"])  # Drop `sii` from features\n",
    "\n",
    "\n",
    "# Ensure `sii` is not in test data\n",
    "if \"sii\" in test.columns:\n",
    "    test = test.drop(columns=[\"sii\"])\n",
    "\n",
    "if \"id\" in train.columns:\n",
    "    train = train.drop(columns=[\"id\"])\n",
    "    test = test.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55f7390e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:53.597933Z",
     "iopub.status.busy": "2024-12-18T08:47:53.597013Z",
     "iopub.status.idle": "2024-12-18T08:47:53.603300Z",
     "shell.execute_reply": "2024-12-18T08:47:53.602340Z"
    },
    "papermill": {
     "duration": 0.022768,
     "end_time": "2024-12-18T08:47:53.605120",
     "exception": false,
     "start_time": "2024-12-18T08:47:53.582352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e78aaada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:53.633888Z",
     "iopub.status.busy": "2024-12-18T08:47:53.633183Z",
     "iopub.status.idle": "2024-12-18T08:47:53.637274Z",
     "shell.execute_reply": "2024-12-18T08:47:53.636499Z"
    },
    "papermill": {
     "duration": 0.020315,
     "end_time": "2024-12-18T08:47:53.639020",
     "exception": false,
     "start_time": "2024-12-18T08:47:53.618705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_dict = {'LightGBM': Params, 'XGBoost': XGB_Params, 'CatBoost': CatBoost_Params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecbbc124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T08:47:53.667970Z",
     "iopub.status.busy": "2024-12-18T08:47:53.667431Z",
     "iopub.status.idle": "2024-12-18T08:50:18.691268Z",
     "shell.execute_reply": "2024-12-18T08:50:18.690014Z"
    },
    "papermill": {
     "duration": 145.041023,
     "end_time": "2024-12-18T08:50:18.693828",
     "exception": false,
     "start_time": "2024-12-18T08:47:53.652805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with CV: Ensemble\n",
      "Training fold 1/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 1 - Train QWK: 0.5000, Validation QWK: 0.3702\n",
      "Training fold 2/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 2 - Train QWK: 0.5263, Validation QWK: 0.4421\n",
      "Training fold 3/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 3 - Train QWK: 0.5486, Validation QWK: 0.3746\n",
      "Training fold 4/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 4 - Train QWK: 0.6183, Validation QWK: 0.3332\n",
      "Training fold 5/5...\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n",
      "Fold 5 - Train QWK: 0.6321, Validation QWK: 0.3364\n",
      "Mean Train QWK --> 0.5650\n",
      "Mean Validation QWK ---> 0.3713\n",
      "KappaOPtimizer.x = [0.58134342 0.93492554 2.36487091]\n",
      "----> || Optimized QWK SCORE :: 0.453\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Train model and make predictions using cross-validation\n",
    "predictions, trained_models = train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method='stacking')\n",
    "\n",
    "# Generate submission\n",
    "final_predictions = next(iter(predictions.values()))\n",
    "submission = generate_submission_file(final_predictions, sample)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 6310681,
     "sourceId": 10210719,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30407,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 310.968269,
   "end_time": "2024-12-18T08:50:23.114528",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-18T08:45:12.146259",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
