{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126150cd",
   "metadata": {
    "papermill": {
     "duration": 0.010264,
     "end_time": "2024-12-22T10:25:19.763139",
     "exception": false,
     "start_time": "2024-12-22T10:25:19.752875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook demonstrates a complete machine learning pipeline, including:\n",
    "- Preprocessing CSV and time-series data.\n",
    "- Using Optuna to tune hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "- Training multiple models with optimized parameters.\n",
    "- Combining predictions using an ensemble method.\n",
    "- Generating a Kaggle submission.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9197d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T13:21:14.156098Z",
     "iopub.status.busy": "2024-12-17T13:21:14.155355Z",
     "iopub.status.idle": "2024-12-17T13:21:14.164244Z",
     "shell.execute_reply": "2024-12-17T13:21:14.162688Z",
     "shell.execute_reply.started": "2024-12-17T13:21:14.156064Z"
    },
    "papermill": {
     "duration": 0.008656,
     "end_time": "2024-12-22T10:25:19.780833",
     "exception": false,
     "start_time": "2024-12-22T10:25:19.772177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Table of Contents**\n",
    "1. [Introduction](#introduction)\n",
    "2. [Libraries and Utilities](#libraries-and-utilities)\n",
    "3. [Preprocessing](#preprocessing)\n",
    "    - [Preprocessing CSV Data](#preprocessing-csv-data)\n",
    "    - [Preprocessing Time-Series Data](#preprocessing-time-series-data)\n",
    "    - [Merging Preprocessed Data](#merging-preprocessed-data)\n",
    "4. [Hyperparameter Tuning with Optuna](#hyperparameter-tuning)\n",
    "    - [LightGBM](#lightgbm)\n",
    "    - [XGBoost](#xgboost)\n",
    "    - [CatBoost](#catboost)\n",
    "5. [Model Training](#model-training)\n",
    "6. [Ensemble Predictions](#ensemble-predictions)\n",
    "7. [Submission](#submission)\n",
    "8. [Main Pipeline](#main-pipeline)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a2f9a",
   "metadata": {
    "papermill": {
     "duration": 0.00862,
     "end_time": "2024-12-22T10:25:19.798444",
     "exception": false,
     "start_time": "2024-12-22T10:25:19.789824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"libraries-and-utilities\"></a>\n",
    "# **Libraries and Utilities**\n",
    "Import the required libraries, including Optuna for hyperparameter optimization.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f73e39",
   "metadata": {
    "papermill": {
     "duration": 0.009125,
     "end_time": "2024-12-22T10:25:19.816116",
     "exception": false,
     "start_time": "2024-12-22T10:25:19.806991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Install packages and import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1fc755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:25:19.835654Z",
     "iopub.status.busy": "2024-12-22T10:25:19.834794Z",
     "iopub.status.idle": "2024-12-22T10:27:33.778942Z",
     "shell.execute_reply": "2024-12-22T10:27:33.777852Z"
    },
    "papermill": {
     "duration": 133.95685,
     "end_time": "2024-12-22T10:27:33.781484",
     "exception": false,
     "start_time": "2024-12-22T10:25:19.824634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.20.3\r\n",
      "Uninstalling protobuf-3.20.3:\r\n",
      "  Successfully uninstalled protobuf-3.20.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: protobuf\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 21.12.2 requires cupy-cuda115, which is not installed.\r\n",
      "tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\r\n",
      "tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\r\n",
      "onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-3.19.6\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/tabtransformertf-0.0.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: tensorflow>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (4.64.1)\r\n",
      "Requirement already satisfied: pandas>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.3.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.7/site-packages (from tabtransformertf==0.0.8) (1.21.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->tabtransformertf==0.0.8) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.1->tabtransformertf==0.0.8) (2022.7.1)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.0->tabtransformertf==0.0.8) (1.2.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.2.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.6.3)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.2)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.19.6)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (15.0.6.1)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.3.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (59.8.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.4.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.29.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.8.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.4.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (23.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (23.1.21)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.14.1)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.16.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.11.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.51.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.38.4)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.8.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.2.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.28.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.6.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.4.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.35.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.9)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (4.11.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2022.12.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.6.2->tabtransformertf==0.0.8) (3.2.2)\r\n",
      "Installing collected packages: tabtransformertf\r\n",
      "Successfully installed tabtransformertf-0.0.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/ft-transformer/tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.19.0) (2.13.3)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.19.0) (23.0)\r\n",
      "tensorflow-addons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall protobuf -y\n",
    "!pip install /kaggle/input/ft-transformer/protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install /kaggle/input/ft-transformer/tabtransformertf-0.0.8-py3-none-any.whl\n",
    "!pip install /kaggle/input/ft-transformer/tensorflow_addons-0.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14149157",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:33.803695Z",
     "iopub.status.busy": "2024-12-22T10:27:33.802860Z",
     "iopub.status.idle": "2024-12-22T10:27:41.277676Z",
     "shell.execute_reply": "2024-12-22T10:27:41.276566Z"
    },
    "papermill": {
     "duration": 7.488545,
     "end_time": "2024-12-22T10:27:41.280169",
     "exception": false,
     "start_time": "2024-12-22T10:27:33.791624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210acaa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:41.301710Z",
     "iopub.status.busy": "2024-12-22T10:27:41.300711Z",
     "iopub.status.idle": "2024-12-22T10:27:45.363908Z",
     "shell.execute_reply": "2024-12-22T10:27:45.363044Z"
    },
    "papermill": {
     "duration": 4.076145,
     "end_time": "2024-12-22T10:27:45.366172",
     "exception": false,
     "start_time": "2024-12-22T10:27:41.290027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAYAAADuMJi0AAAGR0lEQVR42rVZ21IbRxBtCbQrkIR2dQVjsLmDLBsET3nTQ8ouYRkQVf6e/E9+Im958qMfkgoXAaKSSj6C9Jnd2R2NeiRSRaZqitVOT5+Z6dNnWoKGlN94JFp8Ipofkb/7SOXjGyp8wF+z35K3f0uUp/GW4XfLQ8v2gefj3ZCCzojoNfue+43o1Q3l3xB/yA3JO7jnF2pCLnI+pNyx/qw7L+SQ7T2N9p2f8c60QcfcK6KGXsAd+ZvA4LlZYuSSAoOhMs5vwJkEGDlbPMaJoA+FcQ0IH38QLWkbAFLkOOhoMF5tU6/eBRhNjro0ZgKiPRAt3FLhCO/vqdgmNTm32LkmKpvBmQY4q5uAaAgbwDBG2BVv3bfI8KKAMWj2kfw9+pkZREIbEql4ST1x7hgHIANkbJ//MF8mAH/ilTCJ2tIi4ASr1IC3VNqXHKOxjy34mgoImnOQtx1g81fkqTiMOBVGcTogNhiT5iBHET8R8C+iApJUmgim3SQAXhsLQz7ee2G8gOAQNtJckBEplADiAxtX+G9NmhDl0qJKnTvyWlAMPYZnvIviGXRg6/Dh824DBXhP/tbfREXJEIvQ+aaPGjG7pvw6r3xdx+9hqb4dgZaP2XmdHO2K/B0c1+oUph6k8kShBryl/Ft0DYgjTlOieOACHFFpVyUl72T9V3cM1jUoYvxIC2vpCSys/ck70mDYuYvdvKjlMdKAUThneWVU1aAsyjv6PURDiwNsHGBZzY+JtAAgE2TFxdRHJdyIp/f+zqu09M5cDP2F08Ukkpj4YNSdX950HY2pNCCUK/Hhx5ZMBfjNSEzdsIihVzzAMdn9dz4eDYhnyQb9SSCiAryiJcQk82LiTbJ4x2FZJaUenpKnzP95WyDf4Y+QN9EFHHSeDLGdBjjKNQ5vKHf4XMA7KrY0y0GEObBOO/8e1ywuQExOHXktuQyJALEBpcEqhwtHqgiDuCK5b6i0p2MQpcckIIoh+6hYgTZtO8xlMi6O4tKCF/kOGHEg/W0UUpHW0ZoGNZ1ExZWcn7EErgwt4uj50E/sFBjXXIayWvh7WryjasxarZKssXon0zxvvkc32Q0bqbBCuZiKt9dWFysfQefeL29JYFaeztX6tePaZdz5mYx8+6Zq3Mk0wXECQxlhdzgS2wjBHju3j1RIgKyOMdNUE8X0+RAdbSapS11MRCv1SzUXmO6wGZe2SQYrv2MvCSWEv2VODE6DN7bz8ufypgQKW7uQskFTQHULLKyaEyrnlZbgOGLrV5qrn9U79jjm2HJmgkaVN98AfBub91lGPLZBqdroN5LYgjSu4zYZDDHXZOIPC691HqrWI1900I8qLzgKP4ft8DxEWigprPfrO+KcXno9gZz4jjGewWdUcpGCj0qVFuGPYbl2VturndZ2qRvlL8acDO6lF/DY/VjsFesiUK+ypJ+r/ep+cJkSQxEK4PG4WozgA75TYrDDqStE69K8/mzGEM+JXTeqvmedEElMmwCMm2SLd6bNNF9su02zEtoW6nAQtpMj5Gd7fKa//wqonF7UdtHFsVn+6hf1o7AfriPH7M6EeIUEF5zKVxXbYo7kS/OEtOqDYZKPoBsETIixn0uYrasThmzDkhdKPkz2EnaX0HdQbIgr59vAdGYDqjHrxkjS7WOxkTD8sqEqhiwcJETgBYigrBqF08KyDaje9SZ/I1A7MzaTzMGDEulPtZUkuKcyIRAjxEJPVrnVlb/9wkfij31D/pQt1IN+iL8bGJcstBIO7Y5VI/cwDqURbXhMuJxBqD0KLoK3esWFs0Jz5i5ZvJUAfFJMFb9XmGIOnzGpijpcWYCaMqXSQWp8EnCABepQ0Elyi4wfKfsw78ikIqif1pe1AGPlLmojl1SKxHHXp1L+Ut7AmDQHvhI5xHGi4EooO2BR7k78PEkJOdL7cAxQUZ/Tyclu9gnfwGgOmm2lNHGNmZXsq4Pqgc1EG1ATrvKl8s4R9ywwnqulGUnaRLVhxy8v3ieUwy2hbooT68uscW++DCDH0WSzuoyN2D4LUJ/tLECbcSKznwMIFs0ChF4mRTCnQbIIfk4SHJo6A9BMuTnXTs3Ku/KxsgZWqzuSe+Os8cEUfnMBY6UF5gi3SUbd5K7vDjq5WW0UENJlRsWn4sy21Er/E/AvPQSFHy1p4fgAAAAASUVORK5CYII=');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np  # Linear algebra\n",
    "import pandas as pd  # Data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tabtransformertf.utils.preprocessing import df_to_dataset, build_categorical_prep\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2e9068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.388049Z",
     "iopub.status.busy": "2024-12-22T10:27:45.387182Z",
     "iopub.status.idle": "2024-12-22T10:27:45.394786Z",
     "shell.execute_reply": "2024-12-22T10:27:45.393919Z"
    },
    "papermill": {
     "duration": 0.020486,
     "end_time": "2024-12-22T10:27:45.396637",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.376151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72145e",
   "metadata": {
    "papermill": {
     "duration": 0.009283,
     "end_time": "2024-12-22T10:27:45.415633",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.406350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765bd647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.435663Z",
     "iopub.status.busy": "2024-12-22T10:27:45.435322Z",
     "iopub.status.idle": "2024-12-22T10:27:45.440904Z",
     "shell.execute_reply": "2024-12-22T10:27:45.440108Z"
    },
    "papermill": {
     "duration": 0.017725,
     "end_time": "2024-12-22T10:27:45.442726",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.425001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabafed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.463714Z",
     "iopub.status.busy": "2024-12-22T10:27:45.462976Z",
     "iopub.status.idle": "2024-12-22T10:27:45.468920Z",
     "shell.execute_reply": "2024-12-22T10:27:45.468048Z"
    },
    "papermill": {
     "duration": 0.018178,
     "end_time": "2024-12-22T10:27:45.470660",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.452482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_kaggle_working_directory(working_dir):\n",
    "    \"\"\"\n",
    "    Cleans up all files and folders in the specified Kaggle working directory.\n",
    "\n",
    "    Args:\n",
    "        working_dir (str): Path to the Kaggle working directory.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for item in os.listdir(working_dir):\n",
    "        item_path = os.path.join(working_dir, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)  # Remove file or symbolic link\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)  # Remove directory\n",
    "            print(f\"Deleted: {item_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {item_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d6835",
   "metadata": {
    "papermill": {
     "duration": 0.009468,
     "end_time": "2024-12-22T10:27:45.489482",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.480014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "# **Preprocessing**\n",
    "This section contains the preprocessing functions for CSV and time-series data, as well as merging them.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b0828",
   "metadata": {
    "papermill": {
     "duration": 0.009143,
     "end_time": "2024-12-22T10:27:45.508056",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.498913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Imputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67323046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.528688Z",
     "iopub.status.busy": "2024-12-22T10:27:45.527940Z",
     "iopub.status.idle": "2024-12-22T10:27:45.544795Z",
     "shell.execute_reply": "2024-12-22T10:27:45.543846Z"
    },
    "papermill": {
     "duration": 0.029253,
     "end_time": "2024-12-22T10:27:45.546787",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.517534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "def impute(df, method=\"knn\", n_neighbors=5):\n",
    "    # impute categorical columns\n",
    "    if method == \"cat\":\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "        return df\n",
    "\n",
    "    # get numeric columns instead of 'sii'\n",
    "    numeric_cols = df.select_dtypes(include=['number', 'float64', 'int64']).columns\n",
    "    numeric_cols = list(numeric_cols)\n",
    "    if \"sii\" in numeric_cols:\n",
    "        numeric_cols.remove(\"sii\")\n",
    "    numeric_cols = pd.Index(numeric_cols)\n",
    "    \n",
    "    if method == \"knn\":\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        # impute_data = imputer.fit_transform(df[numeric_cols])\n",
    "    if method == \"mean\":\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "    if method == \"median\":\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "    \n",
    "    imputed_data = imputer.fit_transform(df[numeric_cols])\n",
    "    df_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "    for col in df.columns:\n",
    "        if col not in numeric_cols:\n",
    "            df_imputed[col] = df[col]          \n",
    "    df = df_imputed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede76795",
   "metadata": {
    "papermill": {
     "duration": 0.009311,
     "end_time": "2024-12-22T10:27:45.565867",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.556556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess CSV data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775e26a",
   "metadata": {
    "papermill": {
     "duration": 0.009383,
     "end_time": "2024-12-22T10:27:45.584634",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.575251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Feature engineering for CSV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "473009e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.605359Z",
     "iopub.status.busy": "2024-12-22T10:27:45.604704Z",
     "iopub.status.idle": "2024-12-22T10:27:45.616124Z",
     "shell.execute_reply": "2024-12-22T10:27:45.615208Z"
    },
    "papermill": {
     "duration": 0.02401,
     "end_time": "2024-12-22T10:27:45.618037",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.594027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "\n",
    "    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n",
    "    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n",
    "    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n",
    "    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n",
    "    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n",
    "    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n",
    "    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n",
    "    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n",
    "    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n",
    "    df['FFMI_Age'] = df['BIA-BIA_FFMI'] * df['Basic_Demos-Age']\n",
    "    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n",
    "    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n",
    "    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54990496",
   "metadata": {
    "papermill": {
     "duration": 0.009215,
     "end_time": "2024-12-22T10:27:45.636847",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.627632",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After we analyzed the dataset, we observed that there are lot of unreasonable values in the dataset. Therefore, we decided to prune some anomaly samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6f2ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.657547Z",
     "iopub.status.busy": "2024-12-22T10:27:45.656875Z",
     "iopub.status.idle": "2024-12-22T10:27:45.665456Z",
     "shell.execute_reply": "2024-12-22T10:27:45.664579Z"
    },
    "papermill": {
     "duration": 0.021015,
     "end_time": "2024-12-22T10:27:45.667298",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.646283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    input_length = len(df)\n",
    "    df = df.drop(df[df['Physical-BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Systolic_BP'] <= 0].index)\n",
    "    df = df.drop(df[df['Physical-Diastolic_BP'] > 160].index)\n",
    "\n",
    "    children = df[df['Basic_Demos-Age'] <= 12]\n",
    "    df = df.drop(children[children['FGC-FGC_CU'] > 80].index)\n",
    "    df = df.drop(children[children['FGC-FGC_GSND'] > 80].index)\n",
    "\n",
    "    df = df.drop(df[df['BIA-BIA_BMI'] <= 0].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMC'] > 1000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_BMR'] > 40000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_DEE'] > 60000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ECW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_FFM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_ICW'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LDM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_LST'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_SMM'] > 2000].index)\n",
    "    df = df.drop(df[df['BIA-BIA_TBW'] > 2000].index)\n",
    "    output_length = len(df)\n",
    "    print (input_length, output_length)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31ec6d",
   "metadata": {
    "papermill": {
     "duration": 0.009607,
     "end_time": "2024-12-22T10:27:45.686591",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.676984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Preprocess Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53d524a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.707259Z",
     "iopub.status.busy": "2024-12-22T10:27:45.706491Z",
     "iopub.status.idle": "2024-12-22T10:27:45.713726Z",
     "shell.execute_reply": "2024-12-22T10:27:45.712783Z"
    },
    "papermill": {
     "duration": 0.019525,
     "end_time": "2024-12-22T10:27:45.715541",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.696016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_csv_data(train_path, test_path, sample_path):\n",
    "    \"\"\"\n",
    "    Preprocess CSV data with proper handling of missing columns.\n",
    "    Args:\n",
    "        train_path (str): Path to the training CSV file.\n",
    "        test_path (str): Path to the test CSV file.\n",
    "        sample_path (str): Path to the sample submission CSV file\n",
    "    Returns:\n",
    "        tuple: Preprocessed training DataFrame, test DataFrame, and sample submission.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    sample = pd.read_csv(sample_path)\n",
    "\n",
    "    # # remove outlier from train\n",
    "    # train = remove_outliers(train)\n",
    "    # print(\"Train shape after removing outlier: \", train.shape)\n",
    "\n",
    "    # feature engineering for both train and test\n",
    "    train = csv_feature_engineering(train)\n",
    "    test = csv_feature_engineering(test)\n",
    "    print(\"Train shape after feature engineering: \", train.shape)\n",
    "    print(\"Test shape after feature engineering: \", test.shape)\n",
    "\n",
    "    # Only use columns in both train and test\n",
    "    # Ensure that the columns in `train` and `test` match\n",
    "    # Remove some columns\n",
    "    common_columns = test.columns.to_list() \n",
    "    remove_columns = ['BIA-BIA_LDM', 'Physical-Waist_Circumference', 'FGC-FGC_SRL', 'FGC-FGC_GSND', 'BIA-BIA_ECW', 'Physical-BMI', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_FFM', 'BIA-BIA_TBW', 'BIA-BIA_BMR', 'BIA-BIA_ICW', 'BIA-BIA_DEE']\n",
    "    common_columns = [col for col in common_columns if col not in remove_columns]\n",
    "    \n",
    "    train = train[[\"sii\"] + common_columns]\n",
    "    test = test[common_columns]\n",
    "    print(\"Train shape after removing columns: \", train.shape)\n",
    "    print(\"Test shape after removing columns: \", test.shape)\n",
    "\n",
    "    return train, test, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3b946",
   "metadata": {
    "papermill": {
     "duration": 0.009308,
     "end_time": "2024-12-22T10:27:45.734482",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.725174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Preprocess time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "188bab41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.755238Z",
     "iopub.status.busy": "2024-12-22T10:27:45.754931Z",
     "iopub.status.idle": "2024-12-22T10:27:45.777608Z",
     "shell.execute_reply": "2024-12-22T10:27:45.776663Z"
    },
    "papermill": {
     "duration": 0.035408,
     "end_time": "2024-12-22T10:27:45.779452",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.744044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering_ts(worn_data):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the time-series data for a single participant.\n",
    "    Returns a DataFrame with all derived features for this participant.\n",
    "    \"\"\"\n",
    "    mvpa_threshold = 0.1\n",
    "    vig_threshold = 0.5\n",
    "    window_size = 12\n",
    "\n",
    "    # Filter worn data\n",
    "    worn_data = worn_data[worn_data['non-wear_flag'] == 0].copy()\n",
    "\n",
    "    # Time of day conversions\n",
    "    worn_data['time_of_day_hours'] = worn_data['time_of_day'] / 1e9 / 3600\n",
    "    worn_data['day_time'] = worn_data['relative_date_PCIAT'] + (worn_data['time_of_day_hours'] / 24)\n",
    "    worn_data['day_period'] = np.where(\n",
    "        (worn_data['time_of_day_hours'] >= 8) & (worn_data['time_of_day_hours'] < 21),\n",
    "        'day', 'night'\n",
    "    )\n",
    "\n",
    "    # Time differences\n",
    "    worn_data['time_diff'] = (worn_data['day_time'].diff() * 86400).round(0)\n",
    "    worn_data['measurement_after_gap'] = worn_data['time_diff'] > 5\n",
    "\n",
    "    # Classify activity levels\n",
    "    worn_data['activity_type'] = pd.cut(\n",
    "        worn_data['enmo'],\n",
    "        bins=[-np.inf, mvpa_threshold, vig_threshold, np.inf],\n",
    "        labels=['low', 'moderate', 'vigorous']\n",
    "    )\n",
    "\n",
    "    # Aggregate activity periods\n",
    "    activity_group = (\n",
    "        (worn_data['activity_type'] != worn_data['activity_type'].shift()) |\n",
    "        (worn_data['measurement_after_gap'])\n",
    "    ).cumsum()\n",
    "\n",
    "    activity_periods = worn_data.groupby(activity_group).agg(\n",
    "        min=('day_time', 'min'),\n",
    "        max=('day_time', 'max'),\n",
    "        activity_type=('activity_type', 'first')\n",
    "    )\n",
    "    activity_periods['duration_sec'] = (activity_periods['max'] - activity_periods['min']) * 86400 + 5\n",
    "    activity_periods = activity_periods[activity_periods['duration_sec'] >= 60]\n",
    "\n",
    "    # Add day and transition number\n",
    "    activity_periods['day'] = activity_periods['min'].astype(int)\n",
    "    activity_periods['transition_num'] = (\n",
    "        activity_periods.groupby('day')['activity_type']\n",
    "        .apply(lambda x: (x != x.shift()).cumsum())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Calculate activity-level summary statistics\n",
    "    activity_summary = {}\n",
    "    for act_type in ['low', 'moderate']:\n",
    "        activity_data = activity_periods[activity_periods['activity_type'] == act_type]\n",
    "        if activity_data.empty:\n",
    "            for stat in ['median', 'max', 'std']:\n",
    "                activity_summary[f'{act_type}_duration_{stat}'] = 0\n",
    "                activity_summary[f'{act_type}_count_periods_{stat}'] = 0\n",
    "            continue\n",
    "        stats = activity_data.groupby('day').agg(\n",
    "            total_duration=('duration_sec', 'sum'),\n",
    "            count_periods=('duration_sec', 'size')\n",
    "        ).agg(['median', 'max', 'std'])\n",
    "\n",
    "        for stat in ['median', 'max', 'std']:\n",
    "            activity_summary[f'{act_type}_duration_{stat}'] = stats.loc[stat, 'total_duration']\n",
    "            activity_summary[f'{act_type}_count_periods_{stat}'] = stats.loc[stat, 'count_periods']\n",
    "\n",
    "    # Add daily transition statistics\n",
    "    daily_transitions = activity_periods.groupby('day')['transition_num'].max()\n",
    "    trans_stats = daily_transitions.agg(['median', 'max', 'std'])\n",
    "    for stat in ['median', 'max', 'std']:\n",
    "        activity_summary[f'transitions_{stat}'] = trans_stats[stat]\n",
    "\n",
    "    # Hourly activity features\n",
    "    hourly_activity = worn_data.groupby(\n",
    "        [worn_data['relative_date_PCIAT'].astype(int),\n",
    "         worn_data['time_of_day_hours'].astype(int),\n",
    "         worn_data['day_period']]\n",
    "    )['enmo'].agg(['mean', 'max'])\n",
    "\n",
    "    features = hourly_activity['mean'].groupby(\n",
    "        ['relative_date_PCIAT', 'day_period']\n",
    "    ).agg(\n",
    "        std_across_hours='std',\n",
    "        peak_hour=lambda x: x.idxmax()[1] if not x.empty else 0,\n",
    "        entropy=lambda x: -(x / x.sum() * np.log(x / x.sum() + 1e-9)).sum() if not x.empty else 0\n",
    "    )\n",
    "\n",
    "    # Day and night features\n",
    "    if 'day' in features.index.get_level_values('day_period'):\n",
    "        day_features = features.xs('day', level='day_period').agg(['median', 'max', 'std']).stack().to_frame().T\n",
    "        day_features.columns = [f'{stat}_{feature}_day' for stat, feature in day_features.columns]\n",
    "    else:\n",
    "        day_features = pd.DataFrame(columns=[f'{stat}_{feature}_day' for stat in ['median', 'max', 'std'] for feature in ['std_across_hours', 'peak_hour', 'entropy']])\n",
    "\n",
    "    if 'night' in features.index.get_level_values('day_period'):\n",
    "        night_features = features.xs('night', level='day_period').agg(['median', 'max', 'std']).stack().to_frame().T\n",
    "        night_features.columns = [f'{stat}_{feature}_night' for stat, feature in night_features.columns]\n",
    "    else:\n",
    "        night_features = pd.DataFrame(columns=[f'{stat}_{feature}_night' for stat in ['median', 'max', 'std'] for feature in ['std_across_hours', 'peak_hour', 'entropy']])\n",
    "\n",
    "    def merge_mvpa_groups(df, allowed_gap=60, merge_gap=60):\n",
    "        last_mvpa_time = df['day_time'].where(df['is_mvpa']).ffill().shift()\n",
    "        mvpa_time_diff = ((df['day_time'] - last_mvpa_time) * 86400).round(0)\n",
    "        mvpa_group = (\n",
    "            (df['is_mvpa'] != df['is_mvpa'].shift()) |\n",
    "            (df['time_diff'] >= allowed_gap)\n",
    "        ).cumsum()\n",
    "        is_mvpa_start = (\n",
    "            (mvpa_group != mvpa_group.shift()) &\n",
    "            df['is_mvpa']\n",
    "        )\n",
    "        group_increment = is_mvpa_start & (\n",
    "            (mvpa_time_diff >= merge_gap) | last_mvpa_time.isnull()\n",
    "        )\n",
    "        merged_group = group_increment.cumsum()\n",
    "        merged_group.loc[~df['is_mvpa']] = np.nan\n",
    "        return merged_group\n",
    "    \n",
    "    # Calculate daily MVPA statistics\n",
    "    worn_data['is_mvpa'] = worn_data['enmo'] > mvpa_threshold\n",
    "    worn_data['mvpa_merged_group'] = merge_mvpa_groups(worn_data)\n",
    "\n",
    "    mvpa_periods = worn_data[worn_data['is_mvpa']].groupby('mvpa_merged_group')['day_time'].agg(['min', 'max'])\n",
    "    mvpa_periods['duration_sec'] = (mvpa_periods['max'] - mvpa_periods['min']) * 86400\n",
    "    mvpa_periods = mvpa_periods[mvpa_periods['duration_sec'] >= 60]\n",
    "\n",
    "    mvpa_periods['day'] = mvpa_periods['min'].astype(int)\n",
    "    daily_stats = mvpa_periods.groupby('day').agg(\n",
    "        total_duration=('duration_sec', 'sum'),\n",
    "        count_periods=('duration_sec', 'size')\n",
    "    )\n",
    "\n",
    "    # Extract daily stats features\n",
    "    daily_stats_features = daily_stats.agg(\n",
    "        ['median', 'max', 'std']\n",
    "    ).unstack().to_frame().T\n",
    "\n",
    "    daily_stats_features.columns = [\n",
    "        f'{stat}_{feature}' for feature, stat in daily_stats_features.columns\n",
    "    ]\n",
    "\n",
    "    # Combine all features\n",
    "    combined_features = pd.concat(\n",
    "        [pd.DataFrame([activity_summary]), day_features, night_features, daily_stats_features],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed051105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.799548Z",
     "iopub.status.busy": "2024-12-22T10:27:45.799248Z",
     "iopub.status.idle": "2024-12-22T10:27:45.806940Z",
     "shell.execute_reply": "2024-12-22T10:27:45.806112Z"
    },
    "papermill": {
     "duration": 0.01983,
     "end_time": "2024-12-22T10:27:45.808721",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.788891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file_and_engineer_features(filename, dirname, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single participant's Parquet file, apply feature engineering, and save features to disk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the Parquet file\n",
    "        file_path = os.path.join(dirname, filename, 'part-0.parquet')\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Add participant ID\n",
    "        participant_id = filename.split('=')[1]\n",
    "        df['id'] = participant_id\n",
    "\n",
    "        # Apply feature engineering\n",
    "        features = feature_engineering_ts(df)\n",
    "\n",
    "        # Add participant ID to features\n",
    "        features['id'] = participant_id\n",
    "\n",
    "        # Save the engineered features to disk\n",
    "        output_file = os.path.join(output_dir, f\"{participant_id}_features.parquet\")\n",
    "        features.to_parquet(output_file, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "\n",
    "def load_time_series(dirname, output_dir) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all Parquet files, save engineered features to disk, and return the combined features.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Process participants one by one\n",
    "    for filename in tqdm(os.listdir(dirname), desc=\"Processing participants\"):\n",
    "        process_file_and_engineer_features(filename, dirname, output_dir)\n",
    "\n",
    "    # Combine all saved features into a single DataFrame\n",
    "    feature_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\"_features.parquet\")]\n",
    "    combined_features = pd.concat([pd.read_parquet(f) for f in tqdm(feature_files, desc=\"Combining features\")], ignore_index=True)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d24469",
   "metadata": {
    "papermill": {
     "duration": 0.009328,
     "end_time": "2024-12-22T10:27:45.827475",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.818147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803b5fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.848230Z",
     "iopub.status.busy": "2024-12-22T10:27:45.847925Z",
     "iopub.status.idle": "2024-12-22T10:27:45.866323Z",
     "shell.execute_reply": "2024-12-22T10:27:45.865373Z"
    },
    "papermill": {
     "duration": 0.031477,
     "end_time": "2024-12-22T10:27:45.868385",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.836908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sparse Autoencoder\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, sparsity_weight=1e-5):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()  # Outputs in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "# Data preparation function\n",
    "def prepare_data(data, scaler_type=\"MinMaxScaler\"):\n",
    "    \"\"\"\n",
    "    Prepares data for model training, ensuring only numeric columns are scaled.\n",
    "    Returns PyTorch tensor.\n",
    "    \"\"\"\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    if scaler_type == \"RobustScaler\":\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numeric columns\n",
    "    data_scaled = scaler.fit_transform(data[numeric_cols])\n",
    "    return torch.tensor(data_scaled, dtype=torch.float32), scaler\n",
    "\n",
    "\n",
    "# PCA function\n",
    "def apply_pca(data, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    return data_pca, pca\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "# Training function\n",
    "def perform_sparse_autoencoder(data, epochs=100, batch_size=32, learning_rate=0.001, patience=10, scaler_type=\"MinMaxScaler\", use_pca=False, sparsity_weight=1e-5):\n",
    "    # Preprocess data\n",
    "    if use_pca:\n",
    "        data, pca = apply_pca(data)\n",
    "\n",
    "    data_tensor, scaler = prepare_data(data, scaler_type=scaler_type)\n",
    "    train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Ensure data is PyTorch tensors\n",
    "    assert isinstance(train_data, torch.Tensor), \"train_data must be a PyTorch tensor\"\n",
    "    assert isinstance(val_data, torch.Tensor), \"val_data must be a PyTorch tensor\"\n",
    "\n",
    "    # DataLoader setup\n",
    "    train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = SparseAutoencoder(input_dim=data_tensor.shape[1], sparsity_weight=sparsity_weight)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            encoded, outputs = model(batch)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            loss = criterion(outputs, batch)\n",
    "\n",
    "            # Sparsity penalty\n",
    "            l1_penalty = torch.mean(torch.abs(encoded))\n",
    "            loss += sparsity_weight * l1_penalty\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch[0].to(device)\n",
    "                _, outputs = model(batch)\n",
    "                loss = criterion(outputs, batch)\n",
    "                val_loss += loss.item() * batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        stopper(val_loss)\n",
    "        if stopper.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Return encoded features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_features, _ = model(data_tensor.to(device))\n",
    "    encoded_features = encoded_features.cpu().numpy()\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=[f\"feature_{i}\" for i in range(encoded_features.shape[1])])\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aa3f1f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.888745Z",
     "iopub.status.busy": "2024-12-22T10:27:45.888435Z",
     "iopub.status.idle": "2024-12-22T10:27:45.899465Z",
     "shell.execute_reply": "2024-12-22T10:27:45.898549Z"
    },
    "papermill": {
     "duration": 0.023607,
     "end_time": "2024-12-22T10:27:45.901578",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.877971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    # Keep only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    # Scale the numeric data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_numeric)\n",
    "    \n",
    "    # Convert to a PyTorch tensor\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    # Define the autoencoder model\n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    # Set up the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    # Extract encoded data\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    # Return the encoded data as a DataFrame\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a63d5",
   "metadata": {
    "papermill": {
     "duration": 0.009342,
     "end_time": "2024-12-22T10:27:45.920587",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.911245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Preprocess function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7d20b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.941038Z",
     "iopub.status.busy": "2024-12-22T10:27:45.940715Z",
     "iopub.status.idle": "2024-12-22T10:27:45.948707Z",
     "shell.execute_reply": "2024-12-22T10:27:45.947787Z"
    },
    "papermill": {
     "duration": 0.020399,
     "end_time": "2024-12-22T10:27:45.950467",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.930068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_time_series_data(train_ts_path, test_ts_path, use_autoencoder=False, use_imputer=False, impute_method=\"mean\", outdir='/kaggle/working/intermediate_results'):\n",
    "    \"\"\"\n",
    "    Preprocess time-series data, including feature engineering and optional autoencoder-based encoding.\n",
    "    Args:\n",
    "        train_ts_path (str): Path to training time-series data.\n",
    "        test_ts_path (str): Path to test time-series data.\n",
    "        use_autoencoder (bool): Whether to encode features with a sparse autoencoder.\n",
    "        use_imputer (bool): Whether to impute missing values.\n",
    "        impute_method (str): Imputation method (\"mean\", \"knn\", etc.).\n",
    "    Returns:\n",
    "        tuple: Preprocessed training and test time-series DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_ts = load_time_series(train_ts_path, '/kaggle/working/final')\n",
    "    test_ts = load_time_series(test_ts_path, '/kaggle/working/final_test')\n",
    "\n",
    "    # # Apply feature engineering\n",
    "    # train_ts = feature_engineering_ts(train_ts)\n",
    "    # test_ts = feature_engineering_ts(test_ts)\n",
    "\n",
    "    # Impute missing values\n",
    "    if use_imputer:\n",
    "        train_ts = impute(train_ts, method=impute_method)\n",
    "        test_ts = impute(test_ts, method=impute_method)\n",
    "\n",
    "    # Encode features with a sparse autoencoder\n",
    "    if use_autoencoder:\n",
    "        train_ts_encoded = perform_autoencoder(\n",
    "            train_ts, encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        test_ts_encoded = perform_autoencoder(\n",
    "            test_ts, \n",
    "            encoding_dim=60, epochs=100, batch_size=32\n",
    "        )\n",
    "        train_ts_encoded['id'] = train_ts[\"id\"]\n",
    "        test_ts_encoded['id'] = test_ts[\"id\"]\n",
    "\n",
    "        return train_ts_encoded, test_ts_encoded\n",
    "\n",
    "    return train_ts, test_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d3e1a",
   "metadata": {
    "papermill": {
     "duration": 0.009335,
     "end_time": "2024-12-22T10:27:45.969318",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.959983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Merge CSV and time series data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97ad61e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:45.989714Z",
     "iopub.status.busy": "2024-12-22T10:27:45.989093Z",
     "iopub.status.idle": "2024-12-22T10:27:45.997778Z",
     "shell.execute_reply": "2024-12-22T10:27:45.996925Z"
    },
    "papermill": {
     "duration": 0.020762,
     "end_time": "2024-12-22T10:27:45.999562",
     "exception": false,
     "start_time": "2024-12-22T10:27:45.978800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=False, use_numeric_imputation=False, numeric_impute_method=\"knn\"):\n",
    "    \"\"\"\n",
    "    Merge CSV and time-series data into a unified dataset and fill missing values using KNNImputer.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data from CSV.\n",
    "        test (pd.DataFrame): Test data from CSV.\n",
    "        train_ts (pd.DataFrame): Aggregated training time-series data.\n",
    "        test_ts (pd.DataFrame): Aggregated test time-series data.\n",
    "        use_time_series (bool): Whether to include time-series data in the merged dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Merged training and test DataFrames.\n",
    "    \"\"\"\n",
    "    featuresCols = train.columns.to_list()\n",
    "    if use_time_series:\n",
    "        # Merge time-series data with train and test data on 'id'\n",
    "        train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "        test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "    # Drop 'id' column after merging\n",
    "    train = train.drop('id', axis=1)\n",
    "    test = test.drop('id', axis=1)\n",
    "\n",
    "    if use_time_series:\n",
    "        # Feature selection\n",
    "        time_series_cols = train_ts.columns.tolist()\n",
    "        time_series_cols.remove(\"id\")\n",
    "\n",
    "    if np.any(np.isinf(train)):\n",
    "        train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if np.any(np.isinf(test)):\n",
    "        test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if use_numeric_imputation:\n",
    "        train = impute(train, method=numeric_impute_method)\n",
    "        test = impute(test, method=numeric_impute_method)\n",
    "\n",
    "    if use_time_series:\n",
    "        featuresCols += time_series_cols\n",
    "\n",
    "    # Dynamically filter features based on available columns\n",
    "    featuresCols = [col for col in featuresCols if col in train.columns]\n",
    "    print(\"Final features included in train:\", featuresCols)\n",
    "\n",
    "    # Filter features and drop rows with missing target 'sii'\n",
    "    train = train[featuresCols]\n",
    "    train = train.dropna(subset=['sii'])\n",
    "\n",
    "    featuresCols.remove('sii')\n",
    "    test = test[featuresCols]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b90a29",
   "metadata": {
    "papermill": {
     "duration": 0.009284,
     "end_time": "2024-12-22T10:27:46.018183",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.008899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"hyperparameter-tuning\"></a>\n",
    "# **Hyperparameter Tuning with Optuna**\n",
    "This section includes functions to optimize hyperparameters for LightGBM, XGBoost, and CatBoost.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cae77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:20:30.493712Z",
     "iopub.status.busy": "2024-12-17T14:20:30.492829Z",
     "iopub.status.idle": "2024-12-17T14:20:30.519198Z",
     "shell.execute_reply": "2024-12-17T14:20:30.517978Z",
     "shell.execute_reply.started": "2024-12-17T14:20:30.493676Z"
    },
    "papermill": {
     "duration": 0.009301,
     "end_time": "2024-12-22T10:27:46.037008",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.027707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"lightgbm\"></a>\n",
    "### **LightGBM Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c3c0862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.057763Z",
     "iopub.status.busy": "2024-12-22T10:27:46.057102Z",
     "iopub.status.idle": "2024-12-22T10:27:46.065261Z",
     "shell.execute_reply": "2024-12-22T10:27:46.064400Z"
    },
    "papermill": {
     "duration": 0.020596,
     "end_time": "2024-12-22T10:27:46.067160",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.046564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_lightgbm(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n",
    "            \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "            \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "            \"random_state\": 42,\n",
    "            \"n_estimators\": 200\n",
    "        }\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\"\n",
    "            # early_stopping_rounds=50\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for LightGBM:\", study.best_value)\n",
    "    print(\"Best Params for LightGBM:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6629033",
   "metadata": {
    "papermill": {
     "duration": 0.009239,
     "end_time": "2024-12-22T10:27:46.085997",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.076758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "### **XGBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6543d0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.106688Z",
     "iopub.status.busy": "2024-12-22T10:27:46.105924Z",
     "iopub.status.idle": "2024-12-22T10:27:46.115002Z",
     "shell.execute_reply": "2024-12-22T10:27:46.114249Z"
    },
    "papermill": {
     "duration": 0.021184,
     "end_time": "2024-12-22T10:27:46.116746",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.095562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_xgboost(train, target, n_trials=50):\n",
    "    import optuna\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    def is_gpu_available():\n",
    "        try:\n",
    "            import torch\n",
    "            return torch.cuda.is_available()\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    use_gpu = is_gpu_available()\n",
    "    tree_method = \"gpu_hist\" if use_gpu else \"hist\"\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-5, 10.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-5, 10.0),\n",
    "            \"tree_method\": tree_method,\n",
    "        }\n",
    "\n",
    "        # Preprocess the training data to remove non-numeric columns\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric=\"rmse\",\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "\n",
    "        # Predict and evaluate\n",
    "        preds = model.predict(X_valid)\n",
    "        rounded_preds = np.round(preds).astype(int)  # Ensure preds are rounded here\n",
    "        score = quadratic_weighted_kappa(y_valid, rounded_preds)\n",
    "        return score\n",
    "\n",
    "    # Run Optuna optimization\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for XGBoost:\", study.best_value)\n",
    "    print(\"Best Params for XGBoost:\", study.best_params)\n",
    "\n",
    "    return study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e616ad8",
   "metadata": {
    "papermill": {
     "duration": 0.00935,
     "end_time": "2024-12-22T10:27:46.135462",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.126112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"catboost\"></a>\n",
    "### **CatBoost Hyperparameter Optimization**\n",
    "Use Optuna to tune hyperparameters for CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6d05c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.155541Z",
     "iopub.status.busy": "2024-12-22T10:27:46.155237Z",
     "iopub.status.idle": "2024-12-22T10:27:46.162744Z",
     "shell.execute_reply": "2024-12-22T10:27:46.162001Z"
    },
    "papermill": {
     "duration": 0.019634,
     "end_time": "2024-12-22T10:27:46.164536",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.144902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_catboost(train, target, n_trials=50):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 0.3),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 12),\n",
    "            \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 10.0),\n",
    "            \"iterations\": 200,\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "\n",
    "        CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            train, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid))\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        score = quadratic_weighted_kappa(y_valid, preds.round(0).astype(int))\n",
    "        return score\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best Score for CatBoost:\", study.best_value)\n",
    "    print(\"Best Params for CatBoost:\", study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa9706",
   "metadata": {
    "papermill": {
     "duration": 0.009229,
     "end_time": "2024-12-22T10:27:46.183078",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.173849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Prepare Model Optimized Hyperparameter**\n",
    "Create a dictionary of models' best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb05126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.203331Z",
     "iopub.status.busy": "2024-12-22T10:27:46.203018Z",
     "iopub.status.idle": "2024-12-22T10:27:46.209744Z",
     "shell.execute_reply": "2024-12-22T10:27:46.208858Z"
    },
    "papermill": {
     "duration": 0.019363,
     "end_time": "2024-12-22T10:27:46.211821",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.192458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def models_best_params(use_lightgbm=False, use_xgboost=False, use_catboost= False):\n",
    "    best_params_dict = {}\n",
    "    if use_lightgbm:\n",
    "        best_params_lgbm = optimize_lightgbm(train, target, n_trials=50)\n",
    "        best_params_dict['LightGBM'] = best_params_lgbm\n",
    "    if use_xgboost:\n",
    "        best_params_xgb = optimize_xgboost(train, target, n_trials=50)\n",
    "        best_params_dict['XGBoost'] = best_params_xgb\n",
    "    if use_catboost:\n",
    "        best_params_catboost = optimize_catboost(train, target, n_trials=50)\n",
    "        best_params_dict['CatBoost'] = best_params_catboost\n",
    "    return best_params_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292b7ee",
   "metadata": {
    "papermill": {
     "duration": 0.009469,
     "end_time": "2024-12-22T10:27:46.231225",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.221756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **FT-Transformer wrapper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46b10fe8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.252246Z",
     "iopub.status.busy": "2024-12-22T10:27:46.251636Z",
     "iopub.status.idle": "2024-12-22T10:27:46.270377Z",
     "shell.execute_reply": "2024-12-22T10:27:46.269316Z"
    },
    "papermill": {
     "duration": 0.031535,
     "end_time": "2024-12-22T10:27:46.272321",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.240786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "class FTTransformerWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, \n",
    "                 numerical_features=None,\n",
    "                 categorical_features=None,\n",
    "                 embedding_dim=64,\n",
    "                 depth=4,\n",
    "                 heads=8,\n",
    "                 attn_dropout=0.3,\n",
    "                 ff_dropout=0.3,\n",
    "                 out_dim=1,\n",
    "                 out_activation='relu',\n",
    "                 lr=0.001,\n",
    "                 weight_decay=0.0001,\n",
    "                 loss=None,\n",
    "                 metrics=None,\n",
    "                 epochs=1000,\n",
    "                 patience=10,\n",
    "                 batch_size=1024,\n",
    "                 shuffle=True,\n",
    "                 seed=42):  # Add seed parameter\n",
    "        # Model hyperparameters\n",
    "        self.numerical_features = numerical_features\n",
    "        self.categorical_features = categorical_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ff_dropout = ff_dropout\n",
    "        self.out_dim = out_dim\n",
    "        self.out_activation = out_activation\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss = loss or tf.keras.losses.MeanSquaredError()\n",
    "        self.metrics = metrics or [tf.keras.metrics.RootMeanSquaredError()]\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed  # Save seed\n",
    "        \n",
    "        # Internal objects\n",
    "        self.imputer = SimpleImputer(strategy='median')  # Handle missing values\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Set random seeds for reproducibility\n",
    "        self._set_random_seed(self.seed)\n",
    "    \n",
    "    def _set_random_seed(self, seed):\n",
    "        \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Create the encoder\n",
    "        encoder = FTTransformerEncoder(\n",
    "            numerical_features=self.numerical_features,\n",
    "            categorical_features=self.categorical_features,\n",
    "            numerical_data=None,  # Placeholder, as data is processed separately\n",
    "            categorical_data=None,  # Placeholder\n",
    "            y=None,\n",
    "            numerical_embedding_type='linear',\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            depth=self.depth,\n",
    "            heads=self.heads,\n",
    "            attn_dropout=self.attn_dropout,\n",
    "            ff_dropout=self.ff_dropout,\n",
    "            explainable=True,\n",
    "        )\n",
    "        \n",
    "        # Create the complete model\n",
    "        model = FTTransformer(\n",
    "            encoder=encoder,\n",
    "            out_dim=self.out_dim,\n",
    "            out_activation=self.out_activation,\n",
    "        )\n",
    "        \n",
    "        # Compile the model\n",
    "        optimizer = tfa.optimizers.AdamW(\n",
    "            learning_rate=self.lr, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=self.loss,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self._set_random_seed(self.seed)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        \n",
    "        \n",
    "        # Split data into train/validation sets with fixed seed\n",
    "        train_data, val_data, train_target, val_target = train_test_split(\n",
    "            X_imputed, y, test_size=0.2, random_state=self.seed\n",
    "        )\n",
    "\n",
    "        train_data = X_imputed\n",
    "        train_target = y\n",
    "\n",
    "        \n",
    "        train_data_df = pd.DataFrame(train_data, columns = self.numerical_features)\n",
    "        train_target_df = pd.DataFrame(train_target, columns=['sii'])\n",
    "        \n",
    "        # Concatenate the target column\n",
    "        train_data = pd.concat([train_data_df, train_target_df], axis=1)\n",
    "\n",
    "        \n",
    "        val_data_df = pd.DataFrame(val_data, columns = self.numerical_features)\n",
    "        val_target_df = pd.DataFrame(val_target, columns=['sii'])\n",
    "        \n",
    "        # Concatenate the target column\n",
    "        val_data = pd.concat([val_data_df, val_target_df], axis=1)\n",
    "        \n",
    "        # print(\"Train dataframe after concat:\", train_data.columns.to_list())\n",
    "        # print(\"Valid df after concat:\", val_data.columns.to_list())\n",
    "           \n",
    "        # Use df_to_dataset to create TensorFlow datasets\n",
    "        train_dataset = df_to_dataset(train_data, 'sii', shuffle=self.shuffle, batch_size=self.batch_size)\n",
    "        val_dataset = df_to_dataset(val_data, 'sii', shuffle=False, batch_size=self.batch_size)\n",
    "        \n",
    "        # Build and train the model\n",
    "        self.model = self._build_model()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", mode=\"min\", patience=self.patience\n",
    "        )\n",
    "        \n",
    "        self.history = self.model.fit(\n",
    "            train_dataset,\n",
    "            epochs=self.epochs,\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self._set_random_seed(self.seed)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "\n",
    "        X_df = pd.DataFrame(X_imputed, columns = self.numerical_features)\n",
    "        \n",
    "        # Convert X to dataset using df_to_dataset\n",
    "        predict_dataset = df_to_dataset(X_df, shuffle=False, batch_size=self.batch_size)\n",
    "        \n",
    "        # Predict using the trained model\n",
    "        return self.model.predict(predict_dataset)['output'].flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2ca14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:22:26.997103Z",
     "iopub.status.busy": "2024-12-17T14:22:26.996719Z",
     "iopub.status.idle": "2024-12-17T14:22:27.003164Z",
     "shell.execute_reply": "2024-12-17T14:22:27.001924Z",
     "shell.execute_reply.started": "2024-12-17T14:22:26.997069Z"
    },
    "papermill": {
     "duration": 0.009742,
     "end_time": "2024-12-22T10:27:46.291836",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.282094",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"model-training\"></a>\n",
    "# **Model Training**\n",
    "Train multiple models with the optimized hyperparameters.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d151e07e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.313914Z",
     "iopub.status.busy": "2024-12-22T10:27:46.313605Z",
     "iopub.status.idle": "2024-12-22T10:27:46.327688Z",
     "shell.execute_reply": "2024-12-22T10:27:46.326753Z"
    },
    "papermill": {
     "duration": 0.027575,
     "end_time": "2024-12-22T10:27:46.329481",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.301906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_all_models_with_cv(train, test, target, best_params_dict, ensemble_method=None, weights=None, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train models using cross-validation, optionally using Voting or Stacking.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): Training data.\n",
    "        test (pd.DataFrame): Test data.\n",
    "        target (pd.Series): Target variable.\n",
    "        best_params_dict (dict): Optimized hyperparameters for each model.\n",
    "        ensemble_method (str): 'voting', 'stacking', or None for individual models.\n",
    "        n_splits (int): Number of CV splits.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of aggregated predictions for each model or ensemble method.         \n",
    "        dict: A dictionary of trained models.\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    trained_models = {}\n",
    "\n",
    "    models = []\n",
    "    for model_name, params in best_params_dict.items():\n",
    "        if model_name == \"LightGBM\":\n",
    "            model = LGBMRegressor(**params, verbose=-1)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            model = XGBRegressor(**params)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model = CatBoostRegressor(**params)\n",
    "        elif model_name == \"FTTransformer\":\n",
    "            model = FTTransformerWrapper(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        models.append((model_name, model))\n",
    "\n",
    "    if ensemble_method == 'voting':\n",
    "        if weights:\n",
    "            ensemble_model = VotingRegressor(estimators=models, weights=weights)\n",
    "        # Create a Voting Regressor\n",
    "        else:\n",
    "            ensemble_model = VotingRegressor(estimators=models)\n",
    "    elif ensemble_method == 'stacking':\n",
    "        # Create a Stacking Regressor\n",
    "        ensemble_model = StackingRegressor(estimators=models, final_estimator=LinearRegression())\n",
    "\n",
    "    for model_name, model in ([('Ensemble', ensemble_model)] if ensemble_method else models):\n",
    "        print(f\"Training model with CV: {model_name}\")\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "\n",
    "        train_S = []\n",
    "        test_S = []\n",
    "        \n",
    "        oof_non_rounded = np.zeros(len(target), dtype=float) \n",
    "        oof_rounded = np.zeros(len(target), dtype=int) \n",
    "        test_preds = np.zeros((len(test), n_splits))\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(cv.split(train, target)):\n",
    "            print(f\"Training fold {fold + 1}/{n_splits}...\")\n",
    "            X_train, X_valid = train.iloc[train_idx], train.iloc[test_idx]\n",
    "            y_train, y_valid = target.iloc[train_idx], target.iloc[test_idx]\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_valid)\n",
    "\n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "    \n",
    "            train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "            val_kappa = quadratic_weighted_kappa(y_valid, y_val_pred_rounded)\n",
    "    \n",
    "            train_S.append(train_kappa)\n",
    "            test_S.append(val_kappa)\n",
    "            \n",
    "            test_preds[:, fold] = model.predict(test)\n",
    "            \n",
    "            print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "            \n",
    "\n",
    "        print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "        print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(target, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "\n",
    "        print(\"KappaOPtimizer.x =\",  KappaOPtimizer.x)\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(target, oof_tuned)\n",
    "    \n",
    "        print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n",
    "    \n",
    "        tpm = test_preds.mean(axis=1)\n",
    "        tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "        #     # Aggregate test predictions\n",
    "        #     fold_predictions += model.predict(test) / n_splits\n",
    "\n",
    "        predictions[model_name] = tpTuned\n",
    "        trained_models[model_name] = model\n",
    "\n",
    "    return predictions, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c8ffa",
   "metadata": {
    "papermill": {
     "duration": 0.009137,
     "end_time": "2024-12-22T10:27:46.348013",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.338876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"submission\"></a>\n",
    "# **Submission**\n",
    "Generate and save the final submission file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "047512d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.368305Z",
     "iopub.status.busy": "2024-12-22T10:27:46.367757Z",
     "iopub.status.idle": "2024-12-22T10:27:46.372578Z",
     "shell.execute_reply": "2024-12-22T10:27:46.371639Z"
    },
    "papermill": {
     "duration": 0.016952,
     "end_time": "2024-12-22T10:27:46.374356",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.357404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_submission_file(predictions, sample):\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"sii\": np.round(predictions).astype(int)\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved!\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33ecf5",
   "metadata": {
    "papermill": {
     "duration": 0.009486,
     "end_time": "2024-12-22T10:27:46.393524",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.384038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"main-pipeline\"></a>\n",
    "# **Main Pipeline**\n",
    "Run the entire pipeline.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a420925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:46.432791Z",
     "iopub.status.busy": "2024-12-22T10:27:46.431983Z",
     "iopub.status.idle": "2024-12-22T10:27:52.947378Z",
     "shell.execute_reply": "2024-12-22T10:27:52.946093Z"
    },
    "papermill": {
     "duration": 6.528032,
     "end_time": "2024-12-22T10:27:52.949596",
     "exception": false,
     "start_time": "2024-12-22T10:27:46.421564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after feature engineering:  (3960, 99)\n",
      "Test shape after feature engineering:  (20, 77)\n",
      "Train shape after removing columns:  (3960, 65)\n",
      "Test shape after removing columns:  (20, 64)\n",
      "Final features included in train: ['sii', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-Height', 'Physical-Weight', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age', 'BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight', 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight', 'Sex_BMI', 'Sex_HeartRate', 'Age_WaistCirc', 'BMI_FitnessMaxStage', 'Weight_GripStrengthDominant', 'Weight_GripStrengthNonDominant', 'HeartRate_FitnessTime', 'Age_PushUp', 'FFMI_Age', 'InternetUse_SleepDisturbance', 'CGAS_BMI', 'CGAS_FitnessMaxStage']\n",
      "Deleted: /kaggle/working/__notebook__.ipynb\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "n_folds = 5\n",
    "\n",
    "# Set up logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)  # Limit verbosity\n",
    "\n",
    "# Paths\n",
    "train_path = '/kaggle/input/child-mind-institute-problematic-internet-use/train.csv'\n",
    "test_path = '/kaggle/input/child-mind-institute-problematic-internet-use/test.csv'\n",
    "sample_path = '/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv'\n",
    "time_series_train = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\"\n",
    "time_series_test = \"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\"\n",
    "\n",
    "# Toggle for using time-series data\n",
    "use_time_series = False  # Set to False to skip time-series data\n",
    "\n",
    "# Preprocess data\n",
    "train, test, sample = preprocess_csv_data(train_path, test_path, sample_path)\n",
    "\n",
    "if use_time_series:\n",
    "    train_ts, test_ts = preprocess_time_series_data(time_series_train, time_series_test, use_autoencoder=True, use_imputer=True, impute_method=\"mean\")\n",
    "else:\n",
    "    train_ts, test_ts = None, None\n",
    "train, test = merge_csv_and_time_series(train, test, train_ts, test_ts, use_time_series=use_time_series, use_numeric_imputation=True, numeric_impute_method=\"knn\")\n",
    "\n",
    "train = train.dropna(subset=['sii'])\n",
    "\n",
    "# Target and features\n",
    "target = train[\"sii\"]\n",
    "train = train.drop(columns=[\"sii\"])  # Drop `sii` from features\n",
    "\n",
    "\n",
    "# Ensure `sii` is not in test data\n",
    "if \"sii\" in test.columns:\n",
    "    test = test.drop(columns=[\"sii\"])\n",
    "\n",
    "if \"id\" in train.columns:\n",
    "    train = train.drop(columns=[\"id\"])\n",
    "    test = test.drop(columns=[\"id\"])\n",
    "\n",
    "working_dir = '/kaggle/working'\n",
    "clean_kaggle_working_directory(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b22d8cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:52.972218Z",
     "iopub.status.busy": "2024-12-22T10:27:52.971574Z",
     "iopub.status.idle": "2024-12-22T10:27:52.979765Z",
     "shell.execute_reply": "2024-12-22T10:27:52.978778Z"
    },
    "papermill": {
     "duration": 0.022077,
     "end_time": "2024-12-22T10:27:52.981646",
     "exception": false,
     "start_time": "2024-12-22T10:27:52.959569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "TARGET_FEATURE = 'sii'\n",
    "ID_COLUMN = 'id'\n",
    "\n",
    "# Automatically infer numeric and categorical features\n",
    "NUMERIC_FEATURES = train.select_dtypes(include=['int64', 'float64']).columns.difference([TARGET_FEATURE, ID_COLUMN]).tolist()\n",
    "CATEGORICAL_FEATURES = train.select_dtypes(include=['object', 'category']).columns.difference([TARGET_FEATURE, ID_COLUMN]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d4fed38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:53.003826Z",
     "iopub.status.busy": "2024-12-22T10:27:53.003146Z",
     "iopub.status.idle": "2024-12-22T10:27:53.009089Z",
     "shell.execute_reply": "2024-12-22T10:27:53.008176Z"
    },
    "papermill": {
     "duration": 0.019284,
     "end_time": "2024-12-22T10:27:53.010879",
     "exception": false,
     "start_time": "2024-12-22T10:27:52.991595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3bdfdcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:53.031775Z",
     "iopub.status.busy": "2024-12-22T10:27:53.031202Z",
     "iopub.status.idle": "2024-12-22T10:27:57.216483Z",
     "shell.execute_reply": "2024-12-22T10:27:57.215741Z"
    },
    "papermill": {
     "duration": 4.197817,
     "end_time": "2024-12-22T10:27:57.218663",
     "exception": false,
     "start_time": "2024-12-22T10:27:53.020846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old ver\n",
    "FTTransformer_Params = {\n",
    "    'numerical_features': NUMERIC_FEATURES,\n",
    "    'categorical_features': CATEGORICAL_FEATURES,\n",
    "    'embedding_dim': 64,\n",
    "    'depth': 4,\n",
    "    'heads': 8,\n",
    "    'attn_dropout': 0.4,\n",
    "    'ff_dropout': 0.4,\n",
    "    'out_dim': 1,\n",
    "    'out_activation': 'relu',\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'loss': tf.keras.losses.MeanSquaredError(),\n",
    "    'metrics': [tf.keras.metrics.RootMeanSquaredError()],\n",
    "    'epochs': 100,\n",
    "    'patience': 5,\n",
    "    'batch_size': 256,\n",
    "    'shuffle': True,\n",
    "    'seed': 64\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30e8ddd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:57.294916Z",
     "iopub.status.busy": "2024-12-22T10:27:57.294034Z",
     "iopub.status.idle": "2024-12-22T10:27:57.298493Z",
     "shell.execute_reply": "2024-12-22T10:27:57.297552Z"
    },
    "papermill": {
     "duration": 0.017265,
     "end_time": "2024-12-22T10:27:57.300382",
     "exception": false,
     "start_time": "2024-12-22T10:27:57.283117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_params_dict = { 'FTTransformer': FTTransformer_Params }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c3eb48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T10:27:57.320989Z",
     "iopub.status.busy": "2024-12-22T10:27:57.320420Z",
     "iopub.status.idle": "2024-12-22T10:30:34.084128Z",
     "shell.execute_reply": "2024-12-22T10:30:34.082945Z"
    },
    "papermill": {
     "duration": 156.776299,
     "end_time": "2024-12-22T10:30:34.086359",
     "exception": false,
     "start_time": "2024-12-22T10:27:57.310060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with CV: FTTransformer\n",
      "Training fold 1/5...\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 11s 186ms/step - loss: 1.6928 - importances_loss: 0.9122 - output_loss: 0.7806 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.8835 - val_loss: 1.5040 - val_importances_loss: 0.9425 - val_output_loss: 0.5615 - val_importances_root_mean_squared_error: 0.9708 - val_output_root_mean_squared_error: 0.7493\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5421 - importances_loss: 0.9122 - output_loss: 0.6299 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.7936 - val_loss: 1.5237 - val_importances_loss: 0.9425 - val_output_loss: 0.5812 - val_importances_root_mean_squared_error: 0.9708 - val_output_root_mean_squared_error: 0.7624\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.5210 - importances_loss: 0.9122 - output_loss: 0.6088 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.7803 - val_loss: 1.5158 - val_importances_loss: 0.9425 - val_output_loss: 0.5733 - val_importances_root_mean_squared_error: 0.9708 - val_output_root_mean_squared_error: 0.7572\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4849 - importances_loss: 0.9122 - output_loss: 0.5727 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.7568 - val_loss: 1.4682 - val_importances_loss: 0.9425 - val_output_loss: 0.5257 - val_importances_root_mean_squared_error: 0.9708 - val_output_root_mean_squared_error: 0.7251\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4520 - importances_loss: 0.9122 - output_loss: 0.5398 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.7347 - val_loss: 1.4770 - val_importances_loss: 0.9426 - val_output_loss: 0.5344 - val_importances_root_mean_squared_error: 0.9709 - val_output_root_mean_squared_error: 0.7311\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4585 - importances_loss: 0.9122 - output_loss: 0.5462 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.7391 - val_loss: 1.4509 - val_importances_loss: 0.9426 - val_output_loss: 0.5083 - val_importances_root_mean_squared_error: 0.9709 - val_output_root_mean_squared_error: 0.7129\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4338 - importances_loss: 0.9123 - output_loss: 0.5215 - importances_root_mean_squared_error: 0.9551 - output_root_mean_squared_error: 0.7221 - val_loss: 1.4607 - val_importances_loss: 0.9427 - val_output_loss: 0.5180 - val_importances_root_mean_squared_error: 0.9709 - val_output_root_mean_squared_error: 0.7197\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4328 - importances_loss: 0.9123 - output_loss: 0.5204 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.7214 - val_loss: 1.4822 - val_importances_loss: 0.9428 - val_output_loss: 0.5394 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.7344\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.4274 - importances_loss: 0.9124 - output_loss: 0.5150 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.7176 - val_loss: 1.4875 - val_importances_loss: 0.9428 - val_output_loss: 0.5447 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.7380\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4205 - importances_loss: 0.9124 - output_loss: 0.5081 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.7128 - val_loss: 1.4421 - val_importances_loss: 0.9428 - val_output_loss: 0.4993 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.7066\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4016 - importances_loss: 0.9124 - output_loss: 0.4892 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6994 - val_loss: 1.4321 - val_importances_loss: 0.9428 - val_output_loss: 0.4893 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6995\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 1.3973 - importances_loss: 0.9124 - output_loss: 0.4849 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6963 - val_loss: 1.4307 - val_importances_loss: 0.9428 - val_output_loss: 0.4880 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6985\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4025 - importances_loss: 0.9124 - output_loss: 0.4901 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.7001 - val_loss: 1.4359 - val_importances_loss: 0.9428 - val_output_loss: 0.4932 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.7023\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3963 - importances_loss: 0.9124 - output_loss: 0.4839 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6956 - val_loss: 1.4361 - val_importances_loss: 0.9427 - val_output_loss: 0.4934 - val_importances_root_mean_squared_error: 0.9709 - val_output_root_mean_squared_error: 0.7024\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3929 - importances_loss: 0.9124 - output_loss: 0.4805 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6932 - val_loss: 1.4206 - val_importances_loss: 0.9427 - val_output_loss: 0.4779 - val_importances_root_mean_squared_error: 0.9709 - val_output_root_mean_squared_error: 0.6913\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3878 - importances_loss: 0.9124 - output_loss: 0.4754 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6895 - val_loss: 1.4189 - val_importances_loss: 0.9428 - val_output_loss: 0.4762 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6901\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3824 - importances_loss: 0.9124 - output_loss: 0.4700 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6855 - val_loss: 1.4161 - val_importances_loss: 0.9428 - val_output_loss: 0.4734 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6880\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3991 - importances_loss: 0.9124 - output_loss: 0.4867 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6976 - val_loss: 1.4107 - val_importances_loss: 0.9428 - val_output_loss: 0.4679 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6840\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3778 - importances_loss: 0.9124 - output_loss: 0.4654 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6822 - val_loss: 1.4184 - val_importances_loss: 0.9428 - val_output_loss: 0.4756 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6897\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3737 - importances_loss: 0.9125 - output_loss: 0.4613 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6792 - val_loss: 1.4107 - val_importances_loss: 0.9429 - val_output_loss: 0.4678 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6840\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3810 - importances_loss: 0.9125 - output_loss: 0.4685 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6845 - val_loss: 1.4074 - val_importances_loss: 0.9428 - val_output_loss: 0.4646 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6816\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3897 - importances_loss: 0.9125 - output_loss: 0.4772 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6908 - val_loss: 1.4175 - val_importances_loss: 0.9428 - val_output_loss: 0.4747 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6890\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3826 - importances_loss: 0.9125 - output_loss: 0.4701 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6856 - val_loss: 1.4242 - val_importances_loss: 0.9428 - val_output_loss: 0.4814 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6938\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3729 - importances_loss: 0.9125 - output_loss: 0.4605 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6786 - val_loss: 1.4122 - val_importances_loss: 0.9428 - val_output_loss: 0.4694 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.6851\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3863 - importances_loss: 0.9125 - output_loss: 0.4738 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6884 - val_loss: 1.4444 - val_importances_loss: 0.9428 - val_output_loss: 0.5016 - val_importances_root_mean_squared_error: 0.9710 - val_output_root_mean_squared_error: 0.7083\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3929 - importances_loss: 0.9125 - output_loss: 0.4804 - importances_root_mean_squared_error: 0.9552 - output_root_mean_squared_error: 0.6931 - val_loss: 1.4943 - val_importances_loss: 0.9427 - val_output_loss: 0.5516 - val_importances_root_mean_squared_error: 0.9709 - val_output_root_mean_squared_error: 0.7427\n",
      "9/9 [==============================] - 1s 29ms/step\n",
      "3/3 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "Fold 1 - Train QWK: 0.2863, Validation QWK: 0.3091\n",
      "Training fold 2/5...\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 7s 185ms/step - loss: 1.7251 - importances_loss: 0.9158 - output_loss: 0.8092 - importances_root_mean_squared_error: 0.9570 - output_root_mean_squared_error: 0.8996 - val_loss: 1.7727 - val_importances_loss: 1.0059 - val_output_loss: 0.7668 - val_importances_root_mean_squared_error: 1.0029 - val_output_root_mean_squared_error: 0.8757\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.5652 - importances_loss: 0.9158 - output_loss: 0.6494 - importances_root_mean_squared_error: 0.9570 - output_root_mean_squared_error: 0.8058 - val_loss: 1.6225 - val_importances_loss: 1.0059 - val_output_loss: 0.6166 - val_importances_root_mean_squared_error: 1.0029 - val_output_root_mean_squared_error: 0.7852\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5311 - importances_loss: 0.9158 - output_loss: 0.6153 - importances_root_mean_squared_error: 0.9570 - output_root_mean_squared_error: 0.7844 - val_loss: 1.6385 - val_importances_loss: 1.0059 - val_output_loss: 0.6326 - val_importances_root_mean_squared_error: 1.0029 - val_output_root_mean_squared_error: 0.7954\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5246 - importances_loss: 0.9158 - output_loss: 0.6088 - importances_root_mean_squared_error: 0.9570 - output_root_mean_squared_error: 0.7803 - val_loss: 1.5974 - val_importances_loss: 1.0059 - val_output_loss: 0.5915 - val_importances_root_mean_squared_error: 1.0029 - val_output_root_mean_squared_error: 0.7691\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4735 - importances_loss: 0.9159 - output_loss: 0.5576 - importances_root_mean_squared_error: 0.9570 - output_root_mean_squared_error: 0.7467 - val_loss: 1.5781 - val_importances_loss: 1.0060 - val_output_loss: 0.5721 - val_importances_root_mean_squared_error: 1.0030 - val_output_root_mean_squared_error: 0.7564\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4544 - importances_loss: 0.9159 - output_loss: 0.5385 - importances_root_mean_squared_error: 0.9570 - output_root_mean_squared_error: 0.7338 - val_loss: 1.5668 - val_importances_loss: 1.0061 - val_output_loss: 0.5608 - val_importances_root_mean_squared_error: 1.0030 - val_output_root_mean_squared_error: 0.7488\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4401 - importances_loss: 0.9160 - output_loss: 0.5240 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7239 - val_loss: 1.5795 - val_importances_loss: 1.0061 - val_output_loss: 0.5734 - val_importances_root_mean_squared_error: 1.0030 - val_output_root_mean_squared_error: 0.7572\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4351 - importances_loss: 0.9161 - output_loss: 0.5190 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7204 - val_loss: 1.5441 - val_importances_loss: 1.0062 - val_output_loss: 0.5379 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7334\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4203 - importances_loss: 0.9161 - output_loss: 0.5042 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7101 - val_loss: 1.5384 - val_importances_loss: 1.0062 - val_output_loss: 0.5322 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7295\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4103 - importances_loss: 0.9161 - output_loss: 0.4942 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7030 - val_loss: 1.5466 - val_importances_loss: 1.0062 - val_output_loss: 0.5405 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7352\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4146 - importances_loss: 0.9161 - output_loss: 0.4985 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7061 - val_loss: 1.5352 - val_importances_loss: 1.0062 - val_output_loss: 0.5290 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7273\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4087 - importances_loss: 0.9161 - output_loss: 0.4926 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7018 - val_loss: 1.5365 - val_importances_loss: 1.0062 - val_output_loss: 0.5303 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7282\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4065 - importances_loss: 0.9161 - output_loss: 0.4904 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7003 - val_loss: 1.5369 - val_importances_loss: 1.0062 - val_output_loss: 0.5308 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7285\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4024 - importances_loss: 0.9161 - output_loss: 0.4862 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6973 - val_loss: 1.5364 - val_importances_loss: 1.0062 - val_output_loss: 0.5302 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7282\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4005 - importances_loss: 0.9161 - output_loss: 0.4844 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.6960 - val_loss: 1.5249 - val_importances_loss: 1.0062 - val_output_loss: 0.5187 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7202\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4026 - importances_loss: 0.9161 - output_loss: 0.4865 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.6975 - val_loss: 1.5225 - val_importances_loss: 1.0062 - val_output_loss: 0.5163 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7186\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4079 - importances_loss: 0.9161 - output_loss: 0.4918 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7013 - val_loss: 1.5233 - val_importances_loss: 1.0062 - val_output_loss: 0.5171 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7191\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3898 - importances_loss: 0.9161 - output_loss: 0.4737 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.6883 - val_loss: 1.5215 - val_importances_loss: 1.0062 - val_output_loss: 0.5152 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7178\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4126 - importances_loss: 0.9161 - output_loss: 0.4965 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7046 - val_loss: 1.5561 - val_importances_loss: 1.0062 - val_output_loss: 0.5499 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7415\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4098 - importances_loss: 0.9161 - output_loss: 0.4937 - importances_root_mean_squared_error: 0.9571 - output_root_mean_squared_error: 0.7026 - val_loss: 1.5378 - val_importances_loss: 1.0062 - val_output_loss: 0.5316 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7291\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4127 - importances_loss: 0.9161 - output_loss: 0.4965 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.7046 - val_loss: 1.5194 - val_importances_loss: 1.0062 - val_output_loss: 0.5132 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7164\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3968 - importances_loss: 0.9162 - output_loss: 0.4807 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6933 - val_loss: 1.5132 - val_importances_loss: 1.0062 - val_output_loss: 0.5070 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7120\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3889 - importances_loss: 0.9162 - output_loss: 0.4728 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6876 - val_loss: 1.5041 - val_importances_loss: 1.0062 - val_output_loss: 0.4979 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7056\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3891 - importances_loss: 0.9162 - output_loss: 0.4729 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6877 - val_loss: 1.5326 - val_importances_loss: 1.0063 - val_output_loss: 0.5263 - val_importances_root_mean_squared_error: 1.0032 - val_output_root_mean_squared_error: 0.7255\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4141 - importances_loss: 0.9162 - output_loss: 0.4979 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.7056 - val_loss: 1.5541 - val_importances_loss: 1.0063 - val_output_loss: 0.5479 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7402\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4076 - importances_loss: 0.9162 - output_loss: 0.4915 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.7010 - val_loss: 1.5145 - val_importances_loss: 1.0063 - val_output_loss: 0.5082 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7129\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3912 - importances_loss: 0.9162 - output_loss: 0.4750 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6892 - val_loss: 1.5049 - val_importances_loss: 1.0062 - val_output_loss: 0.4987 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7062\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3880 - importances_loss: 0.9162 - output_loss: 0.4719 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6869 - val_loss: 1.5027 - val_importances_loss: 1.0062 - val_output_loss: 0.4965 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7046\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3809 - importances_loss: 0.9162 - output_loss: 0.4647 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6817 - val_loss: 1.4943 - val_importances_loss: 1.0063 - val_output_loss: 0.4881 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6986\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 1.3821 - importances_loss: 0.9162 - output_loss: 0.4659 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6826 - val_loss: 1.4946 - val_importances_loss: 1.0063 - val_output_loss: 0.4883 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6988\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.3785 - importances_loss: 0.9162 - output_loss: 0.4624 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6800 - val_loss: 1.5094 - val_importances_loss: 1.0063 - val_output_loss: 0.5032 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7094\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3812 - importances_loss: 0.9162 - output_loss: 0.4650 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6819 - val_loss: 1.4919 - val_importances_loss: 1.0063 - val_output_loss: 0.4856 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6969\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3797 - importances_loss: 0.9162 - output_loss: 0.4635 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6808 - val_loss: 1.4902 - val_importances_loss: 1.0063 - val_output_loss: 0.4840 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6957\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3801 - importances_loss: 0.9162 - output_loss: 0.4639 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6811 - val_loss: 1.5007 - val_importances_loss: 1.0062 - val_output_loss: 0.4945 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7032\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3751 - importances_loss: 0.9162 - output_loss: 0.4590 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6775 - val_loss: 1.4939 - val_importances_loss: 1.0062 - val_output_loss: 0.4877 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6984\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3812 - importances_loss: 0.9162 - output_loss: 0.4651 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6820 - val_loss: 1.5126 - val_importances_loss: 1.0062 - val_output_loss: 0.5064 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7116\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3786 - importances_loss: 0.9162 - output_loss: 0.4624 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6800 - val_loss: 1.5008 - val_importances_loss: 1.0062 - val_output_loss: 0.4946 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7033\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3813 - importances_loss: 0.9162 - output_loss: 0.4652 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6821 - val_loss: 1.4887 - val_importances_loss: 1.0062 - val_output_loss: 0.4825 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6946\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3908 - importances_loss: 0.9161 - output_loss: 0.4746 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6889 - val_loss: 1.5092 - val_importances_loss: 1.0062 - val_output_loss: 0.5030 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7092\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3896 - importances_loss: 0.9161 - output_loss: 0.4734 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6881 - val_loss: 1.5059 - val_importances_loss: 1.0062 - val_output_loss: 0.4996 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7068\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3886 - importances_loss: 0.9162 - output_loss: 0.4724 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6873 - val_loss: 1.4934 - val_importances_loss: 1.0062 - val_output_loss: 0.4872 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.6980\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3765 - importances_loss: 0.9162 - output_loss: 0.4604 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6785 - val_loss: 1.5071 - val_importances_loss: 1.0062 - val_output_loss: 0.5009 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7077\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3854 - importances_loss: 0.9162 - output_loss: 0.4692 - importances_root_mean_squared_error: 0.9572 - output_root_mean_squared_error: 0.6850 - val_loss: 1.5133 - val_importances_loss: 1.0062 - val_output_loss: 0.5071 - val_importances_root_mean_squared_error: 1.0031 - val_output_root_mean_squared_error: 0.7121\n",
      "9/9 [==============================] - 1s 28ms/step\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "Fold 2 - Train QWK: 0.3757, Validation QWK: 0.3909\n",
      "Training fold 3/5...\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 7s 238ms/step - loss: 1.6430 - importances_loss: 0.9118 - output_loss: 0.7312 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.8551 - val_loss: 1.6330 - val_importances_loss: 0.8647 - val_output_loss: 0.7683 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.8765\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5280 - importances_loss: 0.9118 - output_loss: 0.6162 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7850 - val_loss: 1.4571 - val_importances_loss: 0.8647 - val_output_loss: 0.5924 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.7697\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4920 - importances_loss: 0.9118 - output_loss: 0.5802 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7617 - val_loss: 1.4034 - val_importances_loss: 0.8647 - val_output_loss: 0.5387 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.7339\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4510 - importances_loss: 0.9118 - output_loss: 0.5392 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7343 - val_loss: 1.4318 - val_importances_loss: 0.8647 - val_output_loss: 0.5671 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.7531\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4407 - importances_loss: 0.9118 - output_loss: 0.5289 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7273 - val_loss: 1.3922 - val_importances_loss: 0.8647 - val_output_loss: 0.5275 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.7263\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4282 - importances_loss: 0.9118 - output_loss: 0.5164 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7186 - val_loss: 1.4312 - val_importances_loss: 0.8648 - val_output_loss: 0.5664 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.7526\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4167 - importances_loss: 0.9119 - output_loss: 0.5048 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7105 - val_loss: 1.3785 - val_importances_loss: 0.8648 - val_output_loss: 0.5137 - val_importances_root_mean_squared_error: 0.9299 - val_output_root_mean_squared_error: 0.7167\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4240 - importances_loss: 0.9119 - output_loss: 0.5121 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7156 - val_loss: 1.3707 - val_importances_loss: 0.8648 - val_output_loss: 0.5059 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7113\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4140 - importances_loss: 0.9119 - output_loss: 0.5021 - importances_root_mean_squared_error: 0.9549 - output_root_mean_squared_error: 0.7086 - val_loss: 1.4096 - val_importances_loss: 0.8649 - val_output_loss: 0.5447 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7380\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4073 - importances_loss: 0.9120 - output_loss: 0.4954 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.7038 - val_loss: 1.4122 - val_importances_loss: 0.8649 - val_output_loss: 0.5473 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7398\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4060 - importances_loss: 0.9120 - output_loss: 0.4940 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.7029 - val_loss: 1.3710 - val_importances_loss: 0.8649 - val_output_loss: 0.5061 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7114\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3959 - importances_loss: 0.9120 - output_loss: 0.4840 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6957 - val_loss: 1.3626 - val_importances_loss: 0.8649 - val_output_loss: 0.4977 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7055\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3870 - importances_loss: 0.9119 - output_loss: 0.4751 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6893 - val_loss: 1.3628 - val_importances_loss: 0.8649 - val_output_loss: 0.4979 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7056\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3901 - importances_loss: 0.9119 - output_loss: 0.4781 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6915 - val_loss: 1.3798 - val_importances_loss: 0.8649 - val_output_loss: 0.5149 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7176\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3888 - importances_loss: 0.9119 - output_loss: 0.4769 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6906 - val_loss: 1.3807 - val_importances_loss: 0.8649 - val_output_loss: 0.5158 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7182\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 1.3939 - importances_loss: 0.9119 - output_loss: 0.4820 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6942 - val_loss: 1.3577 - val_importances_loss: 0.8649 - val_output_loss: 0.4928 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7020\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.3961 - importances_loss: 0.9120 - output_loss: 0.4842 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6958 - val_loss: 1.3574 - val_importances_loss: 0.8649 - val_output_loss: 0.4925 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7018\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3899 - importances_loss: 0.9120 - output_loss: 0.4780 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6914 - val_loss: 1.3531 - val_importances_loss: 0.8649 - val_output_loss: 0.4882 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6987\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.3769 - importances_loss: 0.9120 - output_loss: 0.4649 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6818 - val_loss: 1.3526 - val_importances_loss: 0.8649 - val_output_loss: 0.4877 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6984\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3761 - importances_loss: 0.9120 - output_loss: 0.4641 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6812 - val_loss: 1.3525 - val_importances_loss: 0.8649 - val_output_loss: 0.4876 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6983\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3858 - importances_loss: 0.9120 - output_loss: 0.4738 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6883 - val_loss: 1.3625 - val_importances_loss: 0.8649 - val_output_loss: 0.4976 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7054\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 1.3794 - importances_loss: 0.9120 - output_loss: 0.4674 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6837 - val_loss: 1.3795 - val_importances_loss: 0.8650 - val_output_loss: 0.5145 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7173\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3851 - importances_loss: 0.9120 - output_loss: 0.4731 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6878 - val_loss: 1.3560 - val_importances_loss: 0.8649 - val_output_loss: 0.4910 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7007\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.3732 - importances_loss: 0.9120 - output_loss: 0.4611 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6791 - val_loss: 1.3489 - val_importances_loss: 0.8650 - val_output_loss: 0.4840 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6957\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3656 - importances_loss: 0.9121 - output_loss: 0.4535 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6734 - val_loss: 1.3541 - val_importances_loss: 0.8650 - val_output_loss: 0.4891 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6994\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3690 - importances_loss: 0.9120 - output_loss: 0.4569 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6760 - val_loss: 1.3509 - val_importances_loss: 0.8650 - val_output_loss: 0.4859 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6971\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3650 - importances_loss: 0.9120 - output_loss: 0.4530 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6730 - val_loss: 1.3557 - val_importances_loss: 0.8649 - val_output_loss: 0.4907 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.7005\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3792 - importances_loss: 0.9120 - output_loss: 0.4672 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6835 - val_loss: 1.3521 - val_importances_loss: 0.8649 - val_output_loss: 0.4872 - val_importances_root_mean_squared_error: 0.9300 - val_output_root_mean_squared_error: 0.6980\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3948 - importances_loss: 0.9120 - output_loss: 0.4828 - importances_root_mean_squared_error: 0.9550 - output_root_mean_squared_error: 0.6948 - val_loss: 1.3955 - val_importances_loss: 0.8650 - val_output_loss: 0.5305 - val_importances_root_mean_squared_error: 0.9301 - val_output_root_mean_squared_error: 0.7283\n",
      "9/9 [==============================] - 1s 28ms/step\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "Fold 3 - Train QWK: 0.3359, Validation QWK: 0.3441\n",
      "Training fold 4/5...\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 7s 188ms/step - loss: 1.7559 - importances_loss: 0.9136 - output_loss: 0.8424 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.9178 - val_loss: 1.5041 - val_importances_loss: 0.8954 - val_output_loss: 0.6088 - val_importances_root_mean_squared_error: 0.9462 - val_output_root_mean_squared_error: 0.7802\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5685 - importances_loss: 0.9136 - output_loss: 0.6549 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.8093 - val_loss: 1.5155 - val_importances_loss: 0.8954 - val_output_loss: 0.6202 - val_importances_root_mean_squared_error: 0.9462 - val_output_root_mean_squared_error: 0.7875\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.5148 - importances_loss: 0.9136 - output_loss: 0.6012 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7754 - val_loss: 1.4471 - val_importances_loss: 0.8954 - val_output_loss: 0.5518 - val_importances_root_mean_squared_error: 0.9462 - val_output_root_mean_squared_error: 0.7428\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4738 - importances_loss: 0.9136 - output_loss: 0.5602 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7485 - val_loss: 1.4046 - val_importances_loss: 0.8954 - val_output_loss: 0.5092 - val_importances_root_mean_squared_error: 0.9462 - val_output_root_mean_squared_error: 0.7136\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4515 - importances_loss: 0.9136 - output_loss: 0.5379 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7334 - val_loss: 1.4104 - val_importances_loss: 0.8954 - val_output_loss: 0.5150 - val_importances_root_mean_squared_error: 0.9463 - val_output_root_mean_squared_error: 0.7176\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4442 - importances_loss: 0.9136 - output_loss: 0.5306 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7284 - val_loss: 1.3831 - val_importances_loss: 0.8954 - val_output_loss: 0.4877 - val_importances_root_mean_squared_error: 0.9463 - val_output_root_mean_squared_error: 0.6983\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4200 - importances_loss: 0.9137 - output_loss: 0.5063 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7116 - val_loss: 1.3811 - val_importances_loss: 0.8955 - val_output_loss: 0.4856 - val_importances_root_mean_squared_error: 0.9463 - val_output_root_mean_squared_error: 0.6968\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.4198 - importances_loss: 0.9137 - output_loss: 0.5061 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7114 - val_loss: 1.3616 - val_importances_loss: 0.8956 - val_output_loss: 0.4660 - val_importances_root_mean_squared_error: 0.9463 - val_output_root_mean_squared_error: 0.6827\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3917 - importances_loss: 0.9138 - output_loss: 0.4780 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6913 - val_loss: 1.3509 - val_importances_loss: 0.8956 - val_output_loss: 0.4554 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6748\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4078 - importances_loss: 0.9138 - output_loss: 0.4940 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7029 - val_loss: 1.3492 - val_importances_loss: 0.8956 - val_output_loss: 0.4536 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6735\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4285 - importances_loss: 0.9138 - output_loss: 0.5147 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7175 - val_loss: 1.3509 - val_importances_loss: 0.8956 - val_output_loss: 0.4552 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6747\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3974 - importances_loss: 0.9138 - output_loss: 0.4836 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6954 - val_loss: 1.3487 - val_importances_loss: 0.8957 - val_output_loss: 0.4531 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6731\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3932 - importances_loss: 0.9138 - output_loss: 0.4794 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6924 - val_loss: 1.3500 - val_importances_loss: 0.8956 - val_output_loss: 0.4544 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6741\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3868 - importances_loss: 0.9138 - output_loss: 0.4730 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6877 - val_loss: 1.3449 - val_importances_loss: 0.8956 - val_output_loss: 0.4493 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6703\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3938 - importances_loss: 0.9138 - output_loss: 0.4800 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6928 - val_loss: 1.3493 - val_importances_loss: 0.8956 - val_output_loss: 0.4537 - val_importances_root_mean_squared_error: 0.9463 - val_output_root_mean_squared_error: 0.6736\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.3845 - importances_loss: 0.9138 - output_loss: 0.4707 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6861 - val_loss: 1.3392 - val_importances_loss: 0.8956 - val_output_loss: 0.4435 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6660\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 1.3757 - importances_loss: 0.9138 - output_loss: 0.4618 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6796 - val_loss: 1.3386 - val_importances_loss: 0.8956 - val_output_loss: 0.4430 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6656\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3769 - importances_loss: 0.9138 - output_loss: 0.4631 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.6805 - val_loss: 1.3897 - val_importances_loss: 0.8956 - val_output_loss: 0.4941 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.7029\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4151 - importances_loss: 0.9138 - output_loss: 0.5012 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.7080 - val_loss: 1.4102 - val_importances_loss: 0.8956 - val_output_loss: 0.5146 - val_importances_root_mean_squared_error: 0.9463 - val_output_root_mean_squared_error: 0.7174\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 1.4078 - importances_loss: 0.9138 - output_loss: 0.4940 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7029 - val_loss: 1.3446 - val_importances_loss: 0.8957 - val_output_loss: 0.4490 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6700\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3890 - importances_loss: 0.9139 - output_loss: 0.4751 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6893 - val_loss: 1.3389 - val_importances_loss: 0.8957 - val_output_loss: 0.4433 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6658\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3887 - importances_loss: 0.9138 - output_loss: 0.4749 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6891 - val_loss: 1.3443 - val_importances_loss: 0.8957 - val_output_loss: 0.4487 - val_importances_root_mean_squared_error: 0.9464 - val_output_root_mean_squared_error: 0.6698\n",
      "9/9 [==============================] - 1s 28ms/step\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 262ms/step\n",
      "Fold 4 - Train QWK: 0.3501, Validation QWK: 0.3106\n",
      "Training fold 5/5...\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 7s 185ms/step - loss: 1.7083 - importances_loss: 0.9136 - output_loss: 0.7947 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.8915 - val_loss: 1.6043 - val_importances_loss: 0.9858 - val_output_loss: 0.6185 - val_importances_root_mean_squared_error: 0.9929 - val_output_root_mean_squared_error: 0.7864\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5651 - importances_loss: 0.9136 - output_loss: 0.6516 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.8072 - val_loss: 1.6193 - val_importances_loss: 0.9858 - val_output_loss: 0.6335 - val_importances_root_mean_squared_error: 0.9929 - val_output_root_mean_squared_error: 0.7959\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.5341 - importances_loss: 0.9136 - output_loss: 0.6205 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7877 - val_loss: 1.5863 - val_importances_loss: 0.9858 - val_output_loss: 0.6005 - val_importances_root_mean_squared_error: 0.9929 - val_output_root_mean_squared_error: 0.7749\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.5029 - importances_loss: 0.9136 - output_loss: 0.5893 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7677 - val_loss: 1.5551 - val_importances_loss: 0.9859 - val_output_loss: 0.5693 - val_importances_root_mean_squared_error: 0.9929 - val_output_root_mean_squared_error: 0.7545\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4704 - importances_loss: 0.9136 - output_loss: 0.5568 - importances_root_mean_squared_error: 0.9558 - output_root_mean_squared_error: 0.7462 - val_loss: 1.5146 - val_importances_loss: 0.9860 - val_output_loss: 0.5286 - val_importances_root_mean_squared_error: 0.9930 - val_output_root_mean_squared_error: 0.7271\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.4306 - importances_loss: 0.9137 - output_loss: 0.5169 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7189 - val_loss: 1.5385 - val_importances_loss: 0.9859 - val_output_loss: 0.5526 - val_importances_root_mean_squared_error: 0.9929 - val_output_root_mean_squared_error: 0.7434\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4315 - importances_loss: 0.9137 - output_loss: 0.5177 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7195 - val_loss: 1.5148 - val_importances_loss: 0.9860 - val_output_loss: 0.5288 - val_importances_root_mean_squared_error: 0.9930 - val_output_root_mean_squared_error: 0.7272\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4085 - importances_loss: 0.9138 - output_loss: 0.4947 - importances_root_mean_squared_error: 0.9559 - output_root_mean_squared_error: 0.7033 - val_loss: 1.5044 - val_importances_loss: 0.9861 - val_output_loss: 0.5183 - val_importances_root_mean_squared_error: 0.9930 - val_output_root_mean_squared_error: 0.7199\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4102 - importances_loss: 0.9139 - output_loss: 0.4963 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.7045 - val_loss: 1.4850 - val_importances_loss: 0.9862 - val_output_loss: 0.4988 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.7062\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.4006 - importances_loss: 0.9139 - output_loss: 0.4867 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6976 - val_loss: 1.4675 - val_importances_loss: 0.9863 - val_output_loss: 0.4812 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6937\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3978 - importances_loss: 0.9140 - output_loss: 0.4838 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6956 - val_loss: 1.4576 - val_importances_loss: 0.9863 - val_output_loss: 0.4713 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6865\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3983 - importances_loss: 0.9140 - output_loss: 0.4844 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6960 - val_loss: 1.4682 - val_importances_loss: 0.9862 - val_output_loss: 0.4820 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6943\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3876 - importances_loss: 0.9140 - output_loss: 0.4736 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6882 - val_loss: 1.4521 - val_importances_loss: 0.9862 - val_output_loss: 0.4658 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6825\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3908 - importances_loss: 0.9139 - output_loss: 0.4769 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6906 - val_loss: 1.4566 - val_importances_loss: 0.9862 - val_output_loss: 0.4704 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6859\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3869 - importances_loss: 0.9139 - output_loss: 0.4729 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6877 - val_loss: 1.4533 - val_importances_loss: 0.9862 - val_output_loss: 0.4671 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6834\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3827 - importances_loss: 0.9139 - output_loss: 0.4688 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6847 - val_loss: 1.4696 - val_importances_loss: 0.9862 - val_output_loss: 0.4834 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6953\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3840 - importances_loss: 0.9139 - output_loss: 0.4700 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6856 - val_loss: 1.4726 - val_importances_loss: 0.9862 - val_output_loss: 0.4864 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6974\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.3865 - importances_loss: 0.9139 - output_loss: 0.4726 - importances_root_mean_squared_error: 0.9560 - output_root_mean_squared_error: 0.6875 - val_loss: 1.4680 - val_importances_loss: 0.9862 - val_output_loss: 0.4819 - val_importances_root_mean_squared_error: 0.9931 - val_output_root_mean_squared_error: 0.6942\n",
      "9/9 [==============================] - 1s 28ms/step\n",
      "3/3 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "Fold 5 - Train QWK: 0.3644, Validation QWK: 0.2872\n",
      "Mean Train QWK --> 0.3425\n",
      "Mean Validation QWK ---> 0.3284\n",
      "KappaOPtimizer.x = [0.55131234 1.00835571 2.64618972]\n",
      "----> || Optimized QWK SCORE :: 0.399\n",
      "Submission saved!\n"
     ]
    }
   ],
   "source": [
    "# Train model and make predictions using cross-validation\n",
    "predictions, trained_models = train_all_models_with_cv(train, test, target, best_params_dict)\n",
    "\n",
    "\n",
    "# Generate submission\n",
    "final_predictions = next(iter(predictions.values()))\n",
    "submission = generate_submission_file(final_predictions, sample)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6310681,
     "sourceId": 10210719,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30407,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 326.954204,
   "end_time": "2024-12-22T10:30:38.066921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-22T10:25:11.112717",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
